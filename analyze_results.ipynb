{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# # Display full column width\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cmlscratch/astein0/tmp/ipykernel_3167261/50461555.py:7: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = cm.get_cmap(\"Greys\")  # or \"viridis\", \"plasma\", etc.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "cmap = cm.get_cmap(\"Greys\")  # or \"viridis\", \"plasma\", etc.\n",
    "\n",
    "def my_compression_plotter(subset, group_col, metric, base_line, title:str = None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Normalize compression values for colormap\n",
    "    compression_values = subset[\"compression_ratio\"].astype(float)\n",
    "    norm = mcolors.Normalize(vmin=compression_values.min(), vmax=compression_values.max())\n",
    "\n",
    "    for i, (name, group) in enumerate(subset.groupby(group_col)):\n",
    "        x = group[\"num_new_tokens\"]\n",
    "        y = group[metric]\n",
    "        compression = group[\"compression_ratio\"].astype(float)\n",
    "        compression_str = (compression * 100).round(1).astype(str) + \"%\"\n",
    "\n",
    "        label = \" | \".join(name)\n",
    "        plt.plot(x, y, color=f\"C{i}\", label=label)\n",
    "\n",
    "        colors = cmap(norm(compression))\n",
    "        plt.scatter(x, y, c=colors, edgecolor='k', s=60)\n",
    "        # Add annotations\n",
    "        for xi, yi, text in zip(x, y, compression_str):\n",
    "            plt.annotate(text, (xi, yi), textcoords=\"offset points\", xytext=(5, 5), ha='left', fontsize=9)\n",
    "\n",
    "    plt.axhline(y=base_line[metric].values[0], color='gray', linestyle='--', linewidth=1.5, label='Baseline')\n",
    "\n",
    "    plt.xlabel(\"Number of New Tokens\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    else:\n",
    "        plt.title(\"Accuracy vs Number of New Tokens by Finetuning\")\n",
    "    plt.legend(title='|'.join(group_col))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cmlscratch/astein0/tmp/ipykernel_3167261/405337152.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'NAN' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  main_results.fillna(\"NAN\", inplace=True)\n",
      "/cmlscratch/astein0/tmp/ipykernel_3167261/405337152.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  main_results[\"task_name\"].replace(\"ifeval\", \"baseline\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: model_name_sanitized                                       meta-llama__Llama-3.2-3B\n",
      "trained_model_name                                         meta-llama__Llama-3.2-3B\n",
      "model_name                                                             Llama-3.2-3B\n",
      "embeddings_init                                                                 NAN\n",
      "task_name                                                                  baseline\n",
      "exact_match                                                                     NAN\n",
      "exact_match_stderr                                                              NAN\n",
      "prompt_level_strict_acc                                                    0.195933\n",
      "prompt_level_strict_acc_stderr                                             0.017081\n",
      "inst_level_strict_acc                                                       0.32494\n",
      "inst_level_strict_acc_stderr                                                    NAN\n",
      "prompt_level_loose_acc                                                     0.218115\n",
      "prompt_level_loose_acc_stderr                                              0.017771\n",
      "inst_level_loose_acc                                                        0.35012\n",
      "inst_level_loose_acc_stderr                                                     NAN\n",
      "compression_ratio                                                          0.001207\n",
      "learning_ratio                                                              0.00482\n",
      "theoretical_compression_ratio                                                   0.0\n",
      "alias                                                                        ifeval\n",
      "limit                                                                           NAN\n",
      "json_path                         eval_results/full_patching/meta-llama__Llama-3...\n",
      "dataset_str                                                                 unknown\n",
      "finetuning_params                                                               NAN\n",
      "total_batch_size                                                                NAN\n",
      "learning_rate                                                                   NAN\n",
      "main_loss_type                                                                  NAN\n",
      "embedding_init_strategy                                                         NAN\n",
      "num_new_tokens                                                                  NAN\n",
      "unfreeze_params_steps                                                           NAN\n",
      "finetune_params_prefreeze                                                       NAN\n",
      "dataset                                                                         NAN\n",
      "tokenizer_path                                                                  NAN\n",
      "seed                                                                            NAN\n",
      "reset_optimizer                                                                 NAN\n",
      "warmup_steps                                                                    NAN\n",
      "lr_schedule                                                                     NAN\n",
      "warmup_steps_prefreeze                                                          NAN\n",
      "lr_schedule_prefreeze                                                           NAN\n",
      "Name: 12, dtype: object\n",
      "Error: model_name_sanitized                              meta-llama__Llama-3.2-3B-Instruct\n",
      "trained_model_name                                meta-llama__Llama-3.2-3B-Instruct\n",
      "model_name                                                    Llama-3.2-3B-Instruct\n",
      "embeddings_init                                                                 NAN\n",
      "task_name                                                                  baseline\n",
      "exact_match                                                                     NAN\n",
      "exact_match_stderr                                                              NAN\n",
      "prompt_level_strict_acc                                                      0.7061\n",
      "prompt_level_strict_acc_stderr                                             0.019604\n",
      "inst_level_strict_acc                                                      0.790168\n",
      "inst_level_strict_acc_stderr                                                    NAN\n",
      "prompt_level_loose_acc                                                     0.750462\n",
      "prompt_level_loose_acc_stderr                                              0.018622\n",
      "inst_level_loose_acc                                                       0.822542\n",
      "inst_level_loose_acc_stderr                                                     NAN\n",
      "compression_ratio                                                         -0.010776\n",
      "learning_ratio                                                             0.012392\n",
      "theoretical_compression_ratio                                                   0.0\n",
      "alias                                                                        ifeval\n",
      "limit                                                                           NAN\n",
      "json_path                         eval_results/full_patching/meta-llama__Llama-3...\n",
      "dataset_str                                                                 unknown\n",
      "finetuning_params                                                               NAN\n",
      "total_batch_size                                                                NAN\n",
      "learning_rate                                                                   NAN\n",
      "main_loss_type                                                                  NAN\n",
      "embedding_init_strategy                                                         NAN\n",
      "num_new_tokens                                                                  NAN\n",
      "unfreeze_params_steps                                                           NAN\n",
      "finetune_params_prefreeze                                                       NAN\n",
      "dataset                                                                         NAN\n",
      "tokenizer_path                                                                  NAN\n",
      "seed                                                                            NAN\n",
      "reset_optimizer                                                                 NAN\n",
      "warmup_steps                                                                    NAN\n",
      "lr_schedule                                                                     NAN\n",
      "warmup_steps_prefreeze                                                          NAN\n",
      "lr_schedule_prefreeze                                                           NAN\n",
      "Name: 37, dtype: object\n",
      "                                 model_name_sanitized  \\\n",
      "1                            meta-llama__Llama-3.2-3B   \n",
      "57                           meta-llama__Llama-3.2-3B   \n",
      "32                           meta-llama__Llama-3.2-3B   \n",
      "22  output__full_patching__2ecfe6ea-Llama-3.2-3B-m...   \n",
      "40  output__full_patching__d1f6703c-Llama-3.2-3B-m...   \n",
      "78                           meta-llama__Llama-3.2-3B   \n",
      "38  output__full_patching__e8e7ce8d-Llama-3.2-3B-m...   \n",
      "31  output__full_patching__461b9dfe-Llama-3.2-3B-m...   \n",
      "44                           meta-llama__Llama-3.2-3B   \n",
      "8                            meta-llama__Llama-3.2-3B   \n",
      "41  output__full_patching__bd539cf5-Llama-3.2-3B-m...   \n",
      "56  output__full_patching__63d35748-Llama-3.2-3B-m...   \n",
      "55  output__full_patching__d2d49ae1-Llama-3.2-3B-m...   \n",
      "43  output__full_patching__daac169e-Llama-3.2-3B-m...   \n",
      "0   output__full_patching__ba7c8b60-Llama-3.2-3B-m...   \n",
      "20  output__full_patching__3c315a04-Llama-3.2-3B-m...   \n",
      "77  output__full_patching__a84197a7-Llama-3.2-3B-m...   \n",
      "12                           meta-llama__Llama-3.2-3B   \n",
      "42  output__full_patching__234bdfaa-Llama-3.2-3B-m...   \n",
      "34  output__full_patching__8760b763-Llama-3.2-3B-m...   \n",
      "76  output__full_patching__508aaf68-Llama-3.2-3B-m...   \n",
      "2                   meta-llama__Llama-3.2-3B-Instruct   \n",
      "58                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "33                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "45                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "79                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "74  output__full_patching__8ce7f0d6-Llama-3.2-3B-I...   \n",
      "46                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "39                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "15  output__full_patching__4d6edc1e-Llama-3.2-3B-I...   \n",
      "11  output__full_patching__e5175fd2-Llama-3.2-3B-I...   \n",
      "18                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "28  output__full_patching__e93b6877-Llama-3.2-3B-I...   \n",
      "66                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "47  output__full_patching__8d72bbbe-Llama-3.2-3B-I...   \n",
      "48                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "14                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "53  output__full_patching__80c6810f-Llama-3.2-3B-I...   \n",
      "72                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "26                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "17                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "70                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "24                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "27                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "68                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "25                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "63                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "36                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "62                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "71                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "67                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "16  output__full_patching__4b9504d3-Llama-3.2-3B-I...   \n",
      "23  output__full_patching__12f3cea0-Llama-3.2-3B-I...   \n",
      "4                   meta-llama__Llama-3.2-3B-Instruct   \n",
      "21                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "54  output__full_patching__5c0eb32b-Llama-3.2-3B-I...   \n",
      "65                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "3                   meta-llama__Llama-3.2-3B-Instruct   \n",
      "75  output__full_patching__f276bbe3-Llama-3.2-3B-I...   \n",
      "35                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "69                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "10                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "9                   meta-llama__Llama-3.2-3B-Instruct   \n",
      "13                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "7                   meta-llama__Llama-3.2-3B-Instruct   \n",
      "64                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "29                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "5                   meta-llama__Llama-3.2-3B-Instruct   \n",
      "59                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "61                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "51                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "50                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "6                   meta-llama__Llama-3.2-3B-Instruct   \n",
      "60                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "73  output__full_patching__cb47c58d-Llama-3.2-3B-I...   \n",
      "49                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "19  output__full_patching__c5764fce-Llama-3.2-3B-I...   \n",
      "30                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "52                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "37                  meta-llama__Llama-3.2-3B-Instruct   \n",
      "\n",
      "                                   trained_model_name             model_name  \\\n",
      "1                    4eb8fba9-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
      "57                    e45070ca-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
      "32                    0a90e719-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
      "22  output__full_patching__2ecfe6ea-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "40  output__full_patching__d1f6703c-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "78                      74ed1b01-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
      "38  output__full_patching__e8e7ce8d-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "31  output__full_patching__461b9dfe-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "44                     461ac385-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
      "8                     dfe1b80e-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
      "41  output__full_patching__bd539cf5-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "56  output__full_patching__63d35748-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "55  output__full_patching__d2d49ae1-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "43  output__full_patching__daac169e-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "0   output__full_patching__ba7c8b60-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "20  output__full_patching__3c315a04-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "77  output__full_patching__a84197a7-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "12                           meta-llama__Llama-3.2-3B           Llama-3.2-3B   \n",
      "42  output__full_patching__234bdfaa-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "34  output__full_patching__8760b763-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "76  output__full_patching__508aaf68-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
      "2                    4eb8fba9-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
      "58                    e45070ca-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
      "33                    0a90e719-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
      "45                     461ac385-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
      "79                      74ed1b01-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
      "74  output__full_patching__8ce7f0d6-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
      "46          9df250b6-Llama-3.2-3B-Instruct-mixed-1000  Llama-3.2-3B-Instruct   \n",
      "39           958352ea-Llama-3.2-3B-Instruct-mixed-100  Llama-3.2-3B-Instruct   \n",
      "15  output__full_patching__4d6edc1e-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
      "11  output__full_patching__e5175fd2-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
      "18           0a4e765d-Llama-3.2-3B-Instruct-mixed-500  Llama-3.2-3B-Instruct   \n",
      "28  output__full_patching__e93b6877-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
      "66            9e5180ab-Llama-3.2-3B-Instruct-mixed-10  Llama-3.2-3B-Instruct   \n",
      "47  output__full_patching__8d72bbbe-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
      "48             368f5252-Llama-3.2-3B-Instruct-mixed-0  Llama-3.2-3B-Instruct   \n",
      "14          6e1e953a-Llama-3.2-3B-Instruct-mixed-1000  Llama-3.2-3B-Instruct   \n",
      "53  output__full_patching__80c6810f-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
      "72                   4eb8fba9-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
      "26           755abfca-Llama-3.2-3B-Instruct-mixed-500  Llama-3.2-3B-Instruct   \n",
      "17                   461b9dfe-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
      "70                   63d35748-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
      "24                   ba7c8b60-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
      "27                    234bdfaa-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
      "68                    dfe1b80e-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
      "25                   ba7c8b60-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
      "63                    e45070ca-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
      "36                    0a90e719-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
      "62                    e45070ca-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
      "71                   4eb8fba9-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
      "67                    dfe1b80e-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
      "16  output__full_patching__4b9504d3-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
      "23  output__full_patching__12f3cea0-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
      "4                     508aaf68-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
      "21                    2ecfe6ea-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
      "54  output__full_patching__5c0eb32b-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
      "65                    e8e7ce8d-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
      "3                     508aaf68-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
      "75  output__full_patching__f276bbe3-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
      "35                    0a90e719-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
      "69                   63d35748-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
      "10                    3c315a04-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
      "9                     3c315a04-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
      "13                     bd539cf5-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
      "7            2916d55c-Llama-3.2-3B-Instruct-mixed-100  Llama-3.2-3B-Instruct   \n",
      "64                     daac169e-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
      "29                     461ac385-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
      "5                      d2d49ae1-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
      "59                      d1f6703c-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
      "61                      74ed1b01-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
      "51                      a84197a7-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
      "50                      8760b763-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
      "6                      d2d49ae1-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
      "60                      74ed1b01-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
      "73  output__full_patching__cb47c58d-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
      "49                      8760b763-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
      "19  output__full_patching__c5764fce-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
      "30                     461ac385-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
      "52                      a84197a7-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
      "37                  meta-llama__Llama-3.2-3B-Instruct  Llama-3.2-3B-Instruct   \n",
      "\n",
      "   embeddings_init task_name exact_match exact_match_stderr  \\\n",
      "1              NAN     mixed         NAN                NAN   \n",
      "57             NAN     mixed         NAN                NAN   \n",
      "32             NAN     mixed         NAN                NAN   \n",
      "22             NAN     mixed         NAN                NAN   \n",
      "40             NAN     mixed         NAN                NAN   \n",
      "78             NAN     mixed         NAN                NAN   \n",
      "38             NAN     mixed         NAN                NAN   \n",
      "31             NAN     mixed         NAN                NAN   \n",
      "44             NAN     mixed         NAN                NAN   \n",
      "8              NAN     mixed         NAN                NAN   \n",
      "41             NAN     mixed         NAN                NAN   \n",
      "56             NAN     mixed         NAN                NAN   \n",
      "55             NAN     mixed         NAN                NAN   \n",
      "43             NAN     mixed         NAN                NAN   \n",
      "0              NAN     mixed         NAN                NAN   \n",
      "20             NAN     mixed         NAN                NAN   \n",
      "77             NAN     mixed         NAN                NAN   \n",
      "12             NAN  baseline         NAN                NAN   \n",
      "42             NAN     mixed         NAN                NAN   \n",
      "34             NAN     mixed         NAN                NAN   \n",
      "76             NAN     mixed         NAN                NAN   \n",
      "2              NAN     mixed         NAN                NAN   \n",
      "58             NAN     mixed         NAN                NAN   \n",
      "33             NAN     mixed         NAN                NAN   \n",
      "45             NAN     mixed         NAN                NAN   \n",
      "79             NAN     mixed         NAN                NAN   \n",
      "74             NAN     mixed         NAN                NAN   \n",
      "46             NAN     mixed         NAN                NAN   \n",
      "39             NAN     mixed         NAN                NAN   \n",
      "15             NAN     mixed         NAN                NAN   \n",
      "11             NAN     mixed         NAN                NAN   \n",
      "18             NAN     mixed         NAN                NAN   \n",
      "28             NAN     mixed         NAN                NAN   \n",
      "66             NAN     mixed         NAN                NAN   \n",
      "47             NAN     mixed         NAN                NAN   \n",
      "48             NAN     mixed         NAN                NAN   \n",
      "14             NAN     mixed         NAN                NAN   \n",
      "53             NAN     mixed         NAN                NAN   \n",
      "72        new_only     mixed         NAN                NAN   \n",
      "26             NAN     mixed         NAN                NAN   \n",
      "17        new_only     mixed         NAN                NAN   \n",
      "70        new_only     mixed         NAN                NAN   \n",
      "24        new_only     mixed         NAN                NAN   \n",
      "27        new_only     mixed         NAN                NAN   \n",
      "68        new_only     mixed         NAN                NAN   \n",
      "25        new_only     mixed         NAN                NAN   \n",
      "63        new_only     mixed         NAN                NAN   \n",
      "36        new_only     mixed         NAN                NAN   \n",
      "62        new_only     mixed         NAN                NAN   \n",
      "71        new_only     mixed         NAN                NAN   \n",
      "67        new_only     mixed         NAN                NAN   \n",
      "16             NAN     mixed         NAN                NAN   \n",
      "23             NAN     mixed         NAN                NAN   \n",
      "4         new_only     mixed         NAN                NAN   \n",
      "21        new_only     mixed         NAN                NAN   \n",
      "54             NAN     mixed         NAN                NAN   \n",
      "65        new_only     mixed         NAN                NAN   \n",
      "3         new_only     mixed         NAN                NAN   \n",
      "75             NAN     mixed         NAN                NAN   \n",
      "35        new_only     mixed         NAN                NAN   \n",
      "69        new_only     mixed         NAN                NAN   \n",
      "10        new_only     mixed         NAN                NAN   \n",
      "9         new_only     mixed         NAN                NAN   \n",
      "13        new_only     mixed         NAN                NAN   \n",
      "7              NAN     mixed         NAN                NAN   \n",
      "64        new_only     mixed         NAN                NAN   \n",
      "29        new_only     mixed         NAN                NAN   \n",
      "5         new_only     mixed         NAN                NAN   \n",
      "59        new_only     mixed         NAN                NAN   \n",
      "61        new_only     mixed         NAN                NAN   \n",
      "51        new_only     mixed         NAN                NAN   \n",
      "50        new_only     mixed         NAN                NAN   \n",
      "6         new_only     mixed         NAN                NAN   \n",
      "60        new_only     mixed         NAN                NAN   \n",
      "73             NAN     mixed         NAN                NAN   \n",
      "49        new_only     mixed         NAN                NAN   \n",
      "19             NAN     mixed         NAN                NAN   \n",
      "30        new_only     mixed         NAN                NAN   \n",
      "52        new_only     mixed         NAN                NAN   \n",
      "37             NAN  baseline         NAN                NAN   \n",
      "\n",
      "    prompt_level_strict_acc  prompt_level_strict_acc_stderr  \\\n",
      "1                  0.134935                        0.014702   \n",
      "57                 0.138632                        0.014871   \n",
      "32                 0.138632                        0.014871   \n",
      "22                 0.142329                        0.015035   \n",
      "40                 0.149723                        0.015354   \n",
      "78                 0.153420                        0.015509   \n",
      "38                 0.166359                        0.016026   \n",
      "31                 0.171904                        0.016236   \n",
      "44                 0.171904                        0.016236   \n",
      "8                  0.177449                        0.016441   \n",
      "41                 0.177449                        0.016441   \n",
      "56                 0.184843                        0.016704   \n",
      "55                 0.186691                        0.016768   \n",
      "43                 0.188540                        0.016832   \n",
      "0                  0.192237                        0.016958   \n",
      "20                 0.194085                        0.017019   \n",
      "77                 0.195933                        0.017081   \n",
      "12                 0.195933                        0.017081   \n",
      "42                 0.197782                        0.017141   \n",
      "34                 0.205176                        0.017378   \n",
      "76                 0.207024                        0.017436   \n",
      "2                  0.327172                        0.020190   \n",
      "58                 0.343808                        0.020440   \n",
      "33                 0.390018                        0.020990   \n",
      "45                 0.414048                        0.021196   \n",
      "79                 0.415896                        0.021210   \n",
      "74                 0.449168                        0.021405   \n",
      "46                 0.484288                        0.021506   \n",
      "39                 0.486137                        0.021508   \n",
      "15                 0.510166                        0.021512   \n",
      "11                 0.519409                        0.021500   \n",
      "18                 0.523105                        0.021494   \n",
      "28                 0.528651                        0.021481   \n",
      "66                 0.534196                        0.021466   \n",
      "47                 0.554529                        0.021388   \n",
      "48                 0.556377                        0.021379   \n",
      "14                 0.606285                        0.021025   \n",
      "53                 0.621072                        0.020876   \n",
      "72                 0.628466                        0.020794   \n",
      "26                 0.628466                        0.020794   \n",
      "17                 0.632163                        0.020751   \n",
      "70                 0.639556                        0.020661   \n",
      "24                 0.643253                        0.020615   \n",
      "27                 0.643253                        0.020615   \n",
      "68                 0.646950                        0.020566   \n",
      "25                 0.648799                        0.020542   \n",
      "63                 0.650647                        0.020517   \n",
      "36                 0.650647                        0.020517   \n",
      "62                 0.652495                        0.020491   \n",
      "71                 0.652495                        0.020491   \n",
      "67                 0.654344                        0.020466   \n",
      "16                 0.656192                        0.020440   \n",
      "23                 0.656192                        0.020440   \n",
      "4                  0.658041                        0.020413   \n",
      "21                 0.659889                        0.020387   \n",
      "54                 0.659889                        0.020387   \n",
      "65                 0.661738                        0.020360   \n",
      "3                  0.663586                        0.020332   \n",
      "75                 0.674677                        0.020161   \n",
      "35                 0.674677                        0.020161   \n",
      "69                 0.676525                        0.020131   \n",
      "10                 0.678373                        0.020101   \n",
      "9                  0.678373                        0.020101   \n",
      "13                 0.685767                        0.019976   \n",
      "7                  0.685767                        0.019976   \n",
      "64                 0.685767                        0.019976   \n",
      "29                 0.689464                        0.019912   \n",
      "5                  0.693161                        0.019846   \n",
      "59                 0.696858                        0.019779   \n",
      "61                 0.696858                        0.019779   \n",
      "51                 0.696858                        0.019779   \n",
      "50                 0.696858                        0.019779   \n",
      "6                  0.698706                        0.019744   \n",
      "60                 0.704251                        0.019639   \n",
      "73                 0.704251                        0.019639   \n",
      "49                 0.704251                        0.019639   \n",
      "19                 0.704251                        0.019639   \n",
      "30                 0.704251                        0.019639   \n",
      "52                 0.704251                        0.019639   \n",
      "37                 0.706100                        0.019604   \n",
      "\n",
      "    inst_level_strict_acc inst_level_strict_acc_stderr  \\\n",
      "1                0.262590                          NAN   \n",
      "57               0.270983                          NAN   \n",
      "32               0.264988                          NAN   \n",
      "22               0.260192                          NAN   \n",
      "40               0.267386                          NAN   \n",
      "78               0.285372                          NAN   \n",
      "38               0.276978                          NAN   \n",
      "31               0.282974                          NAN   \n",
      "44               0.299760                          NAN   \n",
      "8                0.288969                          NAN   \n",
      "41               0.288969                          NAN   \n",
      "56               0.293765                          NAN   \n",
      "55               0.316547                          NAN   \n",
      "43               0.321343                          NAN   \n",
      "0                0.305755                          NAN   \n",
      "20               0.320144                          NAN   \n",
      "77               0.324940                          NAN   \n",
      "12               0.324940                          NAN   \n",
      "42               0.309353                          NAN   \n",
      "34               0.327338                          NAN   \n",
      "76               0.322542                          NAN   \n",
      "2                0.468825                          NAN   \n",
      "58               0.482014                          NAN   \n",
      "33               0.534772                          NAN   \n",
      "45               0.541966                          NAN   \n",
      "79               0.558753                          NAN   \n",
      "74               0.577938                          NAN   \n",
      "46               0.593525                          NAN   \n",
      "39               0.606715                          NAN   \n",
      "15               0.616307                          NAN   \n",
      "11               0.628297                          NAN   \n",
      "18               0.622302                          NAN   \n",
      "28               0.647482                          NAN   \n",
      "66               0.637890                          NAN   \n",
      "47               0.661871                          NAN   \n",
      "48               0.664269                          NAN   \n",
      "14               0.703837                          NAN   \n",
      "53               0.723022                          NAN   \n",
      "72               0.735012                          NAN   \n",
      "26               0.730216                          NAN   \n",
      "17               0.733813                          NAN   \n",
      "70               0.743405                          NAN   \n",
      "24               0.738609                          NAN   \n",
      "27               0.748201                          NAN   \n",
      "68               0.741007                          NAN   \n",
      "25               0.742206                          NAN   \n",
      "63               0.743405                          NAN   \n",
      "36               0.749400                          NAN   \n",
      "62               0.749400                          NAN   \n",
      "71               0.749400                          NAN   \n",
      "67               0.750600                          NAN   \n",
      "16               0.755396                          NAN   \n",
      "23               0.751799                          NAN   \n",
      "4                0.752998                          NAN   \n",
      "21               0.760192                          NAN   \n",
      "54               0.758993                          NAN   \n",
      "65               0.755396                          NAN   \n",
      "3                0.757794                          NAN   \n",
      "75               0.763789                          NAN   \n",
      "35               0.770983                          NAN   \n",
      "69               0.770983                          NAN   \n",
      "10               0.769784                          NAN   \n",
      "9                0.767386                          NAN   \n",
      "13               0.778177                          NAN   \n",
      "7                0.767386                          NAN   \n",
      "64               0.770983                          NAN   \n",
      "29               0.779376                          NAN   \n",
      "5                0.778177                          NAN   \n",
      "59               0.781775                          NAN   \n",
      "61               0.781775                          NAN   \n",
      "51               0.781775                          NAN   \n",
      "50               0.781775                          NAN   \n",
      "6                0.779376                          NAN   \n",
      "60               0.784173                          NAN   \n",
      "73               0.784173                          NAN   \n",
      "49               0.784173                          NAN   \n",
      "19               0.784173                          NAN   \n",
      "30               0.788969                          NAN   \n",
      "52               0.784173                          NAN   \n",
      "37               0.790168                          NAN   \n",
      "\n",
      "    prompt_level_loose_acc  prompt_level_loose_acc_stderr  \\\n",
      "1                 0.146026                       0.015196   \n",
      "57                0.149723                       0.015354   \n",
      "32                0.164510                       0.015954   \n",
      "22                0.155268                       0.015585   \n",
      "40                0.162662                       0.015882   \n",
      "78                0.166359                       0.016026   \n",
      "38                0.186691                       0.016768   \n",
      "31                0.190388                       0.016895   \n",
      "44                0.179298                       0.016508   \n",
      "8                 0.184843                       0.016704   \n",
      "41                0.199630                       0.017201   \n",
      "56                0.192237                       0.016958   \n",
      "55                0.203327                       0.017320   \n",
      "43                0.207024                       0.017436   \n",
      "0                 0.207024                       0.017436   \n",
      "20                0.210721                       0.017550   \n",
      "77                0.218115                       0.017771   \n",
      "12                0.218115                       0.017771   \n",
      "42                0.212569                       0.017606   \n",
      "34                0.223660                       0.017932   \n",
      "76                0.227357                       0.018036   \n",
      "2                 0.414048                       0.021196   \n",
      "58                0.428835                       0.021298   \n",
      "33                0.482440                       0.021503   \n",
      "45                0.504621                       0.021516   \n",
      "79                0.517560                       0.021503   \n",
      "74                0.493530                       0.021515   \n",
      "46                0.534196                       0.021466   \n",
      "39                0.537893                       0.021455   \n",
      "15                0.548983                       0.021413   \n",
      "11                0.574861                       0.021274   \n",
      "18                0.563771                       0.021341   \n",
      "28                0.602588                       0.021059   \n",
      "66                0.582255                       0.021223   \n",
      "47                0.609982                       0.020990   \n",
      "48                0.621072                       0.020876   \n",
      "14                0.652495                       0.020491   \n",
      "53                0.661738                       0.020360   \n",
      "72                0.670980                       0.020219   \n",
      "26                0.674677                       0.020161   \n",
      "17                0.669131                       0.020248   \n",
      "70                0.678373                       0.020101   \n",
      "24                0.670980                       0.020219   \n",
      "27                0.667283                       0.020277   \n",
      "68                0.682070                       0.020039   \n",
      "25                0.685767                       0.019976   \n",
      "63                0.693161                       0.019846   \n",
      "36                0.713494                       0.019457   \n",
      "62                0.689464                       0.019912   \n",
      "71                0.689464                       0.019912   \n",
      "67                0.695009                       0.019813   \n",
      "16                0.700555                       0.019710   \n",
      "23                0.715342                       0.019419   \n",
      "4                 0.707948                       0.019567   \n",
      "21                0.706100                       0.019604   \n",
      "54                0.707948                       0.019567   \n",
      "65                0.687616                       0.019944   \n",
      "3                 0.719039                       0.019342   \n",
      "75                0.720887                       0.019303   \n",
      "35                0.722736                       0.019264   \n",
      "69                0.707948                       0.019567   \n",
      "10                0.726433                       0.019184   \n",
      "9                 0.730129                       0.019102   \n",
      "13                0.741220                       0.018847   \n",
      "7                 0.739372                       0.018891   \n",
      "64                0.730129                       0.019102   \n",
      "29                0.733826                       0.019019   \n",
      "5                 0.737523                       0.018934   \n",
      "59                0.746765                       0.018714   \n",
      "61                0.746765                       0.018714   \n",
      "51                0.746765                       0.018714   \n",
      "50                0.746765                       0.018714   \n",
      "6                 0.741220                       0.018847   \n",
      "60                0.750462                       0.018622   \n",
      "73                0.750462                       0.018622   \n",
      "49                0.750462                       0.018622   \n",
      "19                0.750462                       0.018622   \n",
      "30                0.748614                       0.018668   \n",
      "52                0.750462                       0.018622   \n",
      "37                0.750462                       0.018622   \n",
      "\n",
      "    inst_level_loose_acc inst_level_loose_acc_stderr  compression_ratio  \\\n",
      "1               0.273381                         NAN           0.101597   \n",
      "57              0.280576                         NAN           0.087240   \n",
      "32              0.292566                         NAN           0.035638   \n",
      "22              0.282974                         NAN           0.040349   \n",
      "40              0.284173                         NAN          -0.001223   \n",
      "78              0.303357                         NAN          -0.000226   \n",
      "38              0.296163                         NAN           0.089000   \n",
      "31              0.320144                         NAN           0.121145   \n",
      "44              0.312950                         NAN           0.017405   \n",
      "8               0.296163                         NAN           0.614980   \n",
      "41              0.316547                         NAN           0.014895   \n",
      "56              0.316547                         NAN           0.158073   \n",
      "55              0.341727                         NAN           0.006792   \n",
      "43              0.345324                         NAN           0.008682   \n",
      "0               0.329736                         NAN           0.133380   \n",
      "20              0.342926                         NAN           0.039745   \n",
      "77              0.350120                         NAN           0.001207   \n",
      "12              0.350120                         NAN           0.001207   \n",
      "42              0.338129                         NAN           0.147659   \n",
      "34              0.351319                         NAN          -0.000266   \n",
      "76              0.347722                         NAN           0.031562   \n",
      "2               0.552758                         NAN           0.016155   \n",
      "58              0.557554                         NAN           0.009105   \n",
      "33              0.609113                         NAN           0.017502   \n",
      "45              0.624700                         NAN          -0.001514   \n",
      "79              0.640288                         NAN           0.012280   \n",
      "74              0.623501                         NAN           0.075506   \n",
      "46              0.643885                         NAN           0.081260   \n",
      "39              0.653477                         NAN           0.028351   \n",
      "15              0.658273                         NAN           0.060745   \n",
      "11              0.679856                         NAN           0.033028   \n",
      "18              0.660671                         NAN           0.068671   \n",
      "28              0.706235                         NAN           0.001851   \n",
      "66              0.681055                         NAN           0.000824   \n",
      "47              0.714628                         NAN          -0.010000   \n",
      "48              0.724221                         NAN          -0.011691   \n",
      "14              0.743405                         NAN           0.042820   \n",
      "53              0.755396                         NAN           0.044952   \n",
      "72              0.769784                         NAN           0.001885   \n",
      "26              0.768585                         NAN           0.036607   \n",
      "17              0.767386                         NAN           0.000799   \n",
      "70              0.776978                         NAN           0.002219   \n",
      "24              0.766187                         NAN           0.007573   \n",
      "27              0.773381                         NAN           0.002964   \n",
      "68              0.769784                         NAN          -0.000689   \n",
      "25              0.773381                         NAN           0.007476   \n",
      "63              0.781775                         NAN          -0.001883   \n",
      "36              0.796163                         NAN          -0.007362   \n",
      "62              0.786571                         NAN          -0.001875   \n",
      "71              0.780576                         NAN           0.003067   \n",
      "67              0.786571                         NAN          -0.000504   \n",
      "16              0.790168                         NAN           0.037358   \n",
      "23              0.798561                         NAN          -0.001833   \n",
      "4               0.793765                         NAN          -0.005003   \n",
      "21              0.799760                         NAN          -0.007376   \n",
      "54              0.798561                         NAN           0.022516   \n",
      "65              0.781775                         NAN          -0.000953   \n",
      "3               0.799760                         NAN          -0.004844   \n",
      "75              0.800959                         NAN          -0.001153   \n",
      "35              0.806954                         NAN          -0.007079   \n",
      "69              0.797362                         NAN           0.002623   \n",
      "10              0.806954                         NAN          -0.002878   \n",
      "9               0.808153                         NAN          -0.002788   \n",
      "13              0.822542                         NAN          -0.010551   \n",
      "7               0.810552                         NAN           0.020501   \n",
      "64              0.809353                         NAN          -0.008610   \n",
      "29              0.814149                         NAN          -0.010191   \n",
      "5               0.814149                         NAN          -0.008155   \n",
      "59              0.818945                         NAN          -0.010761   \n",
      "61              0.818945                         NAN          -0.010761   \n",
      "51              0.818945                         NAN          -0.010761   \n",
      "50              0.818945                         NAN          -0.010761   \n",
      "6               0.816547                         NAN          -0.008277   \n",
      "60              0.820144                         NAN          -0.010785   \n",
      "73              0.820144                         NAN          -0.010785   \n",
      "49              0.820144                         NAN          -0.010785   \n",
      "19              0.820144                         NAN          -0.010785   \n",
      "30              0.826139                         NAN          -0.010125   \n",
      "52              0.820144                         NAN          -0.010785   \n",
      "37              0.822542                         NAN          -0.010776   \n",
      "\n",
      "    learning_ratio  theoretical_compression_ratio   alias limit  \\\n",
      "1         0.022770                       0.122494  ifeval   NAN   \n",
      "57        0.012155                       0.100254  ifeval   NAN   \n",
      "32        0.006736                       0.042507  ifeval   NAN   \n",
      "22        0.006330                       0.046784  ifeval   NAN   \n",
      "40        0.001373                       0.000000  ifeval   NAN   \n",
      "78        0.000229                       0.000000  ifeval   NAN   \n",
      "38        0.010453                       0.100258  ifeval   NAN   \n",
      "31        0.048861                       0.147647  ifeval   NAN   \n",
      "44        0.001320                       0.018738  ifeval   NAN   \n",
      "8         0.019093                       0.632944  ifeval   NAN   \n",
      "41        0.002959                       0.017731  ifeval   NAN   \n",
      "56        0.043921                       0.204460  ifeval   NAN   \n",
      "55        0.018304                       0.016143  ifeval   NAN   \n",
      "43        0.015031                       0.015013  ifeval   NAN   \n",
      "0         0.034137                       0.170377  ifeval   NAN   \n",
      "20        0.028531                       0.064240  ifeval   NAN   \n",
      "77        0.004820                       0.000000  ifeval   NAN   \n",
      "12        0.004820                       0.000000  ifeval   NAN   \n",
      "42        0.043606                       0.192847  ifeval   NAN   \n",
      "34        0.006193                       0.000000  ifeval   NAN   \n",
      "76        0.040201                       0.068354  ifeval   NAN   \n",
      "2        -0.000890                       0.014179  ifeval   NAN   \n",
      "58        0.011711                       0.013769  ifeval   NAN   \n",
      "33       -0.001952                       0.010700  ifeval   NAN   \n",
      "45        0.021735                       0.003100  ifeval   NAN   \n",
      "79       -0.002480                       0.000000  ifeval   NAN   \n",
      "74        0.038008                       0.114864  ifeval   NAN   \n",
      "46        0.058773                       0.118345  ifeval   NAN   \n",
      "39        0.025881                       0.048398  ifeval   NAN   \n",
      "15        0.040485                       0.098851  ifeval   NAN   \n",
      "11        0.020466                       0.052770  ifeval   NAN   \n",
      "18        0.034358                       0.099375  ifeval   NAN   \n",
      "28        0.017905                       0.014476  ifeval   NAN   \n",
      "66        0.039284                       0.013212  ifeval   NAN   \n",
      "47        0.011621                       0.000000  ifeval   NAN   \n",
      "48        0.017971                       0.000000  ifeval   NAN   \n",
      "14        0.105766                       0.150301  ifeval   NAN   \n",
      "53        0.106657                       0.155445  ifeval   NAN   \n",
      "72        0.145274                       0.146620  ifeval   NAN   \n",
      "26        0.088294                       0.126927  ifeval   NAN   \n",
      "17        0.145470                       0.145546  ifeval   NAN   \n",
      "70        0.141326                       0.143011  ifeval   NAN   \n",
      "24        0.139264                       0.146980  ifeval   NAN   \n",
      "27        0.116043                       0.118194  ifeval   NAN   \n",
      "68        0.118552                       0.116691  ifeval   NAN   \n",
      "25        0.136249                       0.143803  ifeval   NAN   \n",
      "63        0.121565                       0.118431  ifeval   NAN   \n",
      "36        0.068735                       0.059582  ifeval   NAN   \n",
      "62        0.120766                       0.117677  ifeval   NAN   \n",
      "71        0.142038                       0.144711  ifeval   NAN   \n",
      "67        0.119391                       0.117738  ifeval   NAN   \n",
      "16        0.090833                       0.130479  ifeval   NAN   \n",
      "23        0.021714                       0.018278  ifeval   NAN   \n",
      "4         0.066387                       0.059817  ifeval   NAN   \n",
      "21        0.068191                       0.059037  ifeval   NAN   \n",
      "54        0.045404                       0.067439  ifeval   NAN   \n",
      "65        0.121166                       0.119027  ifeval   NAN   \n",
      "3         0.065750                       0.059344  ifeval   NAN   \n",
      "75        0.021340                       0.018647  ifeval   NAN   \n",
      "35        0.067890                       0.059081  ifeval   NAN   \n",
      "69        0.139993                       0.142160  ifeval   NAN   \n",
      "10        0.063872                       0.059529  ifeval   NAN   \n",
      "9         0.063923                       0.059693  ifeval   NAN   \n",
      "13        0.028006                       0.015644  ifeval   NAN   \n",
      "7         0.046151                       0.066113  ifeval   NAN   \n",
      "64        0.026663                       0.016339  ifeval   NAN   \n",
      "29        0.027429                       0.015465  ifeval   NAN   \n",
      "5         0.025803                       0.015955  ifeval   NAN   \n",
      "59        0.012375                       0.000000  ifeval   NAN   \n",
      "61        0.012375                       0.000000  ifeval   NAN   \n",
      "51        0.012375                       0.000000  ifeval   NAN   \n",
      "50        0.012375                       0.000000  ifeval   NAN   \n",
      "6         0.025612                       0.015627  ifeval   NAN   \n",
      "60        0.012398                       0.000000  ifeval   NAN   \n",
      "73        0.012398                       0.000000  ifeval   NAN   \n",
      "49        0.012398                       0.000000  ifeval   NAN   \n",
      "19        0.012398                       0.000000  ifeval   NAN   \n",
      "30        0.027598                       0.015702  ifeval   NAN   \n",
      "52        0.012398                       0.000000  ifeval   NAN   \n",
      "37        0.012392                       0.000000  ifeval   NAN   \n",
      "\n",
      "                                            json_path           dataset_str  \\\n",
      "1   eval_results/full_patching/4eb8fba9-Llama-3.2-...               default   \n",
      "57  eval_results/full_patching/e45070ca-Llama-3.2-...               default   \n",
      "32  eval_results/full_patching/0a90e719-Llama-3.2-...               default   \n",
      "22  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "40  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "78  eval_results/full_patching/74ed1b01-Llama-3.2-...               default   \n",
      "38  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "31  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "44  eval_results/full_patching/461ac385-Llama-3.2-...               default   \n",
      "8   eval_results/full_patching/dfe1b80e-Llama-3.2-...               default   \n",
      "41  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "56  eval_results/full_patching/output__full_patchi...               default   \n",
      "55  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "43  eval_results/full_patching/output__full_patchi...               default   \n",
      "0   eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "20  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "77  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "12  eval_results/full_patching/meta-llama__Llama-3...               unknown   \n",
      "42  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "34  eval_results/full_patching/output__full_patchi...               default   \n",
      "76  eval_results/full_patching/output__full_patchi...               default   \n",
      "2   eval_results/full_patching/4eb8fba9-Llama-3.2-...               default   \n",
      "58  eval_results/full_patching/e45070ca-Llama-3.2-...               default   \n",
      "33  eval_results/full_patching/0a90e719-Llama-3.2-...               default   \n",
      "45  eval_results/full_patching/461ac385-Llama-3.2-...               default   \n",
      "79  eval_results/full_patching/74ed1b01-Llama-3.2-...               default   \n",
      "74  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "46  eval_results/full_patching/9df250b6-Llama-3.2-...               default   \n",
      "39  eval_results/full_patching/958352ea-Llama-3.2-...               default   \n",
      "15  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "11  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "18  eval_results/full_patching/0a4e765d-Llama-3.2-...               default   \n",
      "28  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "66  eval_results/full_patching/9e5180ab-Llama-3.2-...               default   \n",
      "47  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "48  eval_results/full_patching/368f5252-Llama-3.2-...               default   \n",
      "14  eval_results/full_patching/6e1e953a-Llama-3.2-...               default   \n",
      "53  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "72  eval_results/full_patching/4eb8fba9-Llama-3.2-...               default   \n",
      "26  eval_results/full_patching/755abfca-Llama-3.2-...               default   \n",
      "17  eval_results/full_patching/461b9dfe-Llama-3.2-...  translation, default   \n",
      "70  eval_results/full_patching/63d35748-Llama-3.2-...               default   \n",
      "24  eval_results/full_patching/ba7c8b60-Llama-3.2-...  translation, default   \n",
      "27  eval_results/full_patching/234bdfaa-Llama-3.2-...  translation, default   \n",
      "68  eval_results/full_patching/dfe1b80e-Llama-3.2-...               default   \n",
      "25  eval_results/full_patching/ba7c8b60-Llama-3.2-...  translation, default   \n",
      "63  eval_results/full_patching/e45070ca-Llama-3.2-...               default   \n",
      "36  eval_results/full_patching/0a90e719-Llama-3.2-...               default   \n",
      "62  eval_results/full_patching/e45070ca-Llama-3.2-...               default   \n",
      "71  eval_results/full_patching/4eb8fba9-Llama-3.2-...               default   \n",
      "67  eval_results/full_patching/dfe1b80e-Llama-3.2-...               default   \n",
      "16  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "23  eval_results/full_patching/output__full_patchi...               default   \n",
      "4   eval_results/full_patching/508aaf68-Llama-3.2-...               default   \n",
      "21  eval_results/full_patching/2ecfe6ea-Llama-3.2-...  translation, default   \n",
      "54  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "65  eval_results/full_patching/e8e7ce8d-Llama-3.2-...  translation, default   \n",
      "3   eval_results/full_patching/508aaf68-Llama-3.2-...               default   \n",
      "75  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "35  eval_results/full_patching/0a90e719-Llama-3.2-...               default   \n",
      "69  eval_results/full_patching/63d35748-Llama-3.2-...               default   \n",
      "10  eval_results/full_patching/3c315a04-Llama-3.2-...  translation, default   \n",
      "9   eval_results/full_patching/3c315a04-Llama-3.2-...  translation, default   \n",
      "13  eval_results/full_patching/bd539cf5-Llama-3.2-...  translation, default   \n",
      "7   eval_results/full_patching/2916d55c-Llama-3.2-...               default   \n",
      "64  eval_results/full_patching/daac169e-Llama-3.2-...               default   \n",
      "29  eval_results/full_patching/461ac385-Llama-3.2-...               default   \n",
      "5   eval_results/full_patching/d2d49ae1-Llama-3.2-...  translation, default   \n",
      "59  eval_results/full_patching/d1f6703c-Llama-3.2-...  translation, default   \n",
      "61  eval_results/full_patching/74ed1b01-Llama-3.2-...               default   \n",
      "51  eval_results/full_patching/a84197a7-Llama-3.2-...  translation, default   \n",
      "50  eval_results/full_patching/8760b763-Llama-3.2-...               default   \n",
      "6   eval_results/full_patching/d2d49ae1-Llama-3.2-...  translation, default   \n",
      "60  eval_results/full_patching/74ed1b01-Llama-3.2-...               default   \n",
      "73  eval_results/full_patching/output__full_patchi...               default   \n",
      "49  eval_results/full_patching/8760b763-Llama-3.2-...               default   \n",
      "19  eval_results/full_patching/output__full_patchi...  translation, default   \n",
      "30  eval_results/full_patching/461ac385-Llama-3.2-...               default   \n",
      "52  eval_results/full_patching/a84197a7-Llama-3.2-...  translation, default   \n",
      "37  eval_results/full_patching/meta-llama__Llama-3...               unknown   \n",
      "\n",
      "   finetuning_params total_batch_size learning_rate main_loss_type  \\\n",
      "1         embeddings             32.0        0.0005          mixed   \n",
      "57        embeddings             32.0        0.0005          mixed   \n",
      "32        embeddings             32.0        0.0005          mixed   \n",
      "22        embeddings             32.0        0.0005          mixed   \n",
      "40        embeddings             32.0        0.0005          mixed   \n",
      "78        embeddings             32.0        0.0005          mixed   \n",
      "38        embeddings             32.0        0.0005          mixed   \n",
      "31        embeddings             32.0        0.0005          mixed   \n",
      "44        embeddings             32.0        0.0005          mixed   \n",
      "8    new_tokens_only             32.0        0.0005          mixed   \n",
      "41        embeddings             32.0        0.0005          mixed   \n",
      "56   new_tokens_only             32.0        0.0005          mixed   \n",
      "55   new_tokens_only             32.0        0.0005          mixed   \n",
      "43   new_tokens_only             32.0        0.0005          mixed   \n",
      "0    new_tokens_only             32.0        0.0005          mixed   \n",
      "20   new_tokens_only             32.0        0.0005          mixed   \n",
      "77   new_tokens_only             32.0        0.0005          mixed   \n",
      "12               NAN              NAN           NAN            NAN   \n",
      "42   new_tokens_only             32.0        0.0005          mixed   \n",
      "34   new_tokens_only             32.0        0.0005          mixed   \n",
      "76   new_tokens_only             32.0        0.0005          mixed   \n",
      "2         embeddings             32.0        0.0005          mixed   \n",
      "58        embeddings             32.0        0.0005          mixed   \n",
      "33        embeddings             32.0        0.0005          mixed   \n",
      "45        embeddings             32.0        0.0005          mixed   \n",
      "79        embeddings             32.0        0.0005          mixed   \n",
      "74        embeddings             32.0        0.0005          mixed   \n",
      "46        embeddings             32.0        0.0005          mixed   \n",
      "39        embeddings             32.0        0.0005          mixed   \n",
      "15        embeddings             32.0        0.0005          mixed   \n",
      "11        embeddings             32.0        0.0005          mixed   \n",
      "18        embeddings             32.0        0.0005          mixed   \n",
      "28        embeddings             32.0        0.0005          mixed   \n",
      "66        embeddings             32.0        0.0005          mixed   \n",
      "47        embeddings             32.0        0.0005          mixed   \n",
      "48        embeddings             32.0        0.0005          mixed   \n",
      "14   new_tokens_only             32.0        0.0005          mixed   \n",
      "53   new_tokens_only             32.0        0.0005          mixed   \n",
      "72        embeddings             32.0        0.0005          mixed   \n",
      "26   new_tokens_only             32.0        0.0005          mixed   \n",
      "17        embeddings             32.0        0.0005          mixed   \n",
      "70   new_tokens_only             32.0        0.0005          mixed   \n",
      "24   new_tokens_only             32.0        0.0005          mixed   \n",
      "27   new_tokens_only             32.0        0.0005          mixed   \n",
      "68   new_tokens_only             32.0        0.0005          mixed   \n",
      "25   new_tokens_only             32.0        0.0005          mixed   \n",
      "63        embeddings             32.0        0.0005          mixed   \n",
      "36        embeddings             32.0        0.0005          mixed   \n",
      "62        embeddings             32.0        0.0005          mixed   \n",
      "71        embeddings             32.0        0.0005          mixed   \n",
      "67   new_tokens_only             32.0        0.0005          mixed   \n",
      "16   new_tokens_only             32.0        0.0005          mixed   \n",
      "23   new_tokens_only             32.0        0.0005          mixed   \n",
      "4    new_tokens_only             32.0        0.0005          mixed   \n",
      "21        embeddings             32.0        0.0005          mixed   \n",
      "54   new_tokens_only             32.0        0.0005          mixed   \n",
      "65        embeddings             32.0        0.0005          mixed   \n",
      "3    new_tokens_only             32.0        0.0005          mixed   \n",
      "75   new_tokens_only             32.0        0.0005          mixed   \n",
      "35        embeddings             32.0        0.0005          mixed   \n",
      "69   new_tokens_only             32.0        0.0005          mixed   \n",
      "10   new_tokens_only             32.0        0.0005          mixed   \n",
      "9    new_tokens_only             32.0        0.0005          mixed   \n",
      "13        embeddings             32.0        0.0005          mixed   \n",
      "7    new_tokens_only             32.0        0.0005          mixed   \n",
      "64   new_tokens_only             32.0        0.0005          mixed   \n",
      "29        embeddings             32.0        0.0005          mixed   \n",
      "5    new_tokens_only             32.0        0.0005          mixed   \n",
      "59        embeddings             32.0        0.0005          mixed   \n",
      "61        embeddings             32.0        0.0005          mixed   \n",
      "51   new_tokens_only             32.0        0.0005          mixed   \n",
      "50   new_tokens_only             32.0        0.0005          mixed   \n",
      "6    new_tokens_only             32.0        0.0005          mixed   \n",
      "60        embeddings             32.0        0.0005          mixed   \n",
      "73   new_tokens_only             32.0        0.0005          mixed   \n",
      "49   new_tokens_only             32.0        0.0005          mixed   \n",
      "19   new_tokens_only             32.0        0.0005          mixed   \n",
      "30        embeddings             32.0        0.0005          mixed   \n",
      "52   new_tokens_only             32.0        0.0005          mixed   \n",
      "37               NAN              NAN           NAN            NAN   \n",
      "\n",
      "   embedding_init_strategy num_new_tokens unfreeze_params_steps  \\\n",
      "1                    merge         1000.0                  -1.0   \n",
      "57                   merge          500.0                  -1.0   \n",
      "32                   merge          100.0                  -1.0   \n",
      "22                   merge          100.0                  -1.0   \n",
      "40                     NAN            0.0                  -1.0   \n",
      "78                     NAN            0.0                  -1.0   \n",
      "38                   merge          500.0                  -1.0   \n",
      "31                   merge         1000.0                  -1.0   \n",
      "44                   merge           10.0                  -1.0   \n",
      "8                    merge          500.0                  -1.0   \n",
      "41                   merge           10.0                  -1.0   \n",
      "56                   merge         1000.0                  -1.0   \n",
      "55                   merge           10.0                  -1.0   \n",
      "43                   merge           10.0                  -1.0   \n",
      "0                    merge         1000.0                  -1.0   \n",
      "20                   merge          100.0                  -1.0   \n",
      "77                     NAN            0.0                  -1.0   \n",
      "12                     NAN            NAN                   NAN   \n",
      "42                   merge          500.0                  -1.0   \n",
      "34                     NAN            0.0                  -1.0   \n",
      "76                   merge          100.0                  -1.0   \n",
      "2                    merge         1000.0                  -1.0   \n",
      "58                   merge          500.0                  -1.0   \n",
      "33                   merge          100.0                  -1.0   \n",
      "45                   merge           10.0                  -1.0   \n",
      "79                     NAN            0.0                  -1.0   \n",
      "74                   merge         1000.0                  -1.0   \n",
      "46                   merge         1000.0                  -1.0   \n",
      "39                   merge          100.0                  -1.0   \n",
      "15                   merge          500.0                  -1.0   \n",
      "11                   merge          100.0                  -1.0   \n",
      "18                   merge          500.0                  -1.0   \n",
      "28                   merge           10.0                  -1.0   \n",
      "66                   merge           10.0                  -1.0   \n",
      "47                     NAN            0.0                  -1.0   \n",
      "48                     NAN            0.0                  -1.0   \n",
      "14                   merge         1000.0                  -1.0   \n",
      "53                   merge         1000.0                  -1.0   \n",
      "72                   merge         1000.0                  -1.0   \n",
      "26                   merge          500.0                  -1.0   \n",
      "17                   merge         1000.0                  -1.0   \n",
      "70                   merge         1000.0                  -1.0   \n",
      "24                   merge         1000.0                  -1.0   \n",
      "27                   merge          500.0                  -1.0   \n",
      "68                   merge          500.0                  -1.0   \n",
      "25                   merge         1000.0                  -1.0   \n",
      "63                   merge          500.0                  -1.0   \n",
      "36                   merge          100.0                  -1.0   \n",
      "62                   merge          500.0                  -1.0   \n",
      "71                   merge         1000.0                  -1.0   \n",
      "67                   merge          500.0                  -1.0   \n",
      "16                   merge          500.0                  -1.0   \n",
      "23                   merge           10.0                  -1.0   \n",
      "4                    merge          100.0                  -1.0   \n",
      "21                   merge          100.0                  -1.0   \n",
      "54                   merge          100.0                  -1.0   \n",
      "65                   merge          500.0                  -1.0   \n",
      "3                    merge          100.0                  -1.0   \n",
      "75                   merge           10.0                  -1.0   \n",
      "35                   merge          100.0                  -1.0   \n",
      "69                   merge         1000.0                  -1.0   \n",
      "10                   merge          100.0                  -1.0   \n",
      "9                    merge          100.0                  -1.0   \n",
      "13                   merge           10.0                  -1.0   \n",
      "7                    merge          100.0                  -1.0   \n",
      "64                   merge           10.0                  -1.0   \n",
      "29                   merge           10.0                  -1.0   \n",
      "5                    merge           10.0                  -1.0   \n",
      "59                     NAN            0.0                  -1.0   \n",
      "61                     NAN            0.0                  -1.0   \n",
      "51                     NAN            0.0                  -1.0   \n",
      "50                     NAN            0.0                  -1.0   \n",
      "6                    merge           10.0                  -1.0   \n",
      "60                     NAN            0.0                  -1.0   \n",
      "73                     NAN            0.0                  -1.0   \n",
      "49                     NAN            0.0                  -1.0   \n",
      "19                     NAN            0.0                  -1.0   \n",
      "30                   merge           10.0                  -1.0   \n",
      "52                     NAN            0.0                  -1.0   \n",
      "37                     NAN            NAN                   NAN   \n",
      "\n",
      "   finetune_params_prefreeze  \\\n",
      "1                        NAN   \n",
      "57                       NAN   \n",
      "32                       NAN   \n",
      "22                       NAN   \n",
      "40                       NAN   \n",
      "78                       NAN   \n",
      "38                       NAN   \n",
      "31                       NAN   \n",
      "44                       NAN   \n",
      "8                        NAN   \n",
      "41                       NAN   \n",
      "56                       NAN   \n",
      "55                       NAN   \n",
      "43                       NAN   \n",
      "0                        NAN   \n",
      "20                       NAN   \n",
      "77                       NAN   \n",
      "12                       NAN   \n",
      "42                       NAN   \n",
      "34                       NAN   \n",
      "76                       NAN   \n",
      "2                        NAN   \n",
      "58                       NAN   \n",
      "33                       NAN   \n",
      "45                       NAN   \n",
      "79                       NAN   \n",
      "74                       NAN   \n",
      "46                       NAN   \n",
      "39                       NAN   \n",
      "15                       NAN   \n",
      "11                       NAN   \n",
      "18                       NAN   \n",
      "28                       NAN   \n",
      "66                       NAN   \n",
      "47                       NAN   \n",
      "48                       NAN   \n",
      "14                       NAN   \n",
      "53                       NAN   \n",
      "72                       NAN   \n",
      "26                       NAN   \n",
      "17                       NAN   \n",
      "70                       NAN   \n",
      "24                       NAN   \n",
      "27                       NAN   \n",
      "68                       NAN   \n",
      "25                       NAN   \n",
      "63                       NAN   \n",
      "36                       NAN   \n",
      "62                       NAN   \n",
      "71                       NAN   \n",
      "67                       NAN   \n",
      "16                       NAN   \n",
      "23                       NAN   \n",
      "4                        NAN   \n",
      "21                       NAN   \n",
      "54                       NAN   \n",
      "65                       NAN   \n",
      "3                        NAN   \n",
      "75                       NAN   \n",
      "35                       NAN   \n",
      "69                       NAN   \n",
      "10                       NAN   \n",
      "9                        NAN   \n",
      "13                       NAN   \n",
      "7                        NAN   \n",
      "64                       NAN   \n",
      "29                       NAN   \n",
      "5                        NAN   \n",
      "59                       NAN   \n",
      "61                       NAN   \n",
      "51                       NAN   \n",
      "50                       NAN   \n",
      "6                        NAN   \n",
      "60                       NAN   \n",
      "73                       NAN   \n",
      "49                       NAN   \n",
      "19                       NAN   \n",
      "30                       NAN   \n",
      "52                       NAN   \n",
      "37                       NAN   \n",
      "\n",
      "                                              dataset  \\\n",
      "1                       magpie-default-tokenized_1000   \n",
      "57                       magpie-default-tokenized_500   \n",
      "32                       magpie-default-tokenized_100   \n",
      "22  magpie-default-tokenized_100,magpie-translatio...   \n",
      "40  magpie-default-tokenized_0,magpie-translation-...   \n",
      "78                         magpie-default-tokenized_0   \n",
      "38  magpie-default-tokenized_500,magpie-translatio...   \n",
      "31  magpie-default-tokenized_1000,magpie-translati...   \n",
      "44                        magpie-default-tokenized_10   \n",
      "8                        magpie-default-tokenized_500   \n",
      "41  magpie-default-tokenized_10,magpie-translation...   \n",
      "56                      magpie-default-tokenized_1000   \n",
      "55  magpie-default-tokenized_10,magpie-translation...   \n",
      "43                        magpie-default-tokenized_10   \n",
      "0   magpie-default-tokenized_1000,magpie-translati...   \n",
      "20  magpie-default-tokenized_100,magpie-translatio...   \n",
      "77  magpie-default-tokenized_0,magpie-translation-...   \n",
      "12                                                NAN   \n",
      "42  magpie-default-tokenized_500,magpie-translatio...   \n",
      "34                         magpie-default-tokenized_0   \n",
      "76                       magpie-default-tokenized_100   \n",
      "2                       magpie-default-tokenized_1000   \n",
      "58                       magpie-default-tokenized_500   \n",
      "33                       magpie-default-tokenized_100   \n",
      "45                        magpie-default-tokenized_10   \n",
      "79                         magpie-default-tokenized_0   \n",
      "74  magpie-default-tokenized_1000,magpie-translati...   \n",
      "46                      magpie-default-tokenized_1000   \n",
      "39                       magpie-default-tokenized_100   \n",
      "15  magpie-default-tokenized_500,magpie-translatio...   \n",
      "11  magpie-default-tokenized_100,magpie-translatio...   \n",
      "18                       magpie-default-tokenized_500   \n",
      "28  magpie-default-tokenized_10,magpie-translation...   \n",
      "66                        magpie-default-tokenized_10   \n",
      "47  magpie-default-tokenized_0,magpie-translation-...   \n",
      "48                         magpie-default-tokenized_0   \n",
      "14                      magpie-default-tokenized_1000   \n",
      "53  magpie-default-tokenized_1000,magpie-translati...   \n",
      "72                      magpie-default-tokenized_1000   \n",
      "26                       magpie-default-tokenized_500   \n",
      "17  magpie-default-tokenized_1000,magpie-translati...   \n",
      "70                      magpie-default-tokenized_1000   \n",
      "24  magpie-default-tokenized_1000,magpie-translati...   \n",
      "27  magpie-default-tokenized_500,magpie-translatio...   \n",
      "68                       magpie-default-tokenized_500   \n",
      "25  magpie-default-tokenized_1000,magpie-translati...   \n",
      "63                       magpie-default-tokenized_500   \n",
      "36                       magpie-default-tokenized_100   \n",
      "62                       magpie-default-tokenized_500   \n",
      "71                      magpie-default-tokenized_1000   \n",
      "67                       magpie-default-tokenized_500   \n",
      "16  magpie-default-tokenized_500,magpie-translatio...   \n",
      "23                        magpie-default-tokenized_10   \n",
      "4                        magpie-default-tokenized_100   \n",
      "21  magpie-default-tokenized_100,magpie-translatio...   \n",
      "54  magpie-default-tokenized_100,magpie-translatio...   \n",
      "65  magpie-default-tokenized_500,magpie-translatio...   \n",
      "3                        magpie-default-tokenized_100   \n",
      "75  magpie-default-tokenized_10,magpie-translation...   \n",
      "35                       magpie-default-tokenized_100   \n",
      "69                      magpie-default-tokenized_1000   \n",
      "10  magpie-default-tokenized_100,magpie-translatio...   \n",
      "9   magpie-default-tokenized_100,magpie-translatio...   \n",
      "13  magpie-default-tokenized_10,magpie-translation...   \n",
      "7                        magpie-default-tokenized_100   \n",
      "64                        magpie-default-tokenized_10   \n",
      "29                        magpie-default-tokenized_10   \n",
      "5   magpie-default-tokenized_10,magpie-translation...   \n",
      "59  magpie-default-tokenized_0,magpie-translation-...   \n",
      "61                         magpie-default-tokenized_0   \n",
      "51  magpie-default-tokenized_0,magpie-translation-...   \n",
      "50                         magpie-default-tokenized_0   \n",
      "6   magpie-default-tokenized_10,magpie-translation...   \n",
      "60                         magpie-default-tokenized_0   \n",
      "73                         magpie-default-tokenized_0   \n",
      "49                         magpie-default-tokenized_0   \n",
      "19  magpie-default-tokenized_0,magpie-translation-...   \n",
      "30                        magpie-default-tokenized_10   \n",
      "52  magpie-default-tokenized_0,magpie-translation-...   \n",
      "37                                                NAN   \n",
      "\n",
      "                                       tokenizer_path    seed reset_optimizer  \\\n",
      "1   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "57  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "32  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "22  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "40                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "78                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "38  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "31  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "44  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "8   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "41  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "56  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "55  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "43  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "0   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "20  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "77                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "12                                                NAN     NAN             NAN   \n",
      "42  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "34                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "76  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "2   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "58  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "33  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "45  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "79                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "74  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "46  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "39  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "15  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "11  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "18  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "28  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "66  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "47                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "48                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "14  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "53  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "72  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "26  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "17  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "70  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "24  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "27  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "68  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "25  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "63  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "36  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "62  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "71  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "67  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "16  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "23  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "4   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "21  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "54  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "65  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "3   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "75  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "35  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "69  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "10  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "9   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "13  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "7   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "64  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "29  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "5   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "59                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "61                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "51                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "50                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "6   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "60                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "73                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "49                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "19                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "30  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
      "52                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
      "37                                                NAN     NAN             NAN   \n",
      "\n",
      "   warmup_steps lr_schedule warmup_steps_prefreeze lr_schedule_prefreeze  \n",
      "1         100.0      cosine                   -1.0                   NAN  \n",
      "57        100.0      cosine                   -1.0                   NAN  \n",
      "32        100.0      cosine                   -1.0                   NAN  \n",
      "22        100.0      cosine                   -1.0                   NAN  \n",
      "40        100.0      cosine                   -1.0                   NAN  \n",
      "78        100.0      cosine                   -1.0                   NAN  \n",
      "38        100.0      cosine                   -1.0                   NAN  \n",
      "31        100.0      cosine                   -1.0                   NAN  \n",
      "44        100.0      cosine                   -1.0                   NAN  \n",
      "8         100.0      cosine                   -1.0                   NAN  \n",
      "41        100.0      cosine                   -1.0                   NAN  \n",
      "56        100.0      cosine                   -1.0                   NAN  \n",
      "55        100.0      cosine                   -1.0                   NAN  \n",
      "43        100.0      cosine                   -1.0                   NAN  \n",
      "0         100.0      cosine                   -1.0                   NAN  \n",
      "20        100.0      cosine                   -1.0                   NAN  \n",
      "77        100.0      cosine                   -1.0                   NAN  \n",
      "12          NAN         NAN                    NAN                   NAN  \n",
      "42        100.0      cosine                   -1.0                   NAN  \n",
      "34        100.0      cosine                   -1.0                   NAN  \n",
      "76        100.0      cosine                   -1.0                   NAN  \n",
      "2         100.0      cosine                   -1.0                   NAN  \n",
      "58        100.0      cosine                   -1.0                   NAN  \n",
      "33        100.0      cosine                   -1.0                   NAN  \n",
      "45        100.0      cosine                   -1.0                   NAN  \n",
      "79        100.0      cosine                   -1.0                   NAN  \n",
      "74        100.0      cosine                   -1.0                   NAN  \n",
      "46        100.0      cosine                   -1.0                   NAN  \n",
      "39        100.0      cosine                   -1.0                   NAN  \n",
      "15        100.0      cosine                   -1.0                   NAN  \n",
      "11        100.0      cosine                   -1.0                   NAN  \n",
      "18        100.0      cosine                   -1.0                   NAN  \n",
      "28        100.0      cosine                   -1.0                   NAN  \n",
      "66        100.0      cosine                   -1.0                   NAN  \n",
      "47        100.0      cosine                   -1.0                   NAN  \n",
      "48        100.0      cosine                   -1.0                   NAN  \n",
      "14        100.0      cosine                   -1.0                   NAN  \n",
      "53        100.0      cosine                   -1.0                   NAN  \n",
      "72        100.0      cosine                   -1.0                   NAN  \n",
      "26        100.0      cosine                   -1.0                   NAN  \n",
      "17        100.0      cosine                   -1.0                   NAN  \n",
      "70        100.0      cosine                   -1.0                   NAN  \n",
      "24        100.0      cosine                   -1.0                   NAN  \n",
      "27        100.0      cosine                   -1.0                   NAN  \n",
      "68        100.0      cosine                   -1.0                   NAN  \n",
      "25        100.0      cosine                   -1.0                   NAN  \n",
      "63        100.0      cosine                   -1.0                   NAN  \n",
      "36        100.0      cosine                   -1.0                   NAN  \n",
      "62        100.0      cosine                   -1.0                   NAN  \n",
      "71        100.0      cosine                   -1.0                   NAN  \n",
      "67        100.0      cosine                   -1.0                   NAN  \n",
      "16        100.0      cosine                   -1.0                   NAN  \n",
      "23        100.0      cosine                   -1.0                   NAN  \n",
      "4         100.0      cosine                   -1.0                   NAN  \n",
      "21        100.0      cosine                   -1.0                   NAN  \n",
      "54        100.0      cosine                   -1.0                   NAN  \n",
      "65        100.0      cosine                   -1.0                   NAN  \n",
      "3         100.0      cosine                   -1.0                   NAN  \n",
      "75        100.0      cosine                   -1.0                   NAN  \n",
      "35        100.0      cosine                   -1.0                   NAN  \n",
      "69        100.0      cosine                   -1.0                   NAN  \n",
      "10        100.0      cosine                   -1.0                   NAN  \n",
      "9         100.0      cosine                   -1.0                   NAN  \n",
      "13        100.0      cosine                   -1.0                   NAN  \n",
      "7         100.0      cosine                   -1.0                   NAN  \n",
      "64        100.0      cosine                   -1.0                   NAN  \n",
      "29        100.0      cosine                   -1.0                   NAN  \n",
      "5         100.0      cosine                   -1.0                   NAN  \n",
      "59        100.0      cosine                   -1.0                   NAN  \n",
      "61        100.0      cosine                   -1.0                   NAN  \n",
      "51        100.0      cosine                   -1.0                   NAN  \n",
      "50        100.0      cosine                   -1.0                   NAN  \n",
      "6         100.0      cosine                   -1.0                   NAN  \n",
      "60        100.0      cosine                   -1.0                   NAN  \n",
      "73        100.0      cosine                   -1.0                   NAN  \n",
      "49        100.0      cosine                   -1.0                   NAN  \n",
      "19        100.0      cosine                   -1.0                   NAN  \n",
      "30        100.0      cosine                   -1.0                   NAN  \n",
      "52        100.0      cosine                   -1.0                   NAN  \n",
      "37          NAN         NAN                    NAN                   NAN  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name_sanitized</th>\n",
       "      <th>trained_model_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>embeddings_init</th>\n",
       "      <th>task_name</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>exact_match_stderr</th>\n",
       "      <th>prompt_level_strict_acc</th>\n",
       "      <th>prompt_level_strict_acc_stderr</th>\n",
       "      <th>inst_level_strict_acc</th>\n",
       "      <th>inst_level_strict_acc_stderr</th>\n",
       "      <th>prompt_level_loose_acc</th>\n",
       "      <th>prompt_level_loose_acc_stderr</th>\n",
       "      <th>inst_level_loose_acc</th>\n",
       "      <th>inst_level_loose_acc_stderr</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>learning_ratio</th>\n",
       "      <th>theoretical_compression_ratio</th>\n",
       "      <th>alias</th>\n",
       "      <th>limit</th>\n",
       "      <th>json_path</th>\n",
       "      <th>dataset_str</th>\n",
       "      <th>finetuning_params</th>\n",
       "      <th>total_batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>main_loss_type</th>\n",
       "      <th>embedding_init_strategy</th>\n",
       "      <th>num_new_tokens</th>\n",
       "      <th>unfreeze_params_steps</th>\n",
       "      <th>finetune_params_prefreeze</th>\n",
       "      <th>dataset</th>\n",
       "      <th>tokenizer_path</th>\n",
       "      <th>seed</th>\n",
       "      <th>reset_optimizer</th>\n",
       "      <th>warmup_steps</th>\n",
       "      <th>lr_schedule</th>\n",
       "      <th>warmup_steps_prefreeze</th>\n",
       "      <th>lr_schedule_prefreeze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>output__full_patching__ba7c8b60-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__ba7c8b60-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.192237</td>\n",
       "      <td>0.016958</td>\n",
       "      <td>0.305755</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.207024</td>\n",
       "      <td>0.017436</td>\n",
       "      <td>0.329736</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.133380</td>\n",
       "      <td>0.034137</td>\n",
       "      <td>0.170377</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>4eb8fba9-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.134935</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>0.262590</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.146026</td>\n",
       "      <td>0.015196</td>\n",
       "      <td>0.273381</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.101597</td>\n",
       "      <td>0.022770</td>\n",
       "      <td>0.122494</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/4eb8fba9-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>4eb8fba9-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.327172</td>\n",
       "      <td>0.020190</td>\n",
       "      <td>0.468825</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.414048</td>\n",
       "      <td>0.021196</td>\n",
       "      <td>0.552758</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.016155</td>\n",
       "      <td>-0.000890</td>\n",
       "      <td>0.014179</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/4eb8fba9-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>508aaf68-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.663586</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>0.757794</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.719039</td>\n",
       "      <td>0.019342</td>\n",
       "      <td>0.799760</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.004844</td>\n",
       "      <td>0.065750</td>\n",
       "      <td>0.059344</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/508aaf68-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>508aaf68-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.658041</td>\n",
       "      <td>0.020413</td>\n",
       "      <td>0.752998</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.707948</td>\n",
       "      <td>0.019567</td>\n",
       "      <td>0.793765</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.005003</td>\n",
       "      <td>0.066387</td>\n",
       "      <td>0.059817</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/508aaf68-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>d2d49ae1-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.693161</td>\n",
       "      <td>0.019846</td>\n",
       "      <td>0.778177</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.737523</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.814149</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.008155</td>\n",
       "      <td>0.025803</td>\n",
       "      <td>0.015955</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/d2d49ae1-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>d2d49ae1-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.698706</td>\n",
       "      <td>0.019744</td>\n",
       "      <td>0.779376</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.741220</td>\n",
       "      <td>0.018847</td>\n",
       "      <td>0.816547</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.008277</td>\n",
       "      <td>0.025612</td>\n",
       "      <td>0.015627</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/d2d49ae1-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>2916d55c-Llama-3.2-3B-Instruct-mixed-100</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.685767</td>\n",
       "      <td>0.019976</td>\n",
       "      <td>0.767386</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.739372</td>\n",
       "      <td>0.018891</td>\n",
       "      <td>0.810552</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.020501</td>\n",
       "      <td>0.046151</td>\n",
       "      <td>0.066113</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/2916d55c-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>dfe1b80e-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.177449</td>\n",
       "      <td>0.016441</td>\n",
       "      <td>0.288969</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.184843</td>\n",
       "      <td>0.016704</td>\n",
       "      <td>0.296163</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.614980</td>\n",
       "      <td>0.019093</td>\n",
       "      <td>0.632944</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/dfe1b80e-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>3c315a04-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.678373</td>\n",
       "      <td>0.020101</td>\n",
       "      <td>0.767386</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.730129</td>\n",
       "      <td>0.019102</td>\n",
       "      <td>0.808153</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.002788</td>\n",
       "      <td>0.063923</td>\n",
       "      <td>0.059693</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/3c315a04-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>3c315a04-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.678373</td>\n",
       "      <td>0.020101</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.726433</td>\n",
       "      <td>0.019184</td>\n",
       "      <td>0.806954</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.002878</td>\n",
       "      <td>0.063872</td>\n",
       "      <td>0.059529</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/3c315a04-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>output__full_patching__e5175fd2-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__e5175fd2-Llama-3.2-3B-I...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.519409</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>0.628297</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.574861</td>\n",
       "      <td>0.021274</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.033028</td>\n",
       "      <td>0.020466</td>\n",
       "      <td>0.052770</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>0.017081</td>\n",
       "      <td>0.324940</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.218115</td>\n",
       "      <td>0.017771</td>\n",
       "      <td>0.350120</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/meta-llama__Llama-3...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>bd539cf5-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.685767</td>\n",
       "      <td>0.019976</td>\n",
       "      <td>0.778177</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.741220</td>\n",
       "      <td>0.018847</td>\n",
       "      <td>0.822542</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010551</td>\n",
       "      <td>0.028006</td>\n",
       "      <td>0.015644</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/bd539cf5-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>6e1e953a-Llama-3.2-3B-Instruct-mixed-1000</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.606285</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.703837</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.652495</td>\n",
       "      <td>0.020491</td>\n",
       "      <td>0.743405</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.042820</td>\n",
       "      <td>0.105766</td>\n",
       "      <td>0.150301</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/6e1e953a-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>output__full_patching__4d6edc1e-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__4d6edc1e-Llama-3.2-3B-I...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.510166</td>\n",
       "      <td>0.021512</td>\n",
       "      <td>0.616307</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.548983</td>\n",
       "      <td>0.021413</td>\n",
       "      <td>0.658273</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.060745</td>\n",
       "      <td>0.040485</td>\n",
       "      <td>0.098851</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>output__full_patching__4b9504d3-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__4b9504d3-Llama-3.2-3B-I...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>0.020440</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.700555</td>\n",
       "      <td>0.019710</td>\n",
       "      <td>0.790168</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.037358</td>\n",
       "      <td>0.090833</td>\n",
       "      <td>0.130479</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>461b9dfe-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.632163</td>\n",
       "      <td>0.020751</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.669131</td>\n",
       "      <td>0.020248</td>\n",
       "      <td>0.767386</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.145470</td>\n",
       "      <td>0.145546</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/461b9dfe-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>0a4e765d-Llama-3.2-3B-Instruct-mixed-500</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.523105</td>\n",
       "      <td>0.021494</td>\n",
       "      <td>0.622302</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.563771</td>\n",
       "      <td>0.021341</td>\n",
       "      <td>0.660671</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.068671</td>\n",
       "      <td>0.034358</td>\n",
       "      <td>0.099375</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/0a4e765d-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>output__full_patching__c5764fce-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__c5764fce-Llama-3.2-3B-I...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>0.019639</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010785</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0,magpie-translation-...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>output__full_patching__3c315a04-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__3c315a04-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.194085</td>\n",
       "      <td>0.017019</td>\n",
       "      <td>0.320144</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.210721</td>\n",
       "      <td>0.017550</td>\n",
       "      <td>0.342926</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.039745</td>\n",
       "      <td>0.028531</td>\n",
       "      <td>0.064240</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>2ecfe6ea-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.659889</td>\n",
       "      <td>0.020387</td>\n",
       "      <td>0.760192</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.706100</td>\n",
       "      <td>0.019604</td>\n",
       "      <td>0.799760</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>0.068191</td>\n",
       "      <td>0.059037</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/2ecfe6ea-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>output__full_patching__2ecfe6ea-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__2ecfe6ea-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.142329</td>\n",
       "      <td>0.015035</td>\n",
       "      <td>0.260192</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.155268</td>\n",
       "      <td>0.015585</td>\n",
       "      <td>0.282974</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.040349</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>0.046784</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>output__full_patching__12f3cea0-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__12f3cea0-Llama-3.2-3B-I...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>0.020440</td>\n",
       "      <td>0.751799</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.715342</td>\n",
       "      <td>0.019419</td>\n",
       "      <td>0.798561</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.001833</td>\n",
       "      <td>0.021714</td>\n",
       "      <td>0.018278</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>ba7c8b60-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.643253</td>\n",
       "      <td>0.020615</td>\n",
       "      <td>0.738609</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.670980</td>\n",
       "      <td>0.020219</td>\n",
       "      <td>0.766187</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.007573</td>\n",
       "      <td>0.139264</td>\n",
       "      <td>0.146980</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/ba7c8b60-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>ba7c8b60-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.648799</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.742206</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.685767</td>\n",
       "      <td>0.019976</td>\n",
       "      <td>0.773381</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>0.136249</td>\n",
       "      <td>0.143803</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/ba7c8b60-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>755abfca-Llama-3.2-3B-Instruct-mixed-500</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.628466</td>\n",
       "      <td>0.020794</td>\n",
       "      <td>0.730216</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.674677</td>\n",
       "      <td>0.020161</td>\n",
       "      <td>0.768585</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.036607</td>\n",
       "      <td>0.088294</td>\n",
       "      <td>0.126927</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/755abfca-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>234bdfaa-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.643253</td>\n",
       "      <td>0.020615</td>\n",
       "      <td>0.748201</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.667283</td>\n",
       "      <td>0.020277</td>\n",
       "      <td>0.773381</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.116043</td>\n",
       "      <td>0.118194</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/234bdfaa-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>output__full_patching__e93b6877-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__e93b6877-Llama-3.2-3B-I...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.528651</td>\n",
       "      <td>0.021481</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.602588</td>\n",
       "      <td>0.021059</td>\n",
       "      <td>0.706235</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.017905</td>\n",
       "      <td>0.014476</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>461ac385-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.689464</td>\n",
       "      <td>0.019912</td>\n",
       "      <td>0.779376</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.733826</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.814149</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010191</td>\n",
       "      <td>0.027429</td>\n",
       "      <td>0.015465</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/461ac385-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>461ac385-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>0.019639</td>\n",
       "      <td>0.788969</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.748614</td>\n",
       "      <td>0.018668</td>\n",
       "      <td>0.826139</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010125</td>\n",
       "      <td>0.027598</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/461ac385-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>output__full_patching__461b9dfe-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__461b9dfe-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.171904</td>\n",
       "      <td>0.016236</td>\n",
       "      <td>0.282974</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.190388</td>\n",
       "      <td>0.016895</td>\n",
       "      <td>0.320144</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.121145</td>\n",
       "      <td>0.048861</td>\n",
       "      <td>0.147647</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>0a90e719-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>0.014871</td>\n",
       "      <td>0.264988</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.164510</td>\n",
       "      <td>0.015954</td>\n",
       "      <td>0.292566</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.035638</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.042507</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/0a90e719-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>0a90e719-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.390018</td>\n",
       "      <td>0.020990</td>\n",
       "      <td>0.534772</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.482440</td>\n",
       "      <td>0.021503</td>\n",
       "      <td>0.609113</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.017502</td>\n",
       "      <td>-0.001952</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/0a90e719-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>output__full_patching__8760b763-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__8760b763-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.205176</td>\n",
       "      <td>0.017378</td>\n",
       "      <td>0.327338</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.223660</td>\n",
       "      <td>0.017932</td>\n",
       "      <td>0.351319</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.000266</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>0a90e719-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.674677</td>\n",
       "      <td>0.020161</td>\n",
       "      <td>0.770983</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.722736</td>\n",
       "      <td>0.019264</td>\n",
       "      <td>0.806954</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.007079</td>\n",
       "      <td>0.067890</td>\n",
       "      <td>0.059081</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/0a90e719-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>0a90e719-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.650647</td>\n",
       "      <td>0.020517</td>\n",
       "      <td>0.749400</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.713494</td>\n",
       "      <td>0.019457</td>\n",
       "      <td>0.796163</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.007362</td>\n",
       "      <td>0.068735</td>\n",
       "      <td>0.059582</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/0a90e719-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.706100</td>\n",
       "      <td>0.019604</td>\n",
       "      <td>0.790168</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.822542</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010776</td>\n",
       "      <td>0.012392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/meta-llama__Llama-3...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>output__full_patching__e8e7ce8d-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__e8e7ce8d-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.166359</td>\n",
       "      <td>0.016026</td>\n",
       "      <td>0.276978</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.186691</td>\n",
       "      <td>0.016768</td>\n",
       "      <td>0.296163</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>0.100258</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>958352ea-Llama-3.2-3B-Instruct-mixed-100</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.486137</td>\n",
       "      <td>0.021508</td>\n",
       "      <td>0.606715</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.537893</td>\n",
       "      <td>0.021455</td>\n",
       "      <td>0.653477</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.028351</td>\n",
       "      <td>0.025881</td>\n",
       "      <td>0.048398</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/958352ea-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>output__full_patching__d1f6703c-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__d1f6703c-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.149723</td>\n",
       "      <td>0.015354</td>\n",
       "      <td>0.267386</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.162662</td>\n",
       "      <td>0.015882</td>\n",
       "      <td>0.284173</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.001223</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0,magpie-translation-...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>output__full_patching__bd539cf5-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__bd539cf5-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.177449</td>\n",
       "      <td>0.016441</td>\n",
       "      <td>0.288969</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.199630</td>\n",
       "      <td>0.017201</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.014895</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.017731</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>output__full_patching__234bdfaa-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__234bdfaa-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.197782</td>\n",
       "      <td>0.017141</td>\n",
       "      <td>0.309353</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.212569</td>\n",
       "      <td>0.017606</td>\n",
       "      <td>0.338129</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.147659</td>\n",
       "      <td>0.043606</td>\n",
       "      <td>0.192847</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>output__full_patching__daac169e-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__daac169e-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.188540</td>\n",
       "      <td>0.016832</td>\n",
       "      <td>0.321343</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.207024</td>\n",
       "      <td>0.017436</td>\n",
       "      <td>0.345324</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.015031</td>\n",
       "      <td>0.015013</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>461ac385-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.171904</td>\n",
       "      <td>0.016236</td>\n",
       "      <td>0.299760</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.179298</td>\n",
       "      <td>0.016508</td>\n",
       "      <td>0.312950</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.017405</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.018738</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/461ac385-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>461ac385-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.414048</td>\n",
       "      <td>0.021196</td>\n",
       "      <td>0.541966</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.504621</td>\n",
       "      <td>0.021516</td>\n",
       "      <td>0.624700</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.001514</td>\n",
       "      <td>0.021735</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/461ac385-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>9df250b6-Llama-3.2-3B-Instruct-mixed-1000</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.484288</td>\n",
       "      <td>0.021506</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.534196</td>\n",
       "      <td>0.021466</td>\n",
       "      <td>0.643885</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.081260</td>\n",
       "      <td>0.058773</td>\n",
       "      <td>0.118345</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/9df250b6-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>output__full_patching__8d72bbbe-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__8d72bbbe-Llama-3.2-3B-I...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.554529</td>\n",
       "      <td>0.021388</td>\n",
       "      <td>0.661871</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.609982</td>\n",
       "      <td>0.020990</td>\n",
       "      <td>0.714628</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0,magpie-translation-...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>368f5252-Llama-3.2-3B-Instruct-mixed-0</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.556377</td>\n",
       "      <td>0.021379</td>\n",
       "      <td>0.664269</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.621072</td>\n",
       "      <td>0.020876</td>\n",
       "      <td>0.724221</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.011691</td>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/368f5252-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>8760b763-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>0.019639</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010785</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/8760b763-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>8760b763-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.696858</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.781775</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.746765</td>\n",
       "      <td>0.018714</td>\n",
       "      <td>0.818945</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010761</td>\n",
       "      <td>0.012375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/8760b763-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>a84197a7-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.696858</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.781775</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.746765</td>\n",
       "      <td>0.018714</td>\n",
       "      <td>0.818945</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010761</td>\n",
       "      <td>0.012375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/a84197a7-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0,magpie-translation-...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>a84197a7-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>0.019639</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010785</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/a84197a7-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0,magpie-translation-...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>output__full_patching__80c6810f-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__80c6810f-Llama-3.2-3B-I...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.621072</td>\n",
       "      <td>0.020876</td>\n",
       "      <td>0.723022</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.661738</td>\n",
       "      <td>0.020360</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.044952</td>\n",
       "      <td>0.106657</td>\n",
       "      <td>0.155445</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>output__full_patching__5c0eb32b-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__5c0eb32b-Llama-3.2-3B-I...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.659889</td>\n",
       "      <td>0.020387</td>\n",
       "      <td>0.758993</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.707948</td>\n",
       "      <td>0.019567</td>\n",
       "      <td>0.798561</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.022516</td>\n",
       "      <td>0.045404</td>\n",
       "      <td>0.067439</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>output__full_patching__d2d49ae1-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__d2d49ae1-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.186691</td>\n",
       "      <td>0.016768</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.203327</td>\n",
       "      <td>0.017320</td>\n",
       "      <td>0.341727</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>0.018304</td>\n",
       "      <td>0.016143</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>output__full_patching__63d35748-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__63d35748-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.184843</td>\n",
       "      <td>0.016704</td>\n",
       "      <td>0.293765</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.192237</td>\n",
       "      <td>0.016958</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.158073</td>\n",
       "      <td>0.043921</td>\n",
       "      <td>0.204460</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>e45070ca-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>0.014871</td>\n",
       "      <td>0.270983</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.149723</td>\n",
       "      <td>0.015354</td>\n",
       "      <td>0.280576</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.087240</td>\n",
       "      <td>0.012155</td>\n",
       "      <td>0.100254</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/e45070ca-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>e45070ca-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.343808</td>\n",
       "      <td>0.020440</td>\n",
       "      <td>0.482014</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.428835</td>\n",
       "      <td>0.021298</td>\n",
       "      <td>0.557554</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.009105</td>\n",
       "      <td>0.011711</td>\n",
       "      <td>0.013769</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/e45070ca-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>d1f6703c-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.696858</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.781775</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.746765</td>\n",
       "      <td>0.018714</td>\n",
       "      <td>0.818945</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010761</td>\n",
       "      <td>0.012375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/d1f6703c-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0,magpie-translation-...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>74ed1b01-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>0.019639</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010785</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/74ed1b01-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>74ed1b01-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.696858</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.781775</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.746765</td>\n",
       "      <td>0.018714</td>\n",
       "      <td>0.818945</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010761</td>\n",
       "      <td>0.012375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/74ed1b01-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>e45070ca-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.652495</td>\n",
       "      <td>0.020491</td>\n",
       "      <td>0.749400</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.689464</td>\n",
       "      <td>0.019912</td>\n",
       "      <td>0.786571</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.001875</td>\n",
       "      <td>0.120766</td>\n",
       "      <td>0.117677</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/e45070ca-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>e45070ca-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.650647</td>\n",
       "      <td>0.020517</td>\n",
       "      <td>0.743405</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.693161</td>\n",
       "      <td>0.019846</td>\n",
       "      <td>0.781775</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.001883</td>\n",
       "      <td>0.121565</td>\n",
       "      <td>0.118431</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/e45070ca-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>daac169e-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.685767</td>\n",
       "      <td>0.019976</td>\n",
       "      <td>0.770983</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.730129</td>\n",
       "      <td>0.019102</td>\n",
       "      <td>0.809353</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.008610</td>\n",
       "      <td>0.026663</td>\n",
       "      <td>0.016339</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/daac169e-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>e8e7ce8d-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.661738</td>\n",
       "      <td>0.020360</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.687616</td>\n",
       "      <td>0.019944</td>\n",
       "      <td>0.781775</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.000953</td>\n",
       "      <td>0.121166</td>\n",
       "      <td>0.119027</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/e8e7ce8d-Llama-3.2-...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>9e5180ab-Llama-3.2-3B-Instruct-mixed-10</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.534196</td>\n",
       "      <td>0.021466</td>\n",
       "      <td>0.637890</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.582255</td>\n",
       "      <td>0.021223</td>\n",
       "      <td>0.681055</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.039284</td>\n",
       "      <td>0.013212</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/9e5180ab-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>dfe1b80e-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.654344</td>\n",
       "      <td>0.020466</td>\n",
       "      <td>0.750600</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.695009</td>\n",
       "      <td>0.019813</td>\n",
       "      <td>0.786571</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>0.119391</td>\n",
       "      <td>0.117738</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/dfe1b80e-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>dfe1b80e-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.646950</td>\n",
       "      <td>0.020566</td>\n",
       "      <td>0.741007</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.682070</td>\n",
       "      <td>0.020039</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.000689</td>\n",
       "      <td>0.118552</td>\n",
       "      <td>0.116691</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/dfe1b80e-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>500.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_500</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>63d35748-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.676525</td>\n",
       "      <td>0.020131</td>\n",
       "      <td>0.770983</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.707948</td>\n",
       "      <td>0.019567</td>\n",
       "      <td>0.797362</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.139993</td>\n",
       "      <td>0.142160</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/63d35748-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>63d35748-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.639556</td>\n",
       "      <td>0.020661</td>\n",
       "      <td>0.743405</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.678373</td>\n",
       "      <td>0.020101</td>\n",
       "      <td>0.776978</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.141326</td>\n",
       "      <td>0.143011</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/63d35748-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>4eb8fba9-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.652495</td>\n",
       "      <td>0.020491</td>\n",
       "      <td>0.749400</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.689464</td>\n",
       "      <td>0.019912</td>\n",
       "      <td>0.780576</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.142038</td>\n",
       "      <td>0.144711</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/4eb8fba9-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>4eb8fba9-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>new_only</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.628466</td>\n",
       "      <td>0.020794</td>\n",
       "      <td>0.735012</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.670980</td>\n",
       "      <td>0.020219</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.145274</td>\n",
       "      <td>0.146620</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/4eb8fba9-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>output__full_patching__cb47c58d-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__cb47c58d-Llama-3.2-3B-I...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>0.019639</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010785</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>output__full_patching__8ce7f0d6-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__8ce7f0d6-Llama-3.2-3B-I...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.449168</td>\n",
       "      <td>0.021405</td>\n",
       "      <td>0.577938</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.493530</td>\n",
       "      <td>0.021515</td>\n",
       "      <td>0.623501</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.075506</td>\n",
       "      <td>0.038008</td>\n",
       "      <td>0.114864</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>output__full_patching__f276bbe3-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__f276bbe3-Llama-3.2-3B-I...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.674677</td>\n",
       "      <td>0.020161</td>\n",
       "      <td>0.763789</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.720887</td>\n",
       "      <td>0.019303</td>\n",
       "      <td>0.800959</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.001153</td>\n",
       "      <td>0.021340</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>output__full_patching__508aaf68-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__508aaf68-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.207024</td>\n",
       "      <td>0.017436</td>\n",
       "      <td>0.322542</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.227357</td>\n",
       "      <td>0.018036</td>\n",
       "      <td>0.347722</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.031562</td>\n",
       "      <td>0.040201</td>\n",
       "      <td>0.068354</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>output__full_patching__a84197a7-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__a84197a7-Llama-3.2-3B-m...</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>0.017081</td>\n",
       "      <td>0.324940</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.218115</td>\n",
       "      <td>0.017771</td>\n",
       "      <td>0.350120</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/output__full_patchi...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0,magpie-translation-...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>74ed1b01-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.153420</td>\n",
       "      <td>0.015509</td>\n",
       "      <td>0.285372</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.166359</td>\n",
       "      <td>0.016026</td>\n",
       "      <td>0.303357</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/74ed1b01-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>74ed1b01-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.415896</td>\n",
       "      <td>0.021210</td>\n",
       "      <td>0.558753</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.517560</td>\n",
       "      <td>0.021503</td>\n",
       "      <td>0.640288</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.012280</td>\n",
       "      <td>-0.002480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/full_patching/74ed1b01-Llama-3.2-...</td>\n",
       "      <td>default</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_0</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model_name_sanitized  \\\n",
       "0   output__full_patching__ba7c8b60-Llama-3.2-3B-m...   \n",
       "1                            meta-llama__Llama-3.2-3B   \n",
       "2                   meta-llama__Llama-3.2-3B-Instruct   \n",
       "3                   meta-llama__Llama-3.2-3B-Instruct   \n",
       "4                   meta-llama__Llama-3.2-3B-Instruct   \n",
       "5                   meta-llama__Llama-3.2-3B-Instruct   \n",
       "6                   meta-llama__Llama-3.2-3B-Instruct   \n",
       "7                   meta-llama__Llama-3.2-3B-Instruct   \n",
       "8                            meta-llama__Llama-3.2-3B   \n",
       "9                   meta-llama__Llama-3.2-3B-Instruct   \n",
       "10                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "11  output__full_patching__e5175fd2-Llama-3.2-3B-I...   \n",
       "12                           meta-llama__Llama-3.2-3B   \n",
       "13                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "14                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "15  output__full_patching__4d6edc1e-Llama-3.2-3B-I...   \n",
       "16  output__full_patching__4b9504d3-Llama-3.2-3B-I...   \n",
       "17                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "18                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "19  output__full_patching__c5764fce-Llama-3.2-3B-I...   \n",
       "20  output__full_patching__3c315a04-Llama-3.2-3B-m...   \n",
       "21                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "22  output__full_patching__2ecfe6ea-Llama-3.2-3B-m...   \n",
       "23  output__full_patching__12f3cea0-Llama-3.2-3B-I...   \n",
       "24                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "25                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "26                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "27                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "28  output__full_patching__e93b6877-Llama-3.2-3B-I...   \n",
       "29                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "30                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "31  output__full_patching__461b9dfe-Llama-3.2-3B-m...   \n",
       "32                           meta-llama__Llama-3.2-3B   \n",
       "33                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "34  output__full_patching__8760b763-Llama-3.2-3B-m...   \n",
       "35                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "36                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "37                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "38  output__full_patching__e8e7ce8d-Llama-3.2-3B-m...   \n",
       "39                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "40  output__full_patching__d1f6703c-Llama-3.2-3B-m...   \n",
       "41  output__full_patching__bd539cf5-Llama-3.2-3B-m...   \n",
       "42  output__full_patching__234bdfaa-Llama-3.2-3B-m...   \n",
       "43  output__full_patching__daac169e-Llama-3.2-3B-m...   \n",
       "44                           meta-llama__Llama-3.2-3B   \n",
       "45                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "46                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "47  output__full_patching__8d72bbbe-Llama-3.2-3B-I...   \n",
       "48                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "49                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "50                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "51                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "52                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "53  output__full_patching__80c6810f-Llama-3.2-3B-I...   \n",
       "54  output__full_patching__5c0eb32b-Llama-3.2-3B-I...   \n",
       "55  output__full_patching__d2d49ae1-Llama-3.2-3B-m...   \n",
       "56  output__full_patching__63d35748-Llama-3.2-3B-m...   \n",
       "57                           meta-llama__Llama-3.2-3B   \n",
       "58                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "59                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "60                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "61                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "62                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "63                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "64                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "65                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "66                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "67                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "68                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "69                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "70                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "71                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "72                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "73  output__full_patching__cb47c58d-Llama-3.2-3B-I...   \n",
       "74  output__full_patching__8ce7f0d6-Llama-3.2-3B-I...   \n",
       "75  output__full_patching__f276bbe3-Llama-3.2-3B-I...   \n",
       "76  output__full_patching__508aaf68-Llama-3.2-3B-m...   \n",
       "77  output__full_patching__a84197a7-Llama-3.2-3B-m...   \n",
       "78                           meta-llama__Llama-3.2-3B   \n",
       "79                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "\n",
       "                                   trained_model_name             model_name  \\\n",
       "0   output__full_patching__ba7c8b60-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "1                    4eb8fba9-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
       "2                    4eb8fba9-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
       "3                     508aaf68-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
       "4                     508aaf68-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
       "5                      d2d49ae1-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
       "6                      d2d49ae1-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
       "7            2916d55c-Llama-3.2-3B-Instruct-mixed-100  Llama-3.2-3B-Instruct   \n",
       "8                     dfe1b80e-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
       "9                     3c315a04-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
       "10                    3c315a04-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
       "11  output__full_patching__e5175fd2-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
       "12                           meta-llama__Llama-3.2-3B           Llama-3.2-3B   \n",
       "13                     bd539cf5-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
       "14          6e1e953a-Llama-3.2-3B-Instruct-mixed-1000  Llama-3.2-3B-Instruct   \n",
       "15  output__full_patching__4d6edc1e-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
       "16  output__full_patching__4b9504d3-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
       "17                   461b9dfe-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
       "18           0a4e765d-Llama-3.2-3B-Instruct-mixed-500  Llama-3.2-3B-Instruct   \n",
       "19  output__full_patching__c5764fce-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
       "20  output__full_patching__3c315a04-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "21                    2ecfe6ea-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
       "22  output__full_patching__2ecfe6ea-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "23  output__full_patching__12f3cea0-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
       "24                   ba7c8b60-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
       "25                   ba7c8b60-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
       "26           755abfca-Llama-3.2-3B-Instruct-mixed-500  Llama-3.2-3B-Instruct   \n",
       "27                    234bdfaa-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
       "28  output__full_patching__e93b6877-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
       "29                     461ac385-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
       "30                     461ac385-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
       "31  output__full_patching__461b9dfe-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "32                    0a90e719-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
       "33                    0a90e719-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
       "34  output__full_patching__8760b763-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "35                    0a90e719-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
       "36                    0a90e719-Llama-3.2-3B-mixed-100           Llama-3.2-3B   \n",
       "37                  meta-llama__Llama-3.2-3B-Instruct  Llama-3.2-3B-Instruct   \n",
       "38  output__full_patching__e8e7ce8d-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "39           958352ea-Llama-3.2-3B-Instruct-mixed-100  Llama-3.2-3B-Instruct   \n",
       "40  output__full_patching__d1f6703c-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "41  output__full_patching__bd539cf5-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "42  output__full_patching__234bdfaa-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "43  output__full_patching__daac169e-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "44                     461ac385-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
       "45                     461ac385-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
       "46          9df250b6-Llama-3.2-3B-Instruct-mixed-1000  Llama-3.2-3B-Instruct   \n",
       "47  output__full_patching__8d72bbbe-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
       "48             368f5252-Llama-3.2-3B-Instruct-mixed-0  Llama-3.2-3B-Instruct   \n",
       "49                      8760b763-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
       "50                      8760b763-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
       "51                      a84197a7-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
       "52                      a84197a7-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
       "53  output__full_patching__80c6810f-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
       "54  output__full_patching__5c0eb32b-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
       "55  output__full_patching__d2d49ae1-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "56  output__full_patching__63d35748-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "57                    e45070ca-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
       "58                    e45070ca-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
       "59                      d1f6703c-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
       "60                      74ed1b01-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
       "61                      74ed1b01-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
       "62                    e45070ca-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
       "63                    e45070ca-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
       "64                     daac169e-Llama-3.2-3B-mixed-10           Llama-3.2-3B   \n",
       "65                    e8e7ce8d-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
       "66            9e5180ab-Llama-3.2-3B-Instruct-mixed-10  Llama-3.2-3B-Instruct   \n",
       "67                    dfe1b80e-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
       "68                    dfe1b80e-Llama-3.2-3B-mixed-500           Llama-3.2-3B   \n",
       "69                   63d35748-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
       "70                   63d35748-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
       "71                   4eb8fba9-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
       "72                   4eb8fba9-Llama-3.2-3B-mixed-1000           Llama-3.2-3B   \n",
       "73  output__full_patching__cb47c58d-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
       "74  output__full_patching__8ce7f0d6-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
       "75  output__full_patching__f276bbe3-Llama-3.2-3B-I...  Llama-3.2-3B-Instruct   \n",
       "76  output__full_patching__508aaf68-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "77  output__full_patching__a84197a7-Llama-3.2-3B-m...           Llama-3.2-3B   \n",
       "78                      74ed1b01-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
       "79                      74ed1b01-Llama-3.2-3B-mixed-0           Llama-3.2-3B   \n",
       "\n",
       "   embeddings_init task_name exact_match exact_match_stderr  \\\n",
       "0              NAN     mixed         NAN                NAN   \n",
       "1              NAN     mixed         NAN                NAN   \n",
       "2              NAN     mixed         NAN                NAN   \n",
       "3         new_only     mixed         NAN                NAN   \n",
       "4         new_only     mixed         NAN                NAN   \n",
       "5         new_only     mixed         NAN                NAN   \n",
       "6         new_only     mixed         NAN                NAN   \n",
       "7              NAN     mixed         NAN                NAN   \n",
       "8              NAN     mixed         NAN                NAN   \n",
       "9         new_only     mixed         NAN                NAN   \n",
       "10        new_only     mixed         NAN                NAN   \n",
       "11             NAN     mixed         NAN                NAN   \n",
       "12             NAN  baseline         NAN                NAN   \n",
       "13        new_only     mixed         NAN                NAN   \n",
       "14             NAN     mixed         NAN                NAN   \n",
       "15             NAN     mixed         NAN                NAN   \n",
       "16             NAN     mixed         NAN                NAN   \n",
       "17        new_only     mixed         NAN                NAN   \n",
       "18             NAN     mixed         NAN                NAN   \n",
       "19             NAN     mixed         NAN                NAN   \n",
       "20             NAN     mixed         NAN                NAN   \n",
       "21        new_only     mixed         NAN                NAN   \n",
       "22             NAN     mixed         NAN                NAN   \n",
       "23             NAN     mixed         NAN                NAN   \n",
       "24        new_only     mixed         NAN                NAN   \n",
       "25        new_only     mixed         NAN                NAN   \n",
       "26             NAN     mixed         NAN                NAN   \n",
       "27        new_only     mixed         NAN                NAN   \n",
       "28             NAN     mixed         NAN                NAN   \n",
       "29        new_only     mixed         NAN                NAN   \n",
       "30        new_only     mixed         NAN                NAN   \n",
       "31             NAN     mixed         NAN                NAN   \n",
       "32             NAN     mixed         NAN                NAN   \n",
       "33             NAN     mixed         NAN                NAN   \n",
       "34             NAN     mixed         NAN                NAN   \n",
       "35        new_only     mixed         NAN                NAN   \n",
       "36        new_only     mixed         NAN                NAN   \n",
       "37             NAN  baseline         NAN                NAN   \n",
       "38             NAN     mixed         NAN                NAN   \n",
       "39             NAN     mixed         NAN                NAN   \n",
       "40             NAN     mixed         NAN                NAN   \n",
       "41             NAN     mixed         NAN                NAN   \n",
       "42             NAN     mixed         NAN                NAN   \n",
       "43             NAN     mixed         NAN                NAN   \n",
       "44             NAN     mixed         NAN                NAN   \n",
       "45             NAN     mixed         NAN                NAN   \n",
       "46             NAN     mixed         NAN                NAN   \n",
       "47             NAN     mixed         NAN                NAN   \n",
       "48             NAN     mixed         NAN                NAN   \n",
       "49        new_only     mixed         NAN                NAN   \n",
       "50        new_only     mixed         NAN                NAN   \n",
       "51        new_only     mixed         NAN                NAN   \n",
       "52        new_only     mixed         NAN                NAN   \n",
       "53             NAN     mixed         NAN                NAN   \n",
       "54             NAN     mixed         NAN                NAN   \n",
       "55             NAN     mixed         NAN                NAN   \n",
       "56             NAN     mixed         NAN                NAN   \n",
       "57             NAN     mixed         NAN                NAN   \n",
       "58             NAN     mixed         NAN                NAN   \n",
       "59        new_only     mixed         NAN                NAN   \n",
       "60        new_only     mixed         NAN                NAN   \n",
       "61        new_only     mixed         NAN                NAN   \n",
       "62        new_only     mixed         NAN                NAN   \n",
       "63        new_only     mixed         NAN                NAN   \n",
       "64        new_only     mixed         NAN                NAN   \n",
       "65        new_only     mixed         NAN                NAN   \n",
       "66             NAN     mixed         NAN                NAN   \n",
       "67        new_only     mixed         NAN                NAN   \n",
       "68        new_only     mixed         NAN                NAN   \n",
       "69        new_only     mixed         NAN                NAN   \n",
       "70        new_only     mixed         NAN                NAN   \n",
       "71        new_only     mixed         NAN                NAN   \n",
       "72        new_only     mixed         NAN                NAN   \n",
       "73             NAN     mixed         NAN                NAN   \n",
       "74             NAN     mixed         NAN                NAN   \n",
       "75             NAN     mixed         NAN                NAN   \n",
       "76             NAN     mixed         NAN                NAN   \n",
       "77             NAN     mixed         NAN                NAN   \n",
       "78             NAN     mixed         NAN                NAN   \n",
       "79             NAN     mixed         NAN                NAN   \n",
       "\n",
       "    prompt_level_strict_acc  prompt_level_strict_acc_stderr  \\\n",
       "0                  0.192237                        0.016958   \n",
       "1                  0.134935                        0.014702   \n",
       "2                  0.327172                        0.020190   \n",
       "3                  0.663586                        0.020332   \n",
       "4                  0.658041                        0.020413   \n",
       "5                  0.693161                        0.019846   \n",
       "6                  0.698706                        0.019744   \n",
       "7                  0.685767                        0.019976   \n",
       "8                  0.177449                        0.016441   \n",
       "9                  0.678373                        0.020101   \n",
       "10                 0.678373                        0.020101   \n",
       "11                 0.519409                        0.021500   \n",
       "12                 0.195933                        0.017081   \n",
       "13                 0.685767                        0.019976   \n",
       "14                 0.606285                        0.021025   \n",
       "15                 0.510166                        0.021512   \n",
       "16                 0.656192                        0.020440   \n",
       "17                 0.632163                        0.020751   \n",
       "18                 0.523105                        0.021494   \n",
       "19                 0.704251                        0.019639   \n",
       "20                 0.194085                        0.017019   \n",
       "21                 0.659889                        0.020387   \n",
       "22                 0.142329                        0.015035   \n",
       "23                 0.656192                        0.020440   \n",
       "24                 0.643253                        0.020615   \n",
       "25                 0.648799                        0.020542   \n",
       "26                 0.628466                        0.020794   \n",
       "27                 0.643253                        0.020615   \n",
       "28                 0.528651                        0.021481   \n",
       "29                 0.689464                        0.019912   \n",
       "30                 0.704251                        0.019639   \n",
       "31                 0.171904                        0.016236   \n",
       "32                 0.138632                        0.014871   \n",
       "33                 0.390018                        0.020990   \n",
       "34                 0.205176                        0.017378   \n",
       "35                 0.674677                        0.020161   \n",
       "36                 0.650647                        0.020517   \n",
       "37                 0.706100                        0.019604   \n",
       "38                 0.166359                        0.016026   \n",
       "39                 0.486137                        0.021508   \n",
       "40                 0.149723                        0.015354   \n",
       "41                 0.177449                        0.016441   \n",
       "42                 0.197782                        0.017141   \n",
       "43                 0.188540                        0.016832   \n",
       "44                 0.171904                        0.016236   \n",
       "45                 0.414048                        0.021196   \n",
       "46                 0.484288                        0.021506   \n",
       "47                 0.554529                        0.021388   \n",
       "48                 0.556377                        0.021379   \n",
       "49                 0.704251                        0.019639   \n",
       "50                 0.696858                        0.019779   \n",
       "51                 0.696858                        0.019779   \n",
       "52                 0.704251                        0.019639   \n",
       "53                 0.621072                        0.020876   \n",
       "54                 0.659889                        0.020387   \n",
       "55                 0.186691                        0.016768   \n",
       "56                 0.184843                        0.016704   \n",
       "57                 0.138632                        0.014871   \n",
       "58                 0.343808                        0.020440   \n",
       "59                 0.696858                        0.019779   \n",
       "60                 0.704251                        0.019639   \n",
       "61                 0.696858                        0.019779   \n",
       "62                 0.652495                        0.020491   \n",
       "63                 0.650647                        0.020517   \n",
       "64                 0.685767                        0.019976   \n",
       "65                 0.661738                        0.020360   \n",
       "66                 0.534196                        0.021466   \n",
       "67                 0.654344                        0.020466   \n",
       "68                 0.646950                        0.020566   \n",
       "69                 0.676525                        0.020131   \n",
       "70                 0.639556                        0.020661   \n",
       "71                 0.652495                        0.020491   \n",
       "72                 0.628466                        0.020794   \n",
       "73                 0.704251                        0.019639   \n",
       "74                 0.449168                        0.021405   \n",
       "75                 0.674677                        0.020161   \n",
       "76                 0.207024                        0.017436   \n",
       "77                 0.195933                        0.017081   \n",
       "78                 0.153420                        0.015509   \n",
       "79                 0.415896                        0.021210   \n",
       "\n",
       "    inst_level_strict_acc inst_level_strict_acc_stderr  \\\n",
       "0                0.305755                          NAN   \n",
       "1                0.262590                          NAN   \n",
       "2                0.468825                          NAN   \n",
       "3                0.757794                          NAN   \n",
       "4                0.752998                          NAN   \n",
       "5                0.778177                          NAN   \n",
       "6                0.779376                          NAN   \n",
       "7                0.767386                          NAN   \n",
       "8                0.288969                          NAN   \n",
       "9                0.767386                          NAN   \n",
       "10               0.769784                          NAN   \n",
       "11               0.628297                          NAN   \n",
       "12               0.324940                          NAN   \n",
       "13               0.778177                          NAN   \n",
       "14               0.703837                          NAN   \n",
       "15               0.616307                          NAN   \n",
       "16               0.755396                          NAN   \n",
       "17               0.733813                          NAN   \n",
       "18               0.622302                          NAN   \n",
       "19               0.784173                          NAN   \n",
       "20               0.320144                          NAN   \n",
       "21               0.760192                          NAN   \n",
       "22               0.260192                          NAN   \n",
       "23               0.751799                          NAN   \n",
       "24               0.738609                          NAN   \n",
       "25               0.742206                          NAN   \n",
       "26               0.730216                          NAN   \n",
       "27               0.748201                          NAN   \n",
       "28               0.647482                          NAN   \n",
       "29               0.779376                          NAN   \n",
       "30               0.788969                          NAN   \n",
       "31               0.282974                          NAN   \n",
       "32               0.264988                          NAN   \n",
       "33               0.534772                          NAN   \n",
       "34               0.327338                          NAN   \n",
       "35               0.770983                          NAN   \n",
       "36               0.749400                          NAN   \n",
       "37               0.790168                          NAN   \n",
       "38               0.276978                          NAN   \n",
       "39               0.606715                          NAN   \n",
       "40               0.267386                          NAN   \n",
       "41               0.288969                          NAN   \n",
       "42               0.309353                          NAN   \n",
       "43               0.321343                          NAN   \n",
       "44               0.299760                          NAN   \n",
       "45               0.541966                          NAN   \n",
       "46               0.593525                          NAN   \n",
       "47               0.661871                          NAN   \n",
       "48               0.664269                          NAN   \n",
       "49               0.784173                          NAN   \n",
       "50               0.781775                          NAN   \n",
       "51               0.781775                          NAN   \n",
       "52               0.784173                          NAN   \n",
       "53               0.723022                          NAN   \n",
       "54               0.758993                          NAN   \n",
       "55               0.316547                          NAN   \n",
       "56               0.293765                          NAN   \n",
       "57               0.270983                          NAN   \n",
       "58               0.482014                          NAN   \n",
       "59               0.781775                          NAN   \n",
       "60               0.784173                          NAN   \n",
       "61               0.781775                          NAN   \n",
       "62               0.749400                          NAN   \n",
       "63               0.743405                          NAN   \n",
       "64               0.770983                          NAN   \n",
       "65               0.755396                          NAN   \n",
       "66               0.637890                          NAN   \n",
       "67               0.750600                          NAN   \n",
       "68               0.741007                          NAN   \n",
       "69               0.770983                          NAN   \n",
       "70               0.743405                          NAN   \n",
       "71               0.749400                          NAN   \n",
       "72               0.735012                          NAN   \n",
       "73               0.784173                          NAN   \n",
       "74               0.577938                          NAN   \n",
       "75               0.763789                          NAN   \n",
       "76               0.322542                          NAN   \n",
       "77               0.324940                          NAN   \n",
       "78               0.285372                          NAN   \n",
       "79               0.558753                          NAN   \n",
       "\n",
       "    prompt_level_loose_acc  prompt_level_loose_acc_stderr  \\\n",
       "0                 0.207024                       0.017436   \n",
       "1                 0.146026                       0.015196   \n",
       "2                 0.414048                       0.021196   \n",
       "3                 0.719039                       0.019342   \n",
       "4                 0.707948                       0.019567   \n",
       "5                 0.737523                       0.018934   \n",
       "6                 0.741220                       0.018847   \n",
       "7                 0.739372                       0.018891   \n",
       "8                 0.184843                       0.016704   \n",
       "9                 0.730129                       0.019102   \n",
       "10                0.726433                       0.019184   \n",
       "11                0.574861                       0.021274   \n",
       "12                0.218115                       0.017771   \n",
       "13                0.741220                       0.018847   \n",
       "14                0.652495                       0.020491   \n",
       "15                0.548983                       0.021413   \n",
       "16                0.700555                       0.019710   \n",
       "17                0.669131                       0.020248   \n",
       "18                0.563771                       0.021341   \n",
       "19                0.750462                       0.018622   \n",
       "20                0.210721                       0.017550   \n",
       "21                0.706100                       0.019604   \n",
       "22                0.155268                       0.015585   \n",
       "23                0.715342                       0.019419   \n",
       "24                0.670980                       0.020219   \n",
       "25                0.685767                       0.019976   \n",
       "26                0.674677                       0.020161   \n",
       "27                0.667283                       0.020277   \n",
       "28                0.602588                       0.021059   \n",
       "29                0.733826                       0.019019   \n",
       "30                0.748614                       0.018668   \n",
       "31                0.190388                       0.016895   \n",
       "32                0.164510                       0.015954   \n",
       "33                0.482440                       0.021503   \n",
       "34                0.223660                       0.017932   \n",
       "35                0.722736                       0.019264   \n",
       "36                0.713494                       0.019457   \n",
       "37                0.750462                       0.018622   \n",
       "38                0.186691                       0.016768   \n",
       "39                0.537893                       0.021455   \n",
       "40                0.162662                       0.015882   \n",
       "41                0.199630                       0.017201   \n",
       "42                0.212569                       0.017606   \n",
       "43                0.207024                       0.017436   \n",
       "44                0.179298                       0.016508   \n",
       "45                0.504621                       0.021516   \n",
       "46                0.534196                       0.021466   \n",
       "47                0.609982                       0.020990   \n",
       "48                0.621072                       0.020876   \n",
       "49                0.750462                       0.018622   \n",
       "50                0.746765                       0.018714   \n",
       "51                0.746765                       0.018714   \n",
       "52                0.750462                       0.018622   \n",
       "53                0.661738                       0.020360   \n",
       "54                0.707948                       0.019567   \n",
       "55                0.203327                       0.017320   \n",
       "56                0.192237                       0.016958   \n",
       "57                0.149723                       0.015354   \n",
       "58                0.428835                       0.021298   \n",
       "59                0.746765                       0.018714   \n",
       "60                0.750462                       0.018622   \n",
       "61                0.746765                       0.018714   \n",
       "62                0.689464                       0.019912   \n",
       "63                0.693161                       0.019846   \n",
       "64                0.730129                       0.019102   \n",
       "65                0.687616                       0.019944   \n",
       "66                0.582255                       0.021223   \n",
       "67                0.695009                       0.019813   \n",
       "68                0.682070                       0.020039   \n",
       "69                0.707948                       0.019567   \n",
       "70                0.678373                       0.020101   \n",
       "71                0.689464                       0.019912   \n",
       "72                0.670980                       0.020219   \n",
       "73                0.750462                       0.018622   \n",
       "74                0.493530                       0.021515   \n",
       "75                0.720887                       0.019303   \n",
       "76                0.227357                       0.018036   \n",
       "77                0.218115                       0.017771   \n",
       "78                0.166359                       0.016026   \n",
       "79                0.517560                       0.021503   \n",
       "\n",
       "    inst_level_loose_acc inst_level_loose_acc_stderr  compression_ratio  \\\n",
       "0               0.329736                         NAN           0.133380   \n",
       "1               0.273381                         NAN           0.101597   \n",
       "2               0.552758                         NAN           0.016155   \n",
       "3               0.799760                         NAN          -0.004844   \n",
       "4               0.793765                         NAN          -0.005003   \n",
       "5               0.814149                         NAN          -0.008155   \n",
       "6               0.816547                         NAN          -0.008277   \n",
       "7               0.810552                         NAN           0.020501   \n",
       "8               0.296163                         NAN           0.614980   \n",
       "9               0.808153                         NAN          -0.002788   \n",
       "10              0.806954                         NAN          -0.002878   \n",
       "11              0.679856                         NAN           0.033028   \n",
       "12              0.350120                         NAN           0.001207   \n",
       "13              0.822542                         NAN          -0.010551   \n",
       "14              0.743405                         NAN           0.042820   \n",
       "15              0.658273                         NAN           0.060745   \n",
       "16              0.790168                         NAN           0.037358   \n",
       "17              0.767386                         NAN           0.000799   \n",
       "18              0.660671                         NAN           0.068671   \n",
       "19              0.820144                         NAN          -0.010785   \n",
       "20              0.342926                         NAN           0.039745   \n",
       "21              0.799760                         NAN          -0.007376   \n",
       "22              0.282974                         NAN           0.040349   \n",
       "23              0.798561                         NAN          -0.001833   \n",
       "24              0.766187                         NAN           0.007573   \n",
       "25              0.773381                         NAN           0.007476   \n",
       "26              0.768585                         NAN           0.036607   \n",
       "27              0.773381                         NAN           0.002964   \n",
       "28              0.706235                         NAN           0.001851   \n",
       "29              0.814149                         NAN          -0.010191   \n",
       "30              0.826139                         NAN          -0.010125   \n",
       "31              0.320144                         NAN           0.121145   \n",
       "32              0.292566                         NAN           0.035638   \n",
       "33              0.609113                         NAN           0.017502   \n",
       "34              0.351319                         NAN          -0.000266   \n",
       "35              0.806954                         NAN          -0.007079   \n",
       "36              0.796163                         NAN          -0.007362   \n",
       "37              0.822542                         NAN          -0.010776   \n",
       "38              0.296163                         NAN           0.089000   \n",
       "39              0.653477                         NAN           0.028351   \n",
       "40              0.284173                         NAN          -0.001223   \n",
       "41              0.316547                         NAN           0.014895   \n",
       "42              0.338129                         NAN           0.147659   \n",
       "43              0.345324                         NAN           0.008682   \n",
       "44              0.312950                         NAN           0.017405   \n",
       "45              0.624700                         NAN          -0.001514   \n",
       "46              0.643885                         NAN           0.081260   \n",
       "47              0.714628                         NAN          -0.010000   \n",
       "48              0.724221                         NAN          -0.011691   \n",
       "49              0.820144                         NAN          -0.010785   \n",
       "50              0.818945                         NAN          -0.010761   \n",
       "51              0.818945                         NAN          -0.010761   \n",
       "52              0.820144                         NAN          -0.010785   \n",
       "53              0.755396                         NAN           0.044952   \n",
       "54              0.798561                         NAN           0.022516   \n",
       "55              0.341727                         NAN           0.006792   \n",
       "56              0.316547                         NAN           0.158073   \n",
       "57              0.280576                         NAN           0.087240   \n",
       "58              0.557554                         NAN           0.009105   \n",
       "59              0.818945                         NAN          -0.010761   \n",
       "60              0.820144                         NAN          -0.010785   \n",
       "61              0.818945                         NAN          -0.010761   \n",
       "62              0.786571                         NAN          -0.001875   \n",
       "63              0.781775                         NAN          -0.001883   \n",
       "64              0.809353                         NAN          -0.008610   \n",
       "65              0.781775                         NAN          -0.000953   \n",
       "66              0.681055                         NAN           0.000824   \n",
       "67              0.786571                         NAN          -0.000504   \n",
       "68              0.769784                         NAN          -0.000689   \n",
       "69              0.797362                         NAN           0.002623   \n",
       "70              0.776978                         NAN           0.002219   \n",
       "71              0.780576                         NAN           0.003067   \n",
       "72              0.769784                         NAN           0.001885   \n",
       "73              0.820144                         NAN          -0.010785   \n",
       "74              0.623501                         NAN           0.075506   \n",
       "75              0.800959                         NAN          -0.001153   \n",
       "76              0.347722                         NAN           0.031562   \n",
       "77              0.350120                         NAN           0.001207   \n",
       "78              0.303357                         NAN          -0.000226   \n",
       "79              0.640288                         NAN           0.012280   \n",
       "\n",
       "    learning_ratio  theoretical_compression_ratio   alias limit  \\\n",
       "0         0.034137                       0.170377  ifeval   NAN   \n",
       "1         0.022770                       0.122494  ifeval   NAN   \n",
       "2        -0.000890                       0.014179  ifeval   NAN   \n",
       "3         0.065750                       0.059344  ifeval   NAN   \n",
       "4         0.066387                       0.059817  ifeval   NAN   \n",
       "5         0.025803                       0.015955  ifeval   NAN   \n",
       "6         0.025612                       0.015627  ifeval   NAN   \n",
       "7         0.046151                       0.066113  ifeval   NAN   \n",
       "8         0.019093                       0.632944  ifeval   NAN   \n",
       "9         0.063923                       0.059693  ifeval   NAN   \n",
       "10        0.063872                       0.059529  ifeval   NAN   \n",
       "11        0.020466                       0.052770  ifeval   NAN   \n",
       "12        0.004820                       0.000000  ifeval   NAN   \n",
       "13        0.028006                       0.015644  ifeval   NAN   \n",
       "14        0.105766                       0.150301  ifeval   NAN   \n",
       "15        0.040485                       0.098851  ifeval   NAN   \n",
       "16        0.090833                       0.130479  ifeval   NAN   \n",
       "17        0.145470                       0.145546  ifeval   NAN   \n",
       "18        0.034358                       0.099375  ifeval   NAN   \n",
       "19        0.012398                       0.000000  ifeval   NAN   \n",
       "20        0.028531                       0.064240  ifeval   NAN   \n",
       "21        0.068191                       0.059037  ifeval   NAN   \n",
       "22        0.006330                       0.046784  ifeval   NAN   \n",
       "23        0.021714                       0.018278  ifeval   NAN   \n",
       "24        0.139264                       0.146980  ifeval   NAN   \n",
       "25        0.136249                       0.143803  ifeval   NAN   \n",
       "26        0.088294                       0.126927  ifeval   NAN   \n",
       "27        0.116043                       0.118194  ifeval   NAN   \n",
       "28        0.017905                       0.014476  ifeval   NAN   \n",
       "29        0.027429                       0.015465  ifeval   NAN   \n",
       "30        0.027598                       0.015702  ifeval   NAN   \n",
       "31        0.048861                       0.147647  ifeval   NAN   \n",
       "32        0.006736                       0.042507  ifeval   NAN   \n",
       "33       -0.001952                       0.010700  ifeval   NAN   \n",
       "34        0.006193                       0.000000  ifeval   NAN   \n",
       "35        0.067890                       0.059081  ifeval   NAN   \n",
       "36        0.068735                       0.059582  ifeval   NAN   \n",
       "37        0.012392                       0.000000  ifeval   NAN   \n",
       "38        0.010453                       0.100258  ifeval   NAN   \n",
       "39        0.025881                       0.048398  ifeval   NAN   \n",
       "40        0.001373                       0.000000  ifeval   NAN   \n",
       "41        0.002959                       0.017731  ifeval   NAN   \n",
       "42        0.043606                       0.192847  ifeval   NAN   \n",
       "43        0.015031                       0.015013  ifeval   NAN   \n",
       "44        0.001320                       0.018738  ifeval   NAN   \n",
       "45        0.021735                       0.003100  ifeval   NAN   \n",
       "46        0.058773                       0.118345  ifeval   NAN   \n",
       "47        0.011621                       0.000000  ifeval   NAN   \n",
       "48        0.017971                       0.000000  ifeval   NAN   \n",
       "49        0.012398                       0.000000  ifeval   NAN   \n",
       "50        0.012375                       0.000000  ifeval   NAN   \n",
       "51        0.012375                       0.000000  ifeval   NAN   \n",
       "52        0.012398                       0.000000  ifeval   NAN   \n",
       "53        0.106657                       0.155445  ifeval   NAN   \n",
       "54        0.045404                       0.067439  ifeval   NAN   \n",
       "55        0.018304                       0.016143  ifeval   NAN   \n",
       "56        0.043921                       0.204460  ifeval   NAN   \n",
       "57        0.012155                       0.100254  ifeval   NAN   \n",
       "58        0.011711                       0.013769  ifeval   NAN   \n",
       "59        0.012375                       0.000000  ifeval   NAN   \n",
       "60        0.012398                       0.000000  ifeval   NAN   \n",
       "61        0.012375                       0.000000  ifeval   NAN   \n",
       "62        0.120766                       0.117677  ifeval   NAN   \n",
       "63        0.121565                       0.118431  ifeval   NAN   \n",
       "64        0.026663                       0.016339  ifeval   NAN   \n",
       "65        0.121166                       0.119027  ifeval   NAN   \n",
       "66        0.039284                       0.013212  ifeval   NAN   \n",
       "67        0.119391                       0.117738  ifeval   NAN   \n",
       "68        0.118552                       0.116691  ifeval   NAN   \n",
       "69        0.139993                       0.142160  ifeval   NAN   \n",
       "70        0.141326                       0.143011  ifeval   NAN   \n",
       "71        0.142038                       0.144711  ifeval   NAN   \n",
       "72        0.145274                       0.146620  ifeval   NAN   \n",
       "73        0.012398                       0.000000  ifeval   NAN   \n",
       "74        0.038008                       0.114864  ifeval   NAN   \n",
       "75        0.021340                       0.018647  ifeval   NAN   \n",
       "76        0.040201                       0.068354  ifeval   NAN   \n",
       "77        0.004820                       0.000000  ifeval   NAN   \n",
       "78        0.000229                       0.000000  ifeval   NAN   \n",
       "79       -0.002480                       0.000000  ifeval   NAN   \n",
       "\n",
       "                                            json_path           dataset_str  \\\n",
       "0   eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "1   eval_results/full_patching/4eb8fba9-Llama-3.2-...               default   \n",
       "2   eval_results/full_patching/4eb8fba9-Llama-3.2-...               default   \n",
       "3   eval_results/full_patching/508aaf68-Llama-3.2-...               default   \n",
       "4   eval_results/full_patching/508aaf68-Llama-3.2-...               default   \n",
       "5   eval_results/full_patching/d2d49ae1-Llama-3.2-...  translation, default   \n",
       "6   eval_results/full_patching/d2d49ae1-Llama-3.2-...  translation, default   \n",
       "7   eval_results/full_patching/2916d55c-Llama-3.2-...               default   \n",
       "8   eval_results/full_patching/dfe1b80e-Llama-3.2-...               default   \n",
       "9   eval_results/full_patching/3c315a04-Llama-3.2-...  translation, default   \n",
       "10  eval_results/full_patching/3c315a04-Llama-3.2-...  translation, default   \n",
       "11  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "12  eval_results/full_patching/meta-llama__Llama-3...               unknown   \n",
       "13  eval_results/full_patching/bd539cf5-Llama-3.2-...  translation, default   \n",
       "14  eval_results/full_patching/6e1e953a-Llama-3.2-...               default   \n",
       "15  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "16  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "17  eval_results/full_patching/461b9dfe-Llama-3.2-...  translation, default   \n",
       "18  eval_results/full_patching/0a4e765d-Llama-3.2-...               default   \n",
       "19  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "20  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "21  eval_results/full_patching/2ecfe6ea-Llama-3.2-...  translation, default   \n",
       "22  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "23  eval_results/full_patching/output__full_patchi...               default   \n",
       "24  eval_results/full_patching/ba7c8b60-Llama-3.2-...  translation, default   \n",
       "25  eval_results/full_patching/ba7c8b60-Llama-3.2-...  translation, default   \n",
       "26  eval_results/full_patching/755abfca-Llama-3.2-...               default   \n",
       "27  eval_results/full_patching/234bdfaa-Llama-3.2-...  translation, default   \n",
       "28  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "29  eval_results/full_patching/461ac385-Llama-3.2-...               default   \n",
       "30  eval_results/full_patching/461ac385-Llama-3.2-...               default   \n",
       "31  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "32  eval_results/full_patching/0a90e719-Llama-3.2-...               default   \n",
       "33  eval_results/full_patching/0a90e719-Llama-3.2-...               default   \n",
       "34  eval_results/full_patching/output__full_patchi...               default   \n",
       "35  eval_results/full_patching/0a90e719-Llama-3.2-...               default   \n",
       "36  eval_results/full_patching/0a90e719-Llama-3.2-...               default   \n",
       "37  eval_results/full_patching/meta-llama__Llama-3...               unknown   \n",
       "38  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "39  eval_results/full_patching/958352ea-Llama-3.2-...               default   \n",
       "40  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "41  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "42  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "43  eval_results/full_patching/output__full_patchi...               default   \n",
       "44  eval_results/full_patching/461ac385-Llama-3.2-...               default   \n",
       "45  eval_results/full_patching/461ac385-Llama-3.2-...               default   \n",
       "46  eval_results/full_patching/9df250b6-Llama-3.2-...               default   \n",
       "47  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "48  eval_results/full_patching/368f5252-Llama-3.2-...               default   \n",
       "49  eval_results/full_patching/8760b763-Llama-3.2-...               default   \n",
       "50  eval_results/full_patching/8760b763-Llama-3.2-...               default   \n",
       "51  eval_results/full_patching/a84197a7-Llama-3.2-...  translation, default   \n",
       "52  eval_results/full_patching/a84197a7-Llama-3.2-...  translation, default   \n",
       "53  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "54  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "55  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "56  eval_results/full_patching/output__full_patchi...               default   \n",
       "57  eval_results/full_patching/e45070ca-Llama-3.2-...               default   \n",
       "58  eval_results/full_patching/e45070ca-Llama-3.2-...               default   \n",
       "59  eval_results/full_patching/d1f6703c-Llama-3.2-...  translation, default   \n",
       "60  eval_results/full_patching/74ed1b01-Llama-3.2-...               default   \n",
       "61  eval_results/full_patching/74ed1b01-Llama-3.2-...               default   \n",
       "62  eval_results/full_patching/e45070ca-Llama-3.2-...               default   \n",
       "63  eval_results/full_patching/e45070ca-Llama-3.2-...               default   \n",
       "64  eval_results/full_patching/daac169e-Llama-3.2-...               default   \n",
       "65  eval_results/full_patching/e8e7ce8d-Llama-3.2-...  translation, default   \n",
       "66  eval_results/full_patching/9e5180ab-Llama-3.2-...               default   \n",
       "67  eval_results/full_patching/dfe1b80e-Llama-3.2-...               default   \n",
       "68  eval_results/full_patching/dfe1b80e-Llama-3.2-...               default   \n",
       "69  eval_results/full_patching/63d35748-Llama-3.2-...               default   \n",
       "70  eval_results/full_patching/63d35748-Llama-3.2-...               default   \n",
       "71  eval_results/full_patching/4eb8fba9-Llama-3.2-...               default   \n",
       "72  eval_results/full_patching/4eb8fba9-Llama-3.2-...               default   \n",
       "73  eval_results/full_patching/output__full_patchi...               default   \n",
       "74  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "75  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "76  eval_results/full_patching/output__full_patchi...               default   \n",
       "77  eval_results/full_patching/output__full_patchi...  translation, default   \n",
       "78  eval_results/full_patching/74ed1b01-Llama-3.2-...               default   \n",
       "79  eval_results/full_patching/74ed1b01-Llama-3.2-...               default   \n",
       "\n",
       "   finetuning_params total_batch_size learning_rate main_loss_type  \\\n",
       "0    new_tokens_only             32.0        0.0005          mixed   \n",
       "1         embeddings             32.0        0.0005          mixed   \n",
       "2         embeddings             32.0        0.0005          mixed   \n",
       "3    new_tokens_only             32.0        0.0005          mixed   \n",
       "4    new_tokens_only             32.0        0.0005          mixed   \n",
       "5    new_tokens_only             32.0        0.0005          mixed   \n",
       "6    new_tokens_only             32.0        0.0005          mixed   \n",
       "7    new_tokens_only             32.0        0.0005          mixed   \n",
       "8    new_tokens_only             32.0        0.0005          mixed   \n",
       "9    new_tokens_only             32.0        0.0005          mixed   \n",
       "10   new_tokens_only             32.0        0.0005          mixed   \n",
       "11        embeddings             32.0        0.0005          mixed   \n",
       "12               NAN              NAN           NAN            NAN   \n",
       "13        embeddings             32.0        0.0005          mixed   \n",
       "14   new_tokens_only             32.0        0.0005          mixed   \n",
       "15        embeddings             32.0        0.0005          mixed   \n",
       "16   new_tokens_only             32.0        0.0005          mixed   \n",
       "17        embeddings             32.0        0.0005          mixed   \n",
       "18        embeddings             32.0        0.0005          mixed   \n",
       "19   new_tokens_only             32.0        0.0005          mixed   \n",
       "20   new_tokens_only             32.0        0.0005          mixed   \n",
       "21        embeddings             32.0        0.0005          mixed   \n",
       "22        embeddings             32.0        0.0005          mixed   \n",
       "23   new_tokens_only             32.0        0.0005          mixed   \n",
       "24   new_tokens_only             32.0        0.0005          mixed   \n",
       "25   new_tokens_only             32.0        0.0005          mixed   \n",
       "26   new_tokens_only             32.0        0.0005          mixed   \n",
       "27   new_tokens_only             32.0        0.0005          mixed   \n",
       "28        embeddings             32.0        0.0005          mixed   \n",
       "29        embeddings             32.0        0.0005          mixed   \n",
       "30        embeddings             32.0        0.0005          mixed   \n",
       "31        embeddings             32.0        0.0005          mixed   \n",
       "32        embeddings             32.0        0.0005          mixed   \n",
       "33        embeddings             32.0        0.0005          mixed   \n",
       "34   new_tokens_only             32.0        0.0005          mixed   \n",
       "35        embeddings             32.0        0.0005          mixed   \n",
       "36        embeddings             32.0        0.0005          mixed   \n",
       "37               NAN              NAN           NAN            NAN   \n",
       "38        embeddings             32.0        0.0005          mixed   \n",
       "39        embeddings             32.0        0.0005          mixed   \n",
       "40        embeddings             32.0        0.0005          mixed   \n",
       "41        embeddings             32.0        0.0005          mixed   \n",
       "42   new_tokens_only             32.0        0.0005          mixed   \n",
       "43   new_tokens_only             32.0        0.0005          mixed   \n",
       "44        embeddings             32.0        0.0005          mixed   \n",
       "45        embeddings             32.0        0.0005          mixed   \n",
       "46        embeddings             32.0        0.0005          mixed   \n",
       "47        embeddings             32.0        0.0005          mixed   \n",
       "48        embeddings             32.0        0.0005          mixed   \n",
       "49   new_tokens_only             32.0        0.0005          mixed   \n",
       "50   new_tokens_only             32.0        0.0005          mixed   \n",
       "51   new_tokens_only             32.0        0.0005          mixed   \n",
       "52   new_tokens_only             32.0        0.0005          mixed   \n",
       "53   new_tokens_only             32.0        0.0005          mixed   \n",
       "54   new_tokens_only             32.0        0.0005          mixed   \n",
       "55   new_tokens_only             32.0        0.0005          mixed   \n",
       "56   new_tokens_only             32.0        0.0005          mixed   \n",
       "57        embeddings             32.0        0.0005          mixed   \n",
       "58        embeddings             32.0        0.0005          mixed   \n",
       "59        embeddings             32.0        0.0005          mixed   \n",
       "60        embeddings             32.0        0.0005          mixed   \n",
       "61        embeddings             32.0        0.0005          mixed   \n",
       "62        embeddings             32.0        0.0005          mixed   \n",
       "63        embeddings             32.0        0.0005          mixed   \n",
       "64   new_tokens_only             32.0        0.0005          mixed   \n",
       "65        embeddings             32.0        0.0005          mixed   \n",
       "66        embeddings             32.0        0.0005          mixed   \n",
       "67   new_tokens_only             32.0        0.0005          mixed   \n",
       "68   new_tokens_only             32.0        0.0005          mixed   \n",
       "69   new_tokens_only             32.0        0.0005          mixed   \n",
       "70   new_tokens_only             32.0        0.0005          mixed   \n",
       "71        embeddings             32.0        0.0005          mixed   \n",
       "72        embeddings             32.0        0.0005          mixed   \n",
       "73   new_tokens_only             32.0        0.0005          mixed   \n",
       "74        embeddings             32.0        0.0005          mixed   \n",
       "75   new_tokens_only             32.0        0.0005          mixed   \n",
       "76   new_tokens_only             32.0        0.0005          mixed   \n",
       "77   new_tokens_only             32.0        0.0005          mixed   \n",
       "78        embeddings             32.0        0.0005          mixed   \n",
       "79        embeddings             32.0        0.0005          mixed   \n",
       "\n",
       "   embedding_init_strategy num_new_tokens unfreeze_params_steps  \\\n",
       "0                    merge         1000.0                  -1.0   \n",
       "1                    merge         1000.0                  -1.0   \n",
       "2                    merge         1000.0                  -1.0   \n",
       "3                    merge          100.0                  -1.0   \n",
       "4                    merge          100.0                  -1.0   \n",
       "5                    merge           10.0                  -1.0   \n",
       "6                    merge           10.0                  -1.0   \n",
       "7                    merge          100.0                  -1.0   \n",
       "8                    merge          500.0                  -1.0   \n",
       "9                    merge          100.0                  -1.0   \n",
       "10                   merge          100.0                  -1.0   \n",
       "11                   merge          100.0                  -1.0   \n",
       "12                     NAN            NAN                   NAN   \n",
       "13                   merge           10.0                  -1.0   \n",
       "14                   merge         1000.0                  -1.0   \n",
       "15                   merge          500.0                  -1.0   \n",
       "16                   merge          500.0                  -1.0   \n",
       "17                   merge         1000.0                  -1.0   \n",
       "18                   merge          500.0                  -1.0   \n",
       "19                     NAN            0.0                  -1.0   \n",
       "20                   merge          100.0                  -1.0   \n",
       "21                   merge          100.0                  -1.0   \n",
       "22                   merge          100.0                  -1.0   \n",
       "23                   merge           10.0                  -1.0   \n",
       "24                   merge         1000.0                  -1.0   \n",
       "25                   merge         1000.0                  -1.0   \n",
       "26                   merge          500.0                  -1.0   \n",
       "27                   merge          500.0                  -1.0   \n",
       "28                   merge           10.0                  -1.0   \n",
       "29                   merge           10.0                  -1.0   \n",
       "30                   merge           10.0                  -1.0   \n",
       "31                   merge         1000.0                  -1.0   \n",
       "32                   merge          100.0                  -1.0   \n",
       "33                   merge          100.0                  -1.0   \n",
       "34                     NAN            0.0                  -1.0   \n",
       "35                   merge          100.0                  -1.0   \n",
       "36                   merge          100.0                  -1.0   \n",
       "37                     NAN            NAN                   NAN   \n",
       "38                   merge          500.0                  -1.0   \n",
       "39                   merge          100.0                  -1.0   \n",
       "40                     NAN            0.0                  -1.0   \n",
       "41                   merge           10.0                  -1.0   \n",
       "42                   merge          500.0                  -1.0   \n",
       "43                   merge           10.0                  -1.0   \n",
       "44                   merge           10.0                  -1.0   \n",
       "45                   merge           10.0                  -1.0   \n",
       "46                   merge         1000.0                  -1.0   \n",
       "47                     NAN            0.0                  -1.0   \n",
       "48                     NAN            0.0                  -1.0   \n",
       "49                     NAN            0.0                  -1.0   \n",
       "50                     NAN            0.0                  -1.0   \n",
       "51                     NAN            0.0                  -1.0   \n",
       "52                     NAN            0.0                  -1.0   \n",
       "53                   merge         1000.0                  -1.0   \n",
       "54                   merge          100.0                  -1.0   \n",
       "55                   merge           10.0                  -1.0   \n",
       "56                   merge         1000.0                  -1.0   \n",
       "57                   merge          500.0                  -1.0   \n",
       "58                   merge          500.0                  -1.0   \n",
       "59                     NAN            0.0                  -1.0   \n",
       "60                     NAN            0.0                  -1.0   \n",
       "61                     NAN            0.0                  -1.0   \n",
       "62                   merge          500.0                  -1.0   \n",
       "63                   merge          500.0                  -1.0   \n",
       "64                   merge           10.0                  -1.0   \n",
       "65                   merge          500.0                  -1.0   \n",
       "66                   merge           10.0                  -1.0   \n",
       "67                   merge          500.0                  -1.0   \n",
       "68                   merge          500.0                  -1.0   \n",
       "69                   merge         1000.0                  -1.0   \n",
       "70                   merge         1000.0                  -1.0   \n",
       "71                   merge         1000.0                  -1.0   \n",
       "72                   merge         1000.0                  -1.0   \n",
       "73                     NAN            0.0                  -1.0   \n",
       "74                   merge         1000.0                  -1.0   \n",
       "75                   merge           10.0                  -1.0   \n",
       "76                   merge          100.0                  -1.0   \n",
       "77                     NAN            0.0                  -1.0   \n",
       "78                     NAN            0.0                  -1.0   \n",
       "79                     NAN            0.0                  -1.0   \n",
       "\n",
       "   finetune_params_prefreeze  \\\n",
       "0                        NAN   \n",
       "1                        NAN   \n",
       "2                        NAN   \n",
       "3                        NAN   \n",
       "4                        NAN   \n",
       "5                        NAN   \n",
       "6                        NAN   \n",
       "7                        NAN   \n",
       "8                        NAN   \n",
       "9                        NAN   \n",
       "10                       NAN   \n",
       "11                       NAN   \n",
       "12                       NAN   \n",
       "13                       NAN   \n",
       "14                       NAN   \n",
       "15                       NAN   \n",
       "16                       NAN   \n",
       "17                       NAN   \n",
       "18                       NAN   \n",
       "19                       NAN   \n",
       "20                       NAN   \n",
       "21                       NAN   \n",
       "22                       NAN   \n",
       "23                       NAN   \n",
       "24                       NAN   \n",
       "25                       NAN   \n",
       "26                       NAN   \n",
       "27                       NAN   \n",
       "28                       NAN   \n",
       "29                       NAN   \n",
       "30                       NAN   \n",
       "31                       NAN   \n",
       "32                       NAN   \n",
       "33                       NAN   \n",
       "34                       NAN   \n",
       "35                       NAN   \n",
       "36                       NAN   \n",
       "37                       NAN   \n",
       "38                       NAN   \n",
       "39                       NAN   \n",
       "40                       NAN   \n",
       "41                       NAN   \n",
       "42                       NAN   \n",
       "43                       NAN   \n",
       "44                       NAN   \n",
       "45                       NAN   \n",
       "46                       NAN   \n",
       "47                       NAN   \n",
       "48                       NAN   \n",
       "49                       NAN   \n",
       "50                       NAN   \n",
       "51                       NAN   \n",
       "52                       NAN   \n",
       "53                       NAN   \n",
       "54                       NAN   \n",
       "55                       NAN   \n",
       "56                       NAN   \n",
       "57                       NAN   \n",
       "58                       NAN   \n",
       "59                       NAN   \n",
       "60                       NAN   \n",
       "61                       NAN   \n",
       "62                       NAN   \n",
       "63                       NAN   \n",
       "64                       NAN   \n",
       "65                       NAN   \n",
       "66                       NAN   \n",
       "67                       NAN   \n",
       "68                       NAN   \n",
       "69                       NAN   \n",
       "70                       NAN   \n",
       "71                       NAN   \n",
       "72                       NAN   \n",
       "73                       NAN   \n",
       "74                       NAN   \n",
       "75                       NAN   \n",
       "76                       NAN   \n",
       "77                       NAN   \n",
       "78                       NAN   \n",
       "79                       NAN   \n",
       "\n",
       "                                              dataset  \\\n",
       "0   magpie-default-tokenized_1000,magpie-translati...   \n",
       "1                       magpie-default-tokenized_1000   \n",
       "2                       magpie-default-tokenized_1000   \n",
       "3                        magpie-default-tokenized_100   \n",
       "4                        magpie-default-tokenized_100   \n",
       "5   magpie-default-tokenized_10,magpie-translation...   \n",
       "6   magpie-default-tokenized_10,magpie-translation...   \n",
       "7                        magpie-default-tokenized_100   \n",
       "8                        magpie-default-tokenized_500   \n",
       "9   magpie-default-tokenized_100,magpie-translatio...   \n",
       "10  magpie-default-tokenized_100,magpie-translatio...   \n",
       "11  magpie-default-tokenized_100,magpie-translatio...   \n",
       "12                                                NAN   \n",
       "13  magpie-default-tokenized_10,magpie-translation...   \n",
       "14                      magpie-default-tokenized_1000   \n",
       "15  magpie-default-tokenized_500,magpie-translatio...   \n",
       "16  magpie-default-tokenized_500,magpie-translatio...   \n",
       "17  magpie-default-tokenized_1000,magpie-translati...   \n",
       "18                       magpie-default-tokenized_500   \n",
       "19  magpie-default-tokenized_0,magpie-translation-...   \n",
       "20  magpie-default-tokenized_100,magpie-translatio...   \n",
       "21  magpie-default-tokenized_100,magpie-translatio...   \n",
       "22  magpie-default-tokenized_100,magpie-translatio...   \n",
       "23                        magpie-default-tokenized_10   \n",
       "24  magpie-default-tokenized_1000,magpie-translati...   \n",
       "25  magpie-default-tokenized_1000,magpie-translati...   \n",
       "26                       magpie-default-tokenized_500   \n",
       "27  magpie-default-tokenized_500,magpie-translatio...   \n",
       "28  magpie-default-tokenized_10,magpie-translation...   \n",
       "29                        magpie-default-tokenized_10   \n",
       "30                        magpie-default-tokenized_10   \n",
       "31  magpie-default-tokenized_1000,magpie-translati...   \n",
       "32                       magpie-default-tokenized_100   \n",
       "33                       magpie-default-tokenized_100   \n",
       "34                         magpie-default-tokenized_0   \n",
       "35                       magpie-default-tokenized_100   \n",
       "36                       magpie-default-tokenized_100   \n",
       "37                                                NAN   \n",
       "38  magpie-default-tokenized_500,magpie-translatio...   \n",
       "39                       magpie-default-tokenized_100   \n",
       "40  magpie-default-tokenized_0,magpie-translation-...   \n",
       "41  magpie-default-tokenized_10,magpie-translation...   \n",
       "42  magpie-default-tokenized_500,magpie-translatio...   \n",
       "43                        magpie-default-tokenized_10   \n",
       "44                        magpie-default-tokenized_10   \n",
       "45                        magpie-default-tokenized_10   \n",
       "46                      magpie-default-tokenized_1000   \n",
       "47  magpie-default-tokenized_0,magpie-translation-...   \n",
       "48                         magpie-default-tokenized_0   \n",
       "49                         magpie-default-tokenized_0   \n",
       "50                         magpie-default-tokenized_0   \n",
       "51  magpie-default-tokenized_0,magpie-translation-...   \n",
       "52  magpie-default-tokenized_0,magpie-translation-...   \n",
       "53  magpie-default-tokenized_1000,magpie-translati...   \n",
       "54  magpie-default-tokenized_100,magpie-translatio...   \n",
       "55  magpie-default-tokenized_10,magpie-translation...   \n",
       "56                      magpie-default-tokenized_1000   \n",
       "57                       magpie-default-tokenized_500   \n",
       "58                       magpie-default-tokenized_500   \n",
       "59  magpie-default-tokenized_0,magpie-translation-...   \n",
       "60                         magpie-default-tokenized_0   \n",
       "61                         magpie-default-tokenized_0   \n",
       "62                       magpie-default-tokenized_500   \n",
       "63                       magpie-default-tokenized_500   \n",
       "64                        magpie-default-tokenized_10   \n",
       "65  magpie-default-tokenized_500,magpie-translatio...   \n",
       "66                        magpie-default-tokenized_10   \n",
       "67                       magpie-default-tokenized_500   \n",
       "68                       magpie-default-tokenized_500   \n",
       "69                      magpie-default-tokenized_1000   \n",
       "70                      magpie-default-tokenized_1000   \n",
       "71                      magpie-default-tokenized_1000   \n",
       "72                      magpie-default-tokenized_1000   \n",
       "73                         magpie-default-tokenized_0   \n",
       "74  magpie-default-tokenized_1000,magpie-translati...   \n",
       "75  magpie-default-tokenized_10,magpie-translation...   \n",
       "76                       magpie-default-tokenized_100   \n",
       "77  magpie-default-tokenized_0,magpie-translation-...   \n",
       "78                         magpie-default-tokenized_0   \n",
       "79                         magpie-default-tokenized_0   \n",
       "\n",
       "                                       tokenizer_path    seed reset_optimizer  \\\n",
       "0   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "1   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "2   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "3   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "4   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "5   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "6   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "7   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "8   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "9   /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "10  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "11  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "12                                                NAN     NAN             NAN   \n",
       "13  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "14  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "15  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "16  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "17  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "18  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "19                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "20  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "21  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "22  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "23  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "24  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "25  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "26  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "27  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "28  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "29  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "30  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "31  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "32  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "33  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "34                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "35  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "36  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "37                                                NAN     NAN             NAN   \n",
       "38  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "39  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "40                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "41  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "42  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "43  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "44  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "45  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "46  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "47                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "48                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "49                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "50                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "51                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "52                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "53  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "54  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "55  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "56  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "57  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "58  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "59                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "60                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "61                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "62  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "63  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "64  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "65  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "66  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "67  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "68  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "69  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "70  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "71  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "72  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "73                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "74  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "75  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "76  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "77                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "78                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "79                   meta-llama/Llama-3.2-3B-Instruct  1234.0           False   \n",
       "\n",
       "   warmup_steps lr_schedule warmup_steps_prefreeze lr_schedule_prefreeze  \n",
       "0         100.0      cosine                   -1.0                   NAN  \n",
       "1         100.0      cosine                   -1.0                   NAN  \n",
       "2         100.0      cosine                   -1.0                   NAN  \n",
       "3         100.0      cosine                   -1.0                   NAN  \n",
       "4         100.0      cosine                   -1.0                   NAN  \n",
       "5         100.0      cosine                   -1.0                   NAN  \n",
       "6         100.0      cosine                   -1.0                   NAN  \n",
       "7         100.0      cosine                   -1.0                   NAN  \n",
       "8         100.0      cosine                   -1.0                   NAN  \n",
       "9         100.0      cosine                   -1.0                   NAN  \n",
       "10        100.0      cosine                   -1.0                   NAN  \n",
       "11        100.0      cosine                   -1.0                   NAN  \n",
       "12          NAN         NAN                    NAN                   NAN  \n",
       "13        100.0      cosine                   -1.0                   NAN  \n",
       "14        100.0      cosine                   -1.0                   NAN  \n",
       "15        100.0      cosine                   -1.0                   NAN  \n",
       "16        100.0      cosine                   -1.0                   NAN  \n",
       "17        100.0      cosine                   -1.0                   NAN  \n",
       "18        100.0      cosine                   -1.0                   NAN  \n",
       "19        100.0      cosine                   -1.0                   NAN  \n",
       "20        100.0      cosine                   -1.0                   NAN  \n",
       "21        100.0      cosine                   -1.0                   NAN  \n",
       "22        100.0      cosine                   -1.0                   NAN  \n",
       "23        100.0      cosine                   -1.0                   NAN  \n",
       "24        100.0      cosine                   -1.0                   NAN  \n",
       "25        100.0      cosine                   -1.0                   NAN  \n",
       "26        100.0      cosine                   -1.0                   NAN  \n",
       "27        100.0      cosine                   -1.0                   NAN  \n",
       "28        100.0      cosine                   -1.0                   NAN  \n",
       "29        100.0      cosine                   -1.0                   NAN  \n",
       "30        100.0      cosine                   -1.0                   NAN  \n",
       "31        100.0      cosine                   -1.0                   NAN  \n",
       "32        100.0      cosine                   -1.0                   NAN  \n",
       "33        100.0      cosine                   -1.0                   NAN  \n",
       "34        100.0      cosine                   -1.0                   NAN  \n",
       "35        100.0      cosine                   -1.0                   NAN  \n",
       "36        100.0      cosine                   -1.0                   NAN  \n",
       "37          NAN         NAN                    NAN                   NAN  \n",
       "38        100.0      cosine                   -1.0                   NAN  \n",
       "39        100.0      cosine                   -1.0                   NAN  \n",
       "40        100.0      cosine                   -1.0                   NAN  \n",
       "41        100.0      cosine                   -1.0                   NAN  \n",
       "42        100.0      cosine                   -1.0                   NAN  \n",
       "43        100.0      cosine                   -1.0                   NAN  \n",
       "44        100.0      cosine                   -1.0                   NAN  \n",
       "45        100.0      cosine                   -1.0                   NAN  \n",
       "46        100.0      cosine                   -1.0                   NAN  \n",
       "47        100.0      cosine                   -1.0                   NAN  \n",
       "48        100.0      cosine                   -1.0                   NAN  \n",
       "49        100.0      cosine                   -1.0                   NAN  \n",
       "50        100.0      cosine                   -1.0                   NAN  \n",
       "51        100.0      cosine                   -1.0                   NAN  \n",
       "52        100.0      cosine                   -1.0                   NAN  \n",
       "53        100.0      cosine                   -1.0                   NAN  \n",
       "54        100.0      cosine                   -1.0                   NAN  \n",
       "55        100.0      cosine                   -1.0                   NAN  \n",
       "56        100.0      cosine                   -1.0                   NAN  \n",
       "57        100.0      cosine                   -1.0                   NAN  \n",
       "58        100.0      cosine                   -1.0                   NAN  \n",
       "59        100.0      cosine                   -1.0                   NAN  \n",
       "60        100.0      cosine                   -1.0                   NAN  \n",
       "61        100.0      cosine                   -1.0                   NAN  \n",
       "62        100.0      cosine                   -1.0                   NAN  \n",
       "63        100.0      cosine                   -1.0                   NAN  \n",
       "64        100.0      cosine                   -1.0                   NAN  \n",
       "65        100.0      cosine                   -1.0                   NAN  \n",
       "66        100.0      cosine                   -1.0                   NAN  \n",
       "67        100.0      cosine                   -1.0                   NAN  \n",
       "68        100.0      cosine                   -1.0                   NAN  \n",
       "69        100.0      cosine                   -1.0                   NAN  \n",
       "70        100.0      cosine                   -1.0                   NAN  \n",
       "71        100.0      cosine                   -1.0                   NAN  \n",
       "72        100.0      cosine                   -1.0                   NAN  \n",
       "73        100.0      cosine                   -1.0                   NAN  \n",
       "74        100.0      cosine                   -1.0                   NAN  \n",
       "75        100.0      cosine                   -1.0                   NAN  \n",
       "76        100.0      cosine                   -1.0                   NAN  \n",
       "77        100.0      cosine                   -1.0                   NAN  \n",
       "78        100.0      cosine                   -1.0                   NAN  \n",
       "79        100.0      cosine                   -1.0                   NAN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dir = \"eval_results\"\n",
    "\n",
    "def get_results(results_file_name: str, experiment: str):\n",
    "    results_file = os.path.join(results_dir, experiment, results_file_name)\n",
    "\n",
    "    df = pd.read_csv(results_file)\n",
    "\n",
    "    main_results = df[df[\"alias\"] == \"ifeval\"]\n",
    "    main_results.fillna(\"NAN\", inplace=True)\n",
    "    # main_results = main_results[main_results[\"limit\"] == \"NAN\"]\n",
    "    main_results[\"task_name\"].replace(\"ifeval\", \"baseline\", inplace=True)\n",
    "    for _, row in main_results.iterrows():\n",
    "        if not \"num_new_tokens\" in row.keys() or row[\"num_new_tokens\"] == \"NAN\":\n",
    "            model_name = row[\"model_name_sanitized\"]\n",
    "            model_name_split = model_name.split(\"-\")\n",
    "            try:\n",
    "                num_toks = int(model_name_split[-1])\n",
    "                main_results.loc[row.name, \"num_new_tokens\"] = num_toks\n",
    "            except:\n",
    "                print(f\"Error: {row}\")\n",
    "    return main_results\n",
    "                \n",
    "\n",
    "def get_aggregated_results(result_df: pd.DataFrame, column: list[str]) -> pd.DataFrame:\n",
    "    aggregated_results = result_df.groupby(column).agg({\n",
    "        \"prompt_level_strict_acc\": [\"mean\", \"std\", \"count\", \"max\"],\n",
    "        \"inst_level_strict_acc\": [\"mean\", \"std\", \"count\", \"max\"],\n",
    "        \"prompt_level_loose_acc\": [\"mean\", \"std\", \"count\", \"max\"],\n",
    "        \"inst_level_loose_acc\": [\"mean\", \"std\", \"count\", \"max\"],\n",
    "        \"compression_ratio\": [\"mean\"],\n",
    "        \"learning_ratio\": [\"mean\"],\n",
    "        \"theoretical_compression_ratio\": [\"mean\"],\n",
    "    })\n",
    "    return aggregated_results\n",
    "\n",
    "# FOR IF EVAL\n",
    "# experiment = \"templating\" # instruct with no embeddings trained\n",
    "# experiment = \"baseline_embeddings\"\n",
    "# experiment = \"patching\"\n",
    "# experiment = \"longer_embeddings\" # instruct with embeddings trained\n",
    "experiment = \"full_patching\"\n",
    "results_file_name = \"eval_results.csv\"\n",
    "main_results = get_results(results_file_name, experiment)\n",
    "# main_results.sort_values(by=\"exact_match\", ascending=True)\n",
    "print(main_results.sort_values(by=\"prompt_level_strict_acc\", ascending=True))\n",
    "\n",
    "if experiment == \"longer_embeddings\":\n",
    "    main_results.loc[main_results[\"finetuning_params\"] == \"embeddings\", \"finetuning_params\"] = \"lora\"\n",
    "\n",
    "# templating_results = get_results(results_file_name, \"templating\")\n",
    "# patching_results = get_results(results_file_name, \"patching\")\n",
    "# main_results = pd.concat([templating_results, patching_results])\n",
    "main_results.sort_values(by=\"prompt_level_strict_acc\", ascending=True)\n",
    "main_results.fillna(\"NAN\", inplace=True)\n",
    "display(main_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>num_new_tokens</th>\n",
       "      <th>finetuning_params</th>\n",
       "      <th>model_name</th>\n",
       "      <th>trained_model_name</th>\n",
       "      <th>model_name_sanitized</th>\n",
       "      <th>dataset</th>\n",
       "      <th colspan=\"4\" halign=\"left\">prompt_level_strict_acc</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>learning_ratio</th>\n",
       "      <th>theoretical_compression_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>74ed1b01-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>magpie-default-tokenized_0</td>\n",
       "      <td>0.153420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153420</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__d1f6703c-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__d1f6703c-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_0,magpie-translation-...</td>\n",
       "      <td>0.149723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.149723</td>\n",
       "      <td>-0.001223</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>368f5252-Llama-3.2-3B-Instruct-mixed-0</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>magpie-default-tokenized_0</td>\n",
       "      <td>0.556377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.556377</td>\n",
       "      <td>-0.011691</td>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__8d72bbbe-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__8d72bbbe-Llama-3.2-3B-I...</td>\n",
       "      <td>magpie-default-tokenized_0,magpie-translation-...</td>\n",
       "      <td>0.554529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.554529</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__8760b763-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__8760b763-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_0</td>\n",
       "      <td>0.205176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.205176</td>\n",
       "      <td>-0.000266</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__a84197a7-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__a84197a7-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_0,magpie-translation-...</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__c5764fce-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__c5764fce-Llama-3.2-3B-I...</td>\n",
       "      <td>magpie-default-tokenized_0,magpie-translation-...</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>-0.010785</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__cb47c58d-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__cb47c58d-Llama-3.2-3B-I...</td>\n",
       "      <td>magpie-default-tokenized_0</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>-0.010785</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>461ac385-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>magpie-default-tokenized_10</td>\n",
       "      <td>0.171904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.171904</td>\n",
       "      <td>0.017405</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.018738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__bd539cf5-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__bd539cf5-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>0.177449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.177449</td>\n",
       "      <td>0.014895</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.017731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>9e5180ab-Llama-3.2-3B-Instruct-mixed-10</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>magpie-default-tokenized_10</td>\n",
       "      <td>0.534196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.534196</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.039284</td>\n",
       "      <td>0.013212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__e93b6877-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__e93b6877-Llama-3.2-3B-I...</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>0.528651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.528651</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.017905</td>\n",
       "      <td>0.014476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__d2d49ae1-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__d2d49ae1-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>0.186691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186691</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>0.018304</td>\n",
       "      <td>0.016143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__daac169e-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__daac169e-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_10</td>\n",
       "      <td>0.188540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.188540</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.015031</td>\n",
       "      <td>0.015013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__12f3cea0-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__12f3cea0-Llama-3.2-3B-I...</td>\n",
       "      <td>magpie-default-tokenized_10</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>-0.001833</td>\n",
       "      <td>0.021714</td>\n",
       "      <td>0.018278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__f276bbe3-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__f276bbe3-Llama-3.2-3B-I...</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>0.674677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.674677</td>\n",
       "      <td>-0.001153</td>\n",
       "      <td>0.021340</td>\n",
       "      <td>0.018647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>0a90e719-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>magpie-default-tokenized_100</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>0.035638</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.042507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__2ecfe6ea-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__2ecfe6ea-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>0.142329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142329</td>\n",
       "      <td>0.040349</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>0.046784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>958352ea-Llama-3.2-3B-Instruct-mixed-100</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>magpie-default-tokenized_100</td>\n",
       "      <td>0.486137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.486137</td>\n",
       "      <td>0.028351</td>\n",
       "      <td>0.025881</td>\n",
       "      <td>0.048398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__e5175fd2-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__e5175fd2-Llama-3.2-3B-I...</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>0.519409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.519409</td>\n",
       "      <td>0.033028</td>\n",
       "      <td>0.020466</td>\n",
       "      <td>0.052770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__3c315a04-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__3c315a04-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>0.194085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.194085</td>\n",
       "      <td>0.039745</td>\n",
       "      <td>0.028531</td>\n",
       "      <td>0.064240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__508aaf68-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__508aaf68-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_100</td>\n",
       "      <td>0.207024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.207024</td>\n",
       "      <td>0.031562</td>\n",
       "      <td>0.040201</td>\n",
       "      <td>0.068354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>2916d55c-Llama-3.2-3B-Instruct-mixed-100</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>magpie-default-tokenized_100</td>\n",
       "      <td>0.685767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.685767</td>\n",
       "      <td>0.020501</td>\n",
       "      <td>0.046151</td>\n",
       "      <td>0.066113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__5c0eb32b-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__5c0eb32b-Llama-3.2-3B-I...</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>0.659889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.659889</td>\n",
       "      <td>0.022516</td>\n",
       "      <td>0.045404</td>\n",
       "      <td>0.067439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>500.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>e45070ca-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>magpie-default-tokenized_500</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>0.087240</td>\n",
       "      <td>0.012155</td>\n",
       "      <td>0.100254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>500.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__e8e7ce8d-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__e8e7ce8d-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_500,magpie-translatio...</td>\n",
       "      <td>0.166359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166359</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>0.100258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>500.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>0a4e765d-Llama-3.2-3B-Instruct-mixed-500</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>magpie-default-tokenized_500</td>\n",
       "      <td>0.523105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.523105</td>\n",
       "      <td>0.068671</td>\n",
       "      <td>0.034358</td>\n",
       "      <td>0.099375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>500.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__4d6edc1e-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__4d6edc1e-Llama-3.2-3B-I...</td>\n",
       "      <td>magpie-default-tokenized_500,magpie-translatio...</td>\n",
       "      <td>0.510166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.510166</td>\n",
       "      <td>0.060745</td>\n",
       "      <td>0.040485</td>\n",
       "      <td>0.098851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>500.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>dfe1b80e-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>magpie-default-tokenized_500</td>\n",
       "      <td>0.177449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.177449</td>\n",
       "      <td>0.614980</td>\n",
       "      <td>0.019093</td>\n",
       "      <td>0.632944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>500.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__234bdfaa-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__234bdfaa-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_500,magpie-translatio...</td>\n",
       "      <td>0.197782</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.197782</td>\n",
       "      <td>0.147659</td>\n",
       "      <td>0.043606</td>\n",
       "      <td>0.192847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>500.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>755abfca-Llama-3.2-3B-Instruct-mixed-500</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>magpie-default-tokenized_500</td>\n",
       "      <td>0.628466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.628466</td>\n",
       "      <td>0.036607</td>\n",
       "      <td>0.088294</td>\n",
       "      <td>0.126927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>500.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__4b9504d3-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__4b9504d3-Llama-3.2-3B-I...</td>\n",
       "      <td>magpie-default-tokenized_500,magpie-translatio...</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>0.037358</td>\n",
       "      <td>0.090833</td>\n",
       "      <td>0.130479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>4eb8fba9-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>magpie-default-tokenized_1000</td>\n",
       "      <td>0.134935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.134935</td>\n",
       "      <td>0.101597</td>\n",
       "      <td>0.022770</td>\n",
       "      <td>0.122494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__461b9dfe-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__461b9dfe-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>0.171904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.171904</td>\n",
       "      <td>0.121145</td>\n",
       "      <td>0.048861</td>\n",
       "      <td>0.147647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>9df250b6-Llama-3.2-3B-Instruct-mixed-1000</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>magpie-default-tokenized_1000</td>\n",
       "      <td>0.484288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.484288</td>\n",
       "      <td>0.081260</td>\n",
       "      <td>0.058773</td>\n",
       "      <td>0.118345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__8ce7f0d6-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__8ce7f0d6-Llama-3.2-3B-I...</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>0.449168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.449168</td>\n",
       "      <td>0.075506</td>\n",
       "      <td>0.038008</td>\n",
       "      <td>0.114864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__63d35748-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__63d35748-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_1000</td>\n",
       "      <td>0.184843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.184843</td>\n",
       "      <td>0.158073</td>\n",
       "      <td>0.043921</td>\n",
       "      <td>0.204460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__ba7c8b60-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__ba7c8b60-Llama-3.2-3B-m...</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>0.192237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.192237</td>\n",
       "      <td>0.133380</td>\n",
       "      <td>0.034137</td>\n",
       "      <td>0.170377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>6e1e953a-Llama-3.2-3B-Instruct-mixed-1000</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>magpie-default-tokenized_1000</td>\n",
       "      <td>0.606285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.606285</td>\n",
       "      <td>0.042820</td>\n",
       "      <td>0.105766</td>\n",
       "      <td>0.150301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__80c6810f-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__80c6810f-Llama-3.2-3B-I...</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>0.621072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.621072</td>\n",
       "      <td>0.044952</td>\n",
       "      <td>0.106657</td>\n",
       "      <td>0.155445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.706100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706100</td>\n",
       "      <td>-0.010776</td>\n",
       "      <td>0.012392</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_new_tokens finetuning_params             model_name  \\\n",
       "                                                             \n",
       "0             0.0        embeddings           Llama-3.2-3B   \n",
       "1             0.0        embeddings           Llama-3.2-3B   \n",
       "2             0.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "3             0.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "4             0.0   new_tokens_only           Llama-3.2-3B   \n",
       "5             0.0   new_tokens_only           Llama-3.2-3B   \n",
       "6             0.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "7             0.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "8            10.0        embeddings           Llama-3.2-3B   \n",
       "9            10.0        embeddings           Llama-3.2-3B   \n",
       "10           10.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "11           10.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "12           10.0   new_tokens_only           Llama-3.2-3B   \n",
       "13           10.0   new_tokens_only           Llama-3.2-3B   \n",
       "14           10.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "15           10.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "16          100.0        embeddings           Llama-3.2-3B   \n",
       "17          100.0        embeddings           Llama-3.2-3B   \n",
       "18          100.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "19          100.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "20          100.0   new_tokens_only           Llama-3.2-3B   \n",
       "21          100.0   new_tokens_only           Llama-3.2-3B   \n",
       "22          100.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "23          100.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "24          500.0        embeddings           Llama-3.2-3B   \n",
       "25          500.0        embeddings           Llama-3.2-3B   \n",
       "26          500.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "27          500.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "28          500.0   new_tokens_only           Llama-3.2-3B   \n",
       "29          500.0   new_tokens_only           Llama-3.2-3B   \n",
       "30          500.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "31          500.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "32         1000.0        embeddings           Llama-3.2-3B   \n",
       "33         1000.0        embeddings           Llama-3.2-3B   \n",
       "34         1000.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "35         1000.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "36         1000.0   new_tokens_only           Llama-3.2-3B   \n",
       "37         1000.0   new_tokens_only           Llama-3.2-3B   \n",
       "38         1000.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "39         1000.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "40            NAN               NAN                    NAN   \n",
       "41            NAN               NAN                    NAN   \n",
       "\n",
       "                                   trained_model_name  \\\n",
       "                                                        \n",
       "0                       74ed1b01-Llama-3.2-3B-mixed-0   \n",
       "1   output__full_patching__d1f6703c-Llama-3.2-3B-m...   \n",
       "2              368f5252-Llama-3.2-3B-Instruct-mixed-0   \n",
       "3   output__full_patching__8d72bbbe-Llama-3.2-3B-I...   \n",
       "4   output__full_patching__8760b763-Llama-3.2-3B-m...   \n",
       "5   output__full_patching__a84197a7-Llama-3.2-3B-m...   \n",
       "6   output__full_patching__c5764fce-Llama-3.2-3B-I...   \n",
       "7   output__full_patching__cb47c58d-Llama-3.2-3B-I...   \n",
       "8                      461ac385-Llama-3.2-3B-mixed-10   \n",
       "9   output__full_patching__bd539cf5-Llama-3.2-3B-m...   \n",
       "10            9e5180ab-Llama-3.2-3B-Instruct-mixed-10   \n",
       "11  output__full_patching__e93b6877-Llama-3.2-3B-I...   \n",
       "12  output__full_patching__d2d49ae1-Llama-3.2-3B-m...   \n",
       "13  output__full_patching__daac169e-Llama-3.2-3B-m...   \n",
       "14  output__full_patching__12f3cea0-Llama-3.2-3B-I...   \n",
       "15  output__full_patching__f276bbe3-Llama-3.2-3B-I...   \n",
       "16                    0a90e719-Llama-3.2-3B-mixed-100   \n",
       "17  output__full_patching__2ecfe6ea-Llama-3.2-3B-m...   \n",
       "18           958352ea-Llama-3.2-3B-Instruct-mixed-100   \n",
       "19  output__full_patching__e5175fd2-Llama-3.2-3B-I...   \n",
       "20  output__full_patching__3c315a04-Llama-3.2-3B-m...   \n",
       "21  output__full_patching__508aaf68-Llama-3.2-3B-m...   \n",
       "22           2916d55c-Llama-3.2-3B-Instruct-mixed-100   \n",
       "23  output__full_patching__5c0eb32b-Llama-3.2-3B-I...   \n",
       "24                    e45070ca-Llama-3.2-3B-mixed-500   \n",
       "25  output__full_patching__e8e7ce8d-Llama-3.2-3B-m...   \n",
       "26           0a4e765d-Llama-3.2-3B-Instruct-mixed-500   \n",
       "27  output__full_patching__4d6edc1e-Llama-3.2-3B-I...   \n",
       "28                    dfe1b80e-Llama-3.2-3B-mixed-500   \n",
       "29  output__full_patching__234bdfaa-Llama-3.2-3B-m...   \n",
       "30           755abfca-Llama-3.2-3B-Instruct-mixed-500   \n",
       "31  output__full_patching__4b9504d3-Llama-3.2-3B-I...   \n",
       "32                   4eb8fba9-Llama-3.2-3B-mixed-1000   \n",
       "33  output__full_patching__461b9dfe-Llama-3.2-3B-m...   \n",
       "34          9df250b6-Llama-3.2-3B-Instruct-mixed-1000   \n",
       "35  output__full_patching__8ce7f0d6-Llama-3.2-3B-I...   \n",
       "36  output__full_patching__63d35748-Llama-3.2-3B-m...   \n",
       "37  output__full_patching__ba7c8b60-Llama-3.2-3B-m...   \n",
       "38          6e1e953a-Llama-3.2-3B-Instruct-mixed-1000   \n",
       "39  output__full_patching__80c6810f-Llama-3.2-3B-I...   \n",
       "40                           meta-llama__Llama-3.2-3B   \n",
       "41                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "\n",
       "                                 model_name_sanitized  \\\n",
       "                                                        \n",
       "0                            meta-llama__Llama-3.2-3B   \n",
       "1   output__full_patching__d1f6703c-Llama-3.2-3B-m...   \n",
       "2                   meta-llama__Llama-3.2-3B-Instruct   \n",
       "3   output__full_patching__8d72bbbe-Llama-3.2-3B-I...   \n",
       "4   output__full_patching__8760b763-Llama-3.2-3B-m...   \n",
       "5   output__full_patching__a84197a7-Llama-3.2-3B-m...   \n",
       "6   output__full_patching__c5764fce-Llama-3.2-3B-I...   \n",
       "7   output__full_patching__cb47c58d-Llama-3.2-3B-I...   \n",
       "8                            meta-llama__Llama-3.2-3B   \n",
       "9   output__full_patching__bd539cf5-Llama-3.2-3B-m...   \n",
       "10                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "11  output__full_patching__e93b6877-Llama-3.2-3B-I...   \n",
       "12  output__full_patching__d2d49ae1-Llama-3.2-3B-m...   \n",
       "13  output__full_patching__daac169e-Llama-3.2-3B-m...   \n",
       "14  output__full_patching__12f3cea0-Llama-3.2-3B-I...   \n",
       "15  output__full_patching__f276bbe3-Llama-3.2-3B-I...   \n",
       "16                           meta-llama__Llama-3.2-3B   \n",
       "17  output__full_patching__2ecfe6ea-Llama-3.2-3B-m...   \n",
       "18                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "19  output__full_patching__e5175fd2-Llama-3.2-3B-I...   \n",
       "20  output__full_patching__3c315a04-Llama-3.2-3B-m...   \n",
       "21  output__full_patching__508aaf68-Llama-3.2-3B-m...   \n",
       "22                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "23  output__full_patching__5c0eb32b-Llama-3.2-3B-I...   \n",
       "24                           meta-llama__Llama-3.2-3B   \n",
       "25  output__full_patching__e8e7ce8d-Llama-3.2-3B-m...   \n",
       "26                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "27  output__full_patching__4d6edc1e-Llama-3.2-3B-I...   \n",
       "28                           meta-llama__Llama-3.2-3B   \n",
       "29  output__full_patching__234bdfaa-Llama-3.2-3B-m...   \n",
       "30                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "31  output__full_patching__4b9504d3-Llama-3.2-3B-I...   \n",
       "32                           meta-llama__Llama-3.2-3B   \n",
       "33  output__full_patching__461b9dfe-Llama-3.2-3B-m...   \n",
       "34                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "35  output__full_patching__8ce7f0d6-Llama-3.2-3B-I...   \n",
       "36  output__full_patching__63d35748-Llama-3.2-3B-m...   \n",
       "37  output__full_patching__ba7c8b60-Llama-3.2-3B-m...   \n",
       "38                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "39  output__full_patching__80c6810f-Llama-3.2-3B-I...   \n",
       "40                           meta-llama__Llama-3.2-3B   \n",
       "41                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "\n",
       "                                              dataset prompt_level_strict_acc  \\\n",
       "                                                                         mean   \n",
       "0                          magpie-default-tokenized_0                0.153420   \n",
       "1   magpie-default-tokenized_0,magpie-translation-...                0.149723   \n",
       "2                          magpie-default-tokenized_0                0.556377   \n",
       "3   magpie-default-tokenized_0,magpie-translation-...                0.554529   \n",
       "4                          magpie-default-tokenized_0                0.205176   \n",
       "5   magpie-default-tokenized_0,magpie-translation-...                0.195933   \n",
       "6   magpie-default-tokenized_0,magpie-translation-...                0.704251   \n",
       "7                          magpie-default-tokenized_0                0.704251   \n",
       "8                         magpie-default-tokenized_10                0.171904   \n",
       "9   magpie-default-tokenized_10,magpie-translation...                0.177449   \n",
       "10                        magpie-default-tokenized_10                0.534196   \n",
       "11  magpie-default-tokenized_10,magpie-translation...                0.528651   \n",
       "12  magpie-default-tokenized_10,magpie-translation...                0.186691   \n",
       "13                        magpie-default-tokenized_10                0.188540   \n",
       "14                        magpie-default-tokenized_10                0.656192   \n",
       "15  magpie-default-tokenized_10,magpie-translation...                0.674677   \n",
       "16                       magpie-default-tokenized_100                0.138632   \n",
       "17  magpie-default-tokenized_100,magpie-translatio...                0.142329   \n",
       "18                       magpie-default-tokenized_100                0.486137   \n",
       "19  magpie-default-tokenized_100,magpie-translatio...                0.519409   \n",
       "20  magpie-default-tokenized_100,magpie-translatio...                0.194085   \n",
       "21                       magpie-default-tokenized_100                0.207024   \n",
       "22                       magpie-default-tokenized_100                0.685767   \n",
       "23  magpie-default-tokenized_100,magpie-translatio...                0.659889   \n",
       "24                       magpie-default-tokenized_500                0.138632   \n",
       "25  magpie-default-tokenized_500,magpie-translatio...                0.166359   \n",
       "26                       magpie-default-tokenized_500                0.523105   \n",
       "27  magpie-default-tokenized_500,magpie-translatio...                0.510166   \n",
       "28                       magpie-default-tokenized_500                0.177449   \n",
       "29  magpie-default-tokenized_500,magpie-translatio...                0.197782   \n",
       "30                       magpie-default-tokenized_500                0.628466   \n",
       "31  magpie-default-tokenized_500,magpie-translatio...                0.656192   \n",
       "32                      magpie-default-tokenized_1000                0.134935   \n",
       "33  magpie-default-tokenized_1000,magpie-translati...                0.171904   \n",
       "34                      magpie-default-tokenized_1000                0.484288   \n",
       "35  magpie-default-tokenized_1000,magpie-translati...                0.449168   \n",
       "36                      magpie-default-tokenized_1000                0.184843   \n",
       "37  magpie-default-tokenized_1000,magpie-translati...                0.192237   \n",
       "38                      magpie-default-tokenized_1000                0.606285   \n",
       "39  magpie-default-tokenized_1000,magpie-translati...                0.621072   \n",
       "40                                                NAN                0.195933   \n",
       "41                                                NAN                0.706100   \n",
       "\n",
       "                       compression_ratio learning_ratio  \\\n",
       "   std count       max              mean           mean   \n",
       "0  NaN     1  0.153420         -0.000226       0.000229   \n",
       "1  NaN     1  0.149723         -0.001223       0.001373   \n",
       "2  NaN     1  0.556377         -0.011691       0.017971   \n",
       "3  NaN     1  0.554529         -0.010000       0.011621   \n",
       "4  NaN     1  0.205176         -0.000266       0.006193   \n",
       "5  NaN     1  0.195933          0.001207       0.004820   \n",
       "6  NaN     1  0.704251         -0.010785       0.012398   \n",
       "7  NaN     1  0.704251         -0.010785       0.012398   \n",
       "8  NaN     1  0.171904          0.017405       0.001320   \n",
       "9  NaN     1  0.177449          0.014895       0.002959   \n",
       "10 NaN     1  0.534196          0.000824       0.039284   \n",
       "11 NaN     1  0.528651          0.001851       0.017905   \n",
       "12 NaN     1  0.186691          0.006792       0.018304   \n",
       "13 NaN     1  0.188540          0.008682       0.015031   \n",
       "14 NaN     1  0.656192         -0.001833       0.021714   \n",
       "15 NaN     1  0.674677         -0.001153       0.021340   \n",
       "16 NaN     1  0.138632          0.035638       0.006736   \n",
       "17 NaN     1  0.142329          0.040349       0.006330   \n",
       "18 NaN     1  0.486137          0.028351       0.025881   \n",
       "19 NaN     1  0.519409          0.033028       0.020466   \n",
       "20 NaN     1  0.194085          0.039745       0.028531   \n",
       "21 NaN     1  0.207024          0.031562       0.040201   \n",
       "22 NaN     1  0.685767          0.020501       0.046151   \n",
       "23 NaN     1  0.659889          0.022516       0.045404   \n",
       "24 NaN     1  0.138632          0.087240       0.012155   \n",
       "25 NaN     1  0.166359          0.089000       0.010453   \n",
       "26 NaN     1  0.523105          0.068671       0.034358   \n",
       "27 NaN     1  0.510166          0.060745       0.040485   \n",
       "28 NaN     1  0.177449          0.614980       0.019093   \n",
       "29 NaN     1  0.197782          0.147659       0.043606   \n",
       "30 NaN     1  0.628466          0.036607       0.088294   \n",
       "31 NaN     1  0.656192          0.037358       0.090833   \n",
       "32 NaN     1  0.134935          0.101597       0.022770   \n",
       "33 NaN     1  0.171904          0.121145       0.048861   \n",
       "34 NaN     1  0.484288          0.081260       0.058773   \n",
       "35 NaN     1  0.449168          0.075506       0.038008   \n",
       "36 NaN     1  0.184843          0.158073       0.043921   \n",
       "37 NaN     1  0.192237          0.133380       0.034137   \n",
       "38 NaN     1  0.606285          0.042820       0.105766   \n",
       "39 NaN     1  0.621072          0.044952       0.106657   \n",
       "40 NaN     1  0.195933          0.001207       0.004820   \n",
       "41 NaN     1  0.706100         -0.010776       0.012392   \n",
       "\n",
       "   theoretical_compression_ratio  \n",
       "                            mean  \n",
       "0                       0.000000  \n",
       "1                       0.000000  \n",
       "2                       0.000000  \n",
       "3                       0.000000  \n",
       "4                       0.000000  \n",
       "5                       0.000000  \n",
       "6                       0.000000  \n",
       "7                       0.000000  \n",
       "8                       0.018738  \n",
       "9                       0.017731  \n",
       "10                      0.013212  \n",
       "11                      0.014476  \n",
       "12                      0.016143  \n",
       "13                      0.015013  \n",
       "14                      0.018278  \n",
       "15                      0.018647  \n",
       "16                      0.042507  \n",
       "17                      0.046784  \n",
       "18                      0.048398  \n",
       "19                      0.052770  \n",
       "20                      0.064240  \n",
       "21                      0.068354  \n",
       "22                      0.066113  \n",
       "23                      0.067439  \n",
       "24                      0.100254  \n",
       "25                      0.100258  \n",
       "26                      0.099375  \n",
       "27                      0.098851  \n",
       "28                      0.632944  \n",
       "29                      0.192847  \n",
       "30                      0.126927  \n",
       "31                      0.130479  \n",
       "32                      0.122494  \n",
       "33                      0.147647  \n",
       "34                      0.118345  \n",
       "35                      0.114864  \n",
       "36                      0.204460  \n",
       "37                      0.170377  \n",
       "38                      0.150301  \n",
       "39                      0.155445  \n",
       "40                      0.000000  \n",
       "41                      0.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_col = [\"num_new_tokens\", \"finetuning_params\", \"model_name\", \"trained_model_name\", \"model_name_sanitized\", \"dataset\"]\n",
    "agged = get_aggregated_results(main_results, agg_col)\n",
    "agged.reset_index(inplace=True)\n",
    "agged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>new_embeddings_init</th>\n",
       "      <th>finetuning</th>\n",
       "      <th>num_new_tokens</th>\n",
       "      <th>prompt_level_strict_acc</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>theoretical_compression_ratio</th>\n",
       "      <th>learning_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched</td>\n",
       "      <td>lora</td>\n",
       "      <td>0</td>\n",
       "      <td>0.665434</td>\n",
       "      <td>-0.011280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>learned</td>\n",
       "      <td>lora</td>\n",
       "      <td>10</td>\n",
       "      <td>0.621072</td>\n",
       "      <td>0.007102</td>\n",
       "      <td>0.017011</td>\n",
       "      <td>0.010211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>learned</td>\n",
       "      <td>lora</td>\n",
       "      <td>100</td>\n",
       "      <td>0.580407</td>\n",
       "      <td>0.035507</td>\n",
       "      <td>0.058599</td>\n",
       "      <td>0.022534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>learned</td>\n",
       "      <td>lora</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.524954</td>\n",
       "      <td>0.063164</td>\n",
       "      <td>0.134281</td>\n",
       "      <td>0.066851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.698706</td>\n",
       "      <td>-0.010853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model_name new_embeddings_init finetuning num_new_tokens  \\\n",
       "0  Llama-3.2-3B-Instruct             patched       lora              0   \n",
       "1           Llama-3.2-3B             learned       lora             10   \n",
       "2           Llama-3.2-3B             learned       lora            100   \n",
       "3           Llama-3.2-3B             learned       lora           1000   \n",
       "4  Llama-3.2-3B-Instruct                None       None       Baseline   \n",
       "\n",
       "   prompt_level_strict_acc  compression_ratio  theoretical_compression_ratio  \\\n",
       "0                 0.665434          -0.011280                       0.000000   \n",
       "1                 0.621072           0.007102                       0.017011   \n",
       "2                 0.580407           0.035507                       0.058599   \n",
       "3                 0.524954           0.063164                       0.134281   \n",
       "4                 0.698706          -0.010853                       0.000000   \n",
       "\n",
       "   learning_ratio  \n",
       "0        0.013046  \n",
       "1        0.010211  \n",
       "2        0.022534  \n",
       "3        0.066851  \n",
       "4        0.012472  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this works for the baseline_embeddings and patched experiments\n",
    "records = []\n",
    "agged.columns = ['_'.join(col).rstrip('_') if isinstance(col, tuple) else col for col in agged.columns]\n",
    "for _, row in agged.iterrows():\n",
    "    data_dict = {}\n",
    "    if not row[\"model_name_sanitized\"].startswith(\"output\"):\n",
    "        model_name = row[\"model_name_sanitized\"].split(\"__\")[-1]\n",
    "        new_embeddings_init = \"None\"\n",
    "        finetuning = \"None\"\n",
    "        row[\"num_new_tokens\"] = \"Baseline\"\n",
    "    elif row[\"model_name\"] == \"patching\":\n",
    "        model_name = \"Llama-3.2-3B-Instruct\"\n",
    "        new_embeddings_init = \"patched\"\n",
    "        finetuning = \"None\"\n",
    "    elif row[\"model_name\"] == \"Llama-3.2-3B\" and row[\"finetuning_params\"] == \"embeddings\":\n",
    "        model_name = \"Llama-3.2-3B\"\n",
    "        new_embeddings_init = \"None\"\n",
    "        finetuning = \"embeddings\"\n",
    "    elif row[\"model_name\"].startswith(\"Llama-3.2-3B-Instruct\") and row[\"num_new_tokens\"] != \"NAN\":\n",
    "        model_name = \"Llama-3.2-3B-Instruct\"\n",
    "        new_embeddings_init = \"patched\"\n",
    "        finetuning = \"lora\"\n",
    "    else:  # loraed\n",
    "        model_name = \"Llama-3.2-3B\"\n",
    "        new_embeddings_init = \"learned\"\n",
    "        finetuning = \"lora\"\n",
    "\n",
    "    data_dict[\"model_name\"] = model_name\n",
    "    data_dict[\"new_embeddings_init\"] = new_embeddings_init\n",
    "    data_dict[\"finetuning\"] = finetuning\n",
    "    try:\n",
    "        data_dict[\"num_new_tokens\"] = int(row[\"num_new_tokens\"])\n",
    "    except:\n",
    "        data_dict[\"num_new_tokens\"] = row[\"num_new_tokens\"]\n",
    "    data_dict[\"prompt_level_strict_acc\"] = row[\"prompt_level_strict_acc_mean\"]\n",
    "    data_dict[\"compression_ratio\"] = row[\"compression_ratio_mean\"]\n",
    "    data_dict[\"theoretical_compression_ratio\"] = row[\"theoretical_compression_ratio_mean\"]\n",
    "    data_dict[\"learning_ratio\"] = row[\"learning_ratio_mean\"]\n",
    "    records.append(data_dict)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Compression</th>\n",
       "      <th>Shortfall</th>\n",
       "      <th>Theoretical</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_new_tokens</th>\n",
       "      <th>model_name</th>\n",
       "      <th>new_embeddings_init</th>\n",
       "      <th>finetuning</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>Llama-3.2-3B-Instruct</th>\n",
       "      <th>patched</th>\n",
       "      <th>lora</th>\n",
       "      <td>66.5%</td>\n",
       "      <td>-1.1%</td>\n",
       "      <td>1.3%</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>Llama-3.2-3B</th>\n",
       "      <th>learned</th>\n",
       "      <th>lora</th>\n",
       "      <td>62.1%</td>\n",
       "      <td>0.7%</td>\n",
       "      <td>1.0%</td>\n",
       "      <td>1.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <th>Llama-3.2-3B</th>\n",
       "      <th>learned</th>\n",
       "      <th>lora</th>\n",
       "      <td>58.0%</td>\n",
       "      <td>3.6%</td>\n",
       "      <td>2.3%</td>\n",
       "      <td>5.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <th>Llama-3.2-3B</th>\n",
       "      <th>learned</th>\n",
       "      <th>lora</th>\n",
       "      <td>52.5%</td>\n",
       "      <td>6.3%</td>\n",
       "      <td>6.7%</td>\n",
       "      <td>13.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <th>Llama-3.2-3B-Instruct</th>\n",
       "      <th>None</th>\n",
       "      <th>None</th>\n",
       "      <td>69.9%</td>\n",
       "      <td>-1.1%</td>\n",
       "      <td>1.2%</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    Accuracy  \\\n",
       "num_new_tokens model_name            new_embeddings_init finetuning            \n",
       "0              Llama-3.2-3B-Instruct patched             lora          66.5%   \n",
       "10             Llama-3.2-3B          learned             lora          62.1%   \n",
       "100            Llama-3.2-3B          learned             lora          58.0%   \n",
       "1000           Llama-3.2-3B          learned             lora          52.5%   \n",
       "Baseline       Llama-3.2-3B-Instruct None                None          69.9%   \n",
       "\n",
       "                                                                    Compression  \\\n",
       "num_new_tokens model_name            new_embeddings_init finetuning               \n",
       "0              Llama-3.2-3B-Instruct patched             lora             -1.1%   \n",
       "10             Llama-3.2-3B          learned             lora              0.7%   \n",
       "100            Llama-3.2-3B          learned             lora              3.6%   \n",
       "1000           Llama-3.2-3B          learned             lora              6.3%   \n",
       "Baseline       Llama-3.2-3B-Instruct None                None             -1.1%   \n",
       "\n",
       "                                                                    Shortfall  \\\n",
       "num_new_tokens model_name            new_embeddings_init finetuning             \n",
       "0              Llama-3.2-3B-Instruct patched             lora            1.3%   \n",
       "10             Llama-3.2-3B          learned             lora            1.0%   \n",
       "100            Llama-3.2-3B          learned             lora            2.3%   \n",
       "1000           Llama-3.2-3B          learned             lora            6.7%   \n",
       "Baseline       Llama-3.2-3B-Instruct None                None            1.2%   \n",
       "\n",
       "                                                                    Theoretical  \n",
       "num_new_tokens model_name            new_embeddings_init finetuning              \n",
       "0              Llama-3.2-3B-Instruct patched             lora              0.0%  \n",
       "10             Llama-3.2-3B          learned             lora              1.7%  \n",
       "100            Llama-3.2-3B          learned             lora              5.9%  \n",
       "1000           Llama-3.2-3B          learned             lora             13.4%  \n",
       "Baseline       Llama-3.2-3B-Instruct None                None              0.0%  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      " &  &  &  & Accuracy & Compression & Shortfall & Theoretical \\\\\n",
      "Tokens & Model & Emb Init & Finetuning &  &  &  &  \\\\\n",
      "\\midrule\n",
      "0 & Llama-3.2-3B-Instruct & patched & lora & 66.5\\% & -1.1\\% & 1.3\\% & 0.0\\% \\\\\n",
      "\\cline{1-8} \\cline{2-8} \\cline{3-8}\n",
      "10 & Llama-3.2-3B & learned & lora & 62.1\\% & 0.7\\% & 1.0\\% & 1.7\\% \\\\\n",
      "\\cline{1-8} \\cline{2-8} \\cline{3-8}\n",
      "100 & Llama-3.2-3B & learned & lora & 58.0\\% & 3.6\\% & 2.3\\% & 5.9\\% \\\\\n",
      "\\cline{1-8} \\cline{2-8} \\cline{3-8}\n",
      "1000 & Llama-3.2-3B & learned & lora & 52.5\\% & 6.3\\% & 6.7\\% & 13.4\\% \\\\\n",
      "\\cline{1-8} \\cline{2-8} \\cline{3-8}\n",
      "Baseline & Llama-3.2-3B-Instruct & None & None & 69.9\\% & -1.1\\% & 1.2\\% & 0.0\\% \\\\\n",
      "\\cline{1-8} \\cline{2-8} \\cline{3-8}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_col = [\"num_new_tokens\", \"model_name\", \"new_embeddings_init\", \"finetuning\"]\n",
    "df_agged = get_aggregated_results(df, agg_col)\n",
    "df_agged.columns = ['_'.join(col).rstrip('_') if isinstance(col, tuple) else col for col in df_agged.columns]\n",
    "mean_cols = [col for col in df_agged.columns if \"mean\" in col]\n",
    "df_agged = df_agged[mean_cols]\n",
    "cols = [\"Accuracy\", \"Compression\", \"Shortfall\", \"Theoretical\"]\n",
    "df_agged.columns = cols\n",
    "for col in cols:\n",
    "    df_agged[col] = (df_agged[col] * 100).round(1).astype(str) + \"%\"\n",
    "display(df_agged)\n",
    "df_agged.index.names = [\"Tokens\", \"Model\", \"Emb Init\", \"Finetuning\"]\n",
    "\n",
    "# df_agged.rename(columns={\"\": \"New Tokens\", \"model_name_\": \"Model\", \"new_embeddings_init_\": \"Embeddings Init\", \"finetuning_\": \"Finetuning\"}, inplace=True)\n",
    "latex_table = df_agged.to_latex(index=True, escape=True)\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>model_name</th>\n",
       "      <th>new_embeddings_init</th>\n",
       "      <th>finetuning</th>\n",
       "      <th>num_new_tokens</th>\n",
       "      <th>prompt_level_strict_acc</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>theoretical_compression_ratio</th>\n",
       "      <th>learning_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>learned</td>\n",
       "      <td>lora</td>\n",
       "      <td>10</td>\n",
       "      <td>0.621072</td>\n",
       "      <td>0.007102</td>\n",
       "      <td>0.017011</td>\n",
       "      <td>0.010211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>learned</td>\n",
       "      <td>lora</td>\n",
       "      <td>100</td>\n",
       "      <td>0.580407</td>\n",
       "      <td>0.035507</td>\n",
       "      <td>0.058599</td>\n",
       "      <td>0.022534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>learned</td>\n",
       "      <td>lora</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.524954</td>\n",
       "      <td>0.063164</td>\n",
       "      <td>0.134281</td>\n",
       "      <td>0.066851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    model_name new_embeddings_init finetuning num_new_tokens  \\\n",
       "1      1  Llama-3.2-3B             learned       lora             10   \n",
       "2      2  Llama-3.2-3B             learned       lora            100   \n",
       "3      3  Llama-3.2-3B             learned       lora           1000   \n",
       "\n",
       "   prompt_level_strict_acc  compression_ratio  theoretical_compression_ratio  \\\n",
       "1                 0.621072           0.007102                       0.017011   \n",
       "2                 0.580407           0.035507                       0.058599   \n",
       "3                 0.524954           0.063164                       0.134281   \n",
       "\n",
       "   learning_ratio  \n",
       "1        0.010211  \n",
       "2        0.022534  \n",
       "3        0.066851  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_flat = df.reset_index()\n",
    "base = df_flat[df_flat[\"model_name\"] == \"Llama-3.2-3B\"]\n",
    "display(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: Scatter plot of Accuracy vs num_new_tokens\n",
    "subset = df_flat[df_flat[\"model_name\"] == \"Llama-3.2-3B\"]  # filter one model for clarity\n",
    "base_line = subset[subset[\"num_new_tokens\"] == \"Baseline\"]\n",
    "subset = subset[subset[\"num_new_tokens\"] != \"Baseline\"]\n",
    "\n",
    "group_col = [\"new_embeddings_init\"]\n",
    "metric = \"prompt_level_strict_acc\"\n",
    "\n",
    "my_compression_plotter(subset, group_col, metric, base_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_flat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example: Scatter plot of Accuracy vs num_new_tokens\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m subset \u001b[38;5;241m=\u001b[39m \u001b[43mdf_flat\u001b[49m[df_flat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama-3.2-3B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# filter one model for clarity\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(subset)\n\u001b[1;32m      4\u001b[0m base_line \u001b[38;5;241m=\u001b[39m subset[subset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_flat' is not defined"
     ]
    }
   ],
   "source": [
    "# Example: Scatter plot of Accuracy vs num_new_tokens\n",
    "subset = df_flat[df_flat[\"model_name\"] == \"Llama-3.2-3B-Instruct\"]  # filter one model for clarity\n",
    "print(subset)\n",
    "base_line = subset[subset[\"num_new_tokens\"] == \"Baseline\"]\n",
    "subset = subset[subset[\"num_new_tokens\"] != \"Baseline\"]\n",
    "\n",
    "group_col = [\"finetuning\"]\n",
    "metric = \"prompt_level_strict_acc\"\n",
    "\n",
    "my_compression_plotter(subset, group_col, metric, base_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Instruct models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: model_name_sanitized                              meta-llama__Llama-3.2-3B-Instruct\n",
      "model_name                                                               meta-llama\n",
      "task_name                                                                  baseline\n",
      "exact_match                                                                     NAN\n",
      "exact_match_stderr                                                              NAN\n",
      "prompt_level_strict_acc                                                    0.700555\n",
      "prompt_level_strict_acc_stderr                                              0.01971\n",
      "inst_level_strict_acc                                                      0.780576\n",
      "inst_level_strict_acc_stderr                                                    NAN\n",
      "prompt_level_loose_acc                                                     0.752311\n",
      "prompt_level_loose_acc_stderr                                              0.018576\n",
      "inst_level_loose_acc                                                       0.820144\n",
      "inst_level_loose_acc_stderr                                                     NAN\n",
      "compression_ratio                                                         -0.010601\n",
      "learning_ratio                                                             0.012159\n",
      "theoretical_compression_ratio                                                   0.0\n",
      "alias                                                                        ifeval\n",
      "limit                                                                           NAN\n",
      "json_path                         eval_results/templating/meta-llama__Llama-3.2-...\n",
      "finetuning_params                                                               NAN\n",
      "total_batch_size                                                                NAN\n",
      "learning_rate                                                                   NAN\n",
      "main_loss_type                                                                  NAN\n",
      "embedding_init_strategy                                                         NAN\n",
      "num_new_tokens                                                                  NAN\n",
      "unfreeze_params_steps                                                           NAN\n",
      "finetune_params_after_unfreeze                                                  NAN\n",
      "dataset                                                                         NAN\n",
      "tokenizer_path                                                                  NAN\n",
      "seed                                                                            NAN\n",
      "reset_optimizer                                                                 NAN\n",
      "warmup_steps                                                                    NAN\n",
      "lr_schedule                                                                     NAN\n",
      "target_modules                                                                  NAN\n",
      "r                                                                               NAN\n",
      "lora_alpha                                                                      NAN\n",
      "lora_dropout                                                                    NAN\n",
      "Name: 7, dtype: object\n",
      "Error: model_name_sanitized                              meta-llama__Llama-3.2-3B-Instruct\n",
      "model_name                                                               meta-llama\n",
      "task_name                                                                  baseline\n",
      "exact_match                                                                     NAN\n",
      "exact_match_stderr                                                              NAN\n",
      "prompt_level_strict_acc                                                    0.695009\n",
      "prompt_level_strict_acc_stderr                                             0.019813\n",
      "inst_level_strict_acc                                                      0.779376\n",
      "inst_level_strict_acc_stderr                                                    NAN\n",
      "prompt_level_loose_acc                                                     0.748614\n",
      "prompt_level_loose_acc_stderr                                              0.018668\n",
      "inst_level_loose_acc                                                       0.818945\n",
      "inst_level_loose_acc_stderr                                                     NAN\n",
      "compression_ratio                                                         -0.010754\n",
      "learning_ratio                                                             0.012367\n",
      "theoretical_compression_ratio                                                   0.0\n",
      "alias                                                                        ifeval\n",
      "limit                                                                           NAN\n",
      "json_path                         eval_results/patching/meta-llama__Llama-3.2-3B...\n",
      "Name: 0, dtype: object\n",
      "Error: model_name_sanitized                                       meta-llama__Llama-3.2-3B\n",
      "model_name                                                               meta-llama\n",
      "task_name                                                                  baseline\n",
      "exact_match                                                                     NAN\n",
      "exact_match_stderr                                                              NAN\n",
      "prompt_level_strict_acc                                                    0.207024\n",
      "prompt_level_strict_acc_stderr                                             0.017436\n",
      "inst_level_strict_acc                                                      0.333333\n",
      "inst_level_strict_acc_stderr                                                    NAN\n",
      "prompt_level_loose_acc                                                      0.22366\n",
      "prompt_level_loose_acc_stderr                                              0.017932\n",
      "inst_level_loose_acc                                                       0.357314\n",
      "inst_level_loose_acc_stderr                                                     NAN\n",
      "compression_ratio                                                          0.001626\n",
      "learning_ratio                                                             0.002928\n",
      "theoretical_compression_ratio                                                   0.0\n",
      "alias                                                                        ifeval\n",
      "limit                                                                           NAN\n",
      "json_path                         eval_results/baseline_embeddings/meta-llama__L...\n",
      "finetuning_params                                                               NAN\n",
      "total_batch_size                                                                NAN\n",
      "learning_rate                                                                   NAN\n",
      "main_loss_type                                                                  NAN\n",
      "embedding_init_strategy                                                         NAN\n",
      "num_new_tokens                                                                  NAN\n",
      "unfreeze_params_steps                                                           NAN\n",
      "finetune_params_prefreeze                                                       NAN\n",
      "dataset                                                                         NAN\n",
      "tokenizer_path                                                                  NAN\n",
      "seed                                                                            NAN\n",
      "reset_optimizer                                                                 NAN\n",
      "warmup_steps                                                                    NAN\n",
      "lr_schedule                                                                     NAN\n",
      "warmup_steps_prefreeze                                                          NAN\n",
      "lr_schedule_prefreeze                                                           NAN\n",
      "target_modules                                                                  NAN\n",
      "r                                                                               NAN\n",
      "lora_alpha                                                                      NAN\n",
      "lora_dropout                                                                    NAN\n",
      "Name: 2, dtype: object\n",
      "Error: model_name_sanitized                              meta-llama__Llama-3.2-3B-Instruct\n",
      "model_name                                                               meta-llama\n",
      "task_name                                                                  baseline\n",
      "exact_match                                                                     NAN\n",
      "exact_match_stderr                                                              NAN\n",
      "prompt_level_strict_acc                                                    0.695009\n",
      "prompt_level_strict_acc_stderr                                             0.019813\n",
      "inst_level_strict_acc                                                      0.779376\n",
      "inst_level_strict_acc_stderr                                                    NAN\n",
      "prompt_level_loose_acc                                                     0.748614\n",
      "prompt_level_loose_acc_stderr                                              0.018668\n",
      "inst_level_loose_acc                                                       0.818945\n",
      "inst_level_loose_acc_stderr                                                     NAN\n",
      "compression_ratio                                                         -0.010754\n",
      "learning_ratio                                                             0.012367\n",
      "theoretical_compression_ratio                                                   0.0\n",
      "alias                                                                        ifeval\n",
      "limit                                                                           NAN\n",
      "json_path                         eval_results/baseline_embeddings/meta-llama__L...\n",
      "finetuning_params                                                               NAN\n",
      "total_batch_size                                                                NAN\n",
      "learning_rate                                                                   NAN\n",
      "main_loss_type                                                                  NAN\n",
      "embedding_init_strategy                                                         NAN\n",
      "num_new_tokens                                                                  NAN\n",
      "unfreeze_params_steps                                                           NAN\n",
      "finetune_params_prefreeze                                                       NAN\n",
      "dataset                                                                         NAN\n",
      "tokenizer_path                                                                  NAN\n",
      "seed                                                                            NAN\n",
      "reset_optimizer                                                                 NAN\n",
      "warmup_steps                                                                    NAN\n",
      "lr_schedule                                                                     NAN\n",
      "warmup_steps_prefreeze                                                          NAN\n",
      "lr_schedule_prefreeze                                                           NAN\n",
      "target_modules                                                                  NAN\n",
      "r                                                                               NAN\n",
      "lora_alpha                                                                      NAN\n",
      "lora_dropout                                                                    NAN\n",
      "Name: 9, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cmlscratch/astein0/tmp/ipykernel_2514341/2063324347.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'NAN' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  main_results.fillna(\"NAN\", inplace=True)\n",
      "/cmlscratch/astein0/tmp/ipykernel_2514341/2063324347.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  main_results[\"task_name\"].replace(\"ifeval\", \"baseline\", inplace=True)\n",
      "/cmlscratch/astein0/tmp/ipykernel_2514341/2063324347.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'NAN' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  main_results.fillna(\"NAN\", inplace=True)\n",
      "/cmlscratch/astein0/tmp/ipykernel_2514341/2063324347.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  main_results[\"task_name\"].replace(\"ifeval\", \"baseline\", inplace=True)\n",
      "/cmlscratch/astein0/tmp/ipykernel_2514341/2063324347.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'NAN' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  main_results.fillna(\"NAN\", inplace=True)\n",
      "/cmlscratch/astein0/tmp/ipykernel_2514341/2063324347.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  main_results[\"task_name\"].replace(\"ifeval\", \"baseline\", inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name_sanitized</th>\n",
       "      <th>model_name</th>\n",
       "      <th>task_name</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>exact_match_stderr</th>\n",
       "      <th>prompt_level_strict_acc</th>\n",
       "      <th>prompt_level_strict_acc_stderr</th>\n",
       "      <th>inst_level_strict_acc</th>\n",
       "      <th>inst_level_strict_acc_stderr</th>\n",
       "      <th>prompt_level_loose_acc</th>\n",
       "      <th>prompt_level_loose_acc_stderr</th>\n",
       "      <th>inst_level_loose_acc</th>\n",
       "      <th>inst_level_loose_acc_stderr</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>learning_ratio</th>\n",
       "      <th>theoretical_compression_ratio</th>\n",
       "      <th>alias</th>\n",
       "      <th>limit</th>\n",
       "      <th>json_path</th>\n",
       "      <th>finetuning_params</th>\n",
       "      <th>total_batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>main_loss_type</th>\n",
       "      <th>embedding_init_strategy</th>\n",
       "      <th>num_new_tokens</th>\n",
       "      <th>unfreeze_params_steps</th>\n",
       "      <th>finetune_params_after_unfreeze</th>\n",
       "      <th>dataset</th>\n",
       "      <th>tokenizer_path</th>\n",
       "      <th>seed</th>\n",
       "      <th>reset_optimizer</th>\n",
       "      <th>warmup_steps</th>\n",
       "      <th>lr_schedule</th>\n",
       "      <th>target_modules</th>\n",
       "      <th>r</th>\n",
       "      <th>lora_alpha</th>\n",
       "      <th>lora_dropout</th>\n",
       "      <th>finetune_params_prefreeze</th>\n",
       "      <th>warmup_steps_prefreeze</th>\n",
       "      <th>lr_schedule_prefreeze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>output__templating__043305d0-Llama-3.2-3B-Inst...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>SFT</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.670980</td>\n",
       "      <td>0.020219</td>\n",
       "      <td>0.760192</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.730129</td>\n",
       "      <td>0.019102</td>\n",
       "      <td>0.809353</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010802</td>\n",
       "      <td>0.012554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/templating/output__templating__04...</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>all</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>lora</td>\n",
       "      <td>magpie-default-tokenized_0,magpie-translation-...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>True</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>linear</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>output__templating__b0948c7c-Llama-3.2-3B-Inst...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.597043</td>\n",
       "      <td>0.021107</td>\n",
       "      <td>0.696643</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.683919</td>\n",
       "      <td>0.020008</td>\n",
       "      <td>0.774580</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.035448</td>\n",
       "      <td>0.024583</td>\n",
       "      <td>0.060531</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/templating/output__templating__b0...</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>lora</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>True</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>linear</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>output__templating__f2270e48-Llama-3.2-3B-Inst...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.628466</td>\n",
       "      <td>0.020794</td>\n",
       "      <td>0.723022</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.689464</td>\n",
       "      <td>0.019912</td>\n",
       "      <td>0.779376</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.014769</td>\n",
       "      <td>0.014429</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/templating/output__templating__f2...</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>first_last</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>True</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>output__templating__eb5e0411-Llama-3.2-3B-Inst...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.645102</td>\n",
       "      <td>0.020591</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.720887</td>\n",
       "      <td>0.019303</td>\n",
       "      <td>0.802158</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0.024059</td>\n",
       "      <td>0.017551</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/templating/output__templating__eb...</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>lora</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>True</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>linear</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>output__templating__ce7afe78-Llama-3.2-3B-Inst...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.613678</td>\n",
       "      <td>0.020953</td>\n",
       "      <td>0.717026</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.687616</td>\n",
       "      <td>0.019944</td>\n",
       "      <td>0.780576</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.020545</td>\n",
       "      <td>0.034478</td>\n",
       "      <td>0.055265</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/templating/output__templating__ce...</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>first_last</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>True</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>output__templating__747113b2-Llama-3.2-3B-Inst...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.543438</td>\n",
       "      <td>0.021435</td>\n",
       "      <td>0.655875</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.617375</td>\n",
       "      <td>0.020915</td>\n",
       "      <td>0.726619</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.051198</td>\n",
       "      <td>0.076424</td>\n",
       "      <td>0.131187</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/templating/output__templating__74...</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>lora</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>True</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>linear</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>output__templating__381d099f-Llama-3.2-3B-Inst...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.608133</td>\n",
       "      <td>0.021007</td>\n",
       "      <td>0.718225</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.670980</td>\n",
       "      <td>0.020219</td>\n",
       "      <td>0.768585</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.023592</td>\n",
       "      <td>0.095626</td>\n",
       "      <td>0.120402</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/templating/output__templating__38...</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>first_last</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>True</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>meta-llama</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.700555</td>\n",
       "      <td>0.019710</td>\n",
       "      <td>0.780576</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.752311</td>\n",
       "      <td>0.018576</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010601</td>\n",
       "      <td>0.012159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/templating/meta-llama__Llama-3.2-...</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>output__templating__5226688f-Llama-3.2-3B-Inst...</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>SFT</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.659889</td>\n",
       "      <td>0.020387</td>\n",
       "      <td>0.749400</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.719039</td>\n",
       "      <td>0.019342</td>\n",
       "      <td>0.796163</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.011285</td>\n",
       "      <td>0.013064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/templating/output__templating__52...</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>all</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>first_last</td>\n",
       "      <td>magpie-default-tokenized_0,magpie-translation-...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>True</td>\n",
       "      <td>100.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>meta-llama</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.695009</td>\n",
       "      <td>0.019813</td>\n",
       "      <td>0.779376</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.748614</td>\n",
       "      <td>0.018668</td>\n",
       "      <td>0.818945</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.010754</td>\n",
       "      <td>0.012367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/patching/meta-llama__Llama-3.2-3B...</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>output__patching__Llama-3.2-3B-Instruct_plus_e...</td>\n",
       "      <td>patching</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.615527</td>\n",
       "      <td>0.020934</td>\n",
       "      <td>0.719424</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>0.020440</td>\n",
       "      <td>0.751799</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.058259</td>\n",
       "      <td>0.106773</td>\n",
       "      <td>0.169940</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/patching/output__patching__Llama-...</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>output__patching__Llama-3.2-3B-Instruct_plus_b...</td>\n",
       "      <td>patching</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.659889</td>\n",
       "      <td>0.020387</td>\n",
       "      <td>0.750600</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.711645</td>\n",
       "      <td>0.019494</td>\n",
       "      <td>0.790168</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.006034</td>\n",
       "      <td>0.058326</td>\n",
       "      <td>0.063345</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/patching/output__patching__Llama-...</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>output__patching__Llama-3.2-3B-Instruct_plus_7...</td>\n",
       "      <td>patching</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.680222</td>\n",
       "      <td>0.020070</td>\n",
       "      <td>0.768585</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.735675</td>\n",
       "      <td>0.018976</td>\n",
       "      <td>0.811751</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-0.006470</td>\n",
       "      <td>0.025475</td>\n",
       "      <td>0.017298</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/patching/output__patching__Llama-...</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>output__baseline_embeddings__fd51a57e-Llama-3....</td>\n",
       "      <td>Llama-3.2-3B-Instruct_plus_e08a94c1-Llama-3.2-...</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.404806</td>\n",
       "      <td>0.021123</td>\n",
       "      <td>0.532374</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.478743</td>\n",
       "      <td>0.021497</td>\n",
       "      <td>0.611511</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.110800</td>\n",
       "      <td>0.032170</td>\n",
       "      <td>0.136866</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/baseline_embeddings/output__basel...</td>\n",
       "      <td>lora</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_1000,magpie-translati...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>250.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>linear</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>output__baseline_embeddings__c329e22b-Llama-3....</td>\n",
       "      <td>Llama-3.2-3B-Instruct_plus_75ec96b0-Llama-3.2-...</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.534196</td>\n",
       "      <td>0.021466</td>\n",
       "      <td>0.645084</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.589649</td>\n",
       "      <td>0.021168</td>\n",
       "      <td>0.699041</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.037306</td>\n",
       "      <td>0.014978</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/baseline_embeddings/output__basel...</td>\n",
       "      <td>lora</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_10,magpie-translation...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>250.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>linear</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>output__baseline_embeddings__607f97cd-Llama-3....</td>\n",
       "      <td>Llama-3.2-3B-Instruct_plus_b9ece787-Llama-3.2-...</td>\n",
       "      <td>mixed</td>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.467652</td>\n",
       "      <td>0.021471</td>\n",
       "      <td>0.592326</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.528651</td>\n",
       "      <td>0.021481</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.037431</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.051094</td>\n",
       "      <td>ifeval</td>\n",
       "      <td>NAN</td>\n",
       "      <td>eval_results/baseline_embeddings/output__basel...</td>\n",
       "      <td>lora</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>mixed</td>\n",
       "      <td>merge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>magpie-default-tokenized_100,magpie-translatio...</td>\n",
       "      <td>/cmlscratch/astein0/efficient_tokenization_for...</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>False</td>\n",
       "      <td>250.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>linear</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NAN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model_name_sanitized  \\\n",
       "0   output__templating__043305d0-Llama-3.2-3B-Inst...   \n",
       "1   output__templating__b0948c7c-Llama-3.2-3B-Inst...   \n",
       "2   output__templating__f2270e48-Llama-3.2-3B-Inst...   \n",
       "3   output__templating__eb5e0411-Llama-3.2-3B-Inst...   \n",
       "4   output__templating__ce7afe78-Llama-3.2-3B-Inst...   \n",
       "5   output__templating__747113b2-Llama-3.2-3B-Inst...   \n",
       "6   output__templating__381d099f-Llama-3.2-3B-Inst...   \n",
       "7                   meta-llama__Llama-3.2-3B-Instruct   \n",
       "8   output__templating__5226688f-Llama-3.2-3B-Inst...   \n",
       "0                   meta-llama__Llama-3.2-3B-Instruct   \n",
       "1   output__patching__Llama-3.2-3B-Instruct_plus_e...   \n",
       "2   output__patching__Llama-3.2-3B-Instruct_plus_b...   \n",
       "3   output__patching__Llama-3.2-3B-Instruct_plus_7...   \n",
       "14  output__baseline_embeddings__fd51a57e-Llama-3....   \n",
       "16  output__baseline_embeddings__c329e22b-Llama-3....   \n",
       "18  output__baseline_embeddings__607f97cd-Llama-3....   \n",
       "\n",
       "                                           model_name task_name exact_match  \\\n",
       "0                               Llama-3.2-3B-Instruct       SFT         NAN   \n",
       "1                               Llama-3.2-3B-Instruct     mixed         NAN   \n",
       "2                               Llama-3.2-3B-Instruct     mixed         NAN   \n",
       "3                               Llama-3.2-3B-Instruct     mixed         NAN   \n",
       "4                               Llama-3.2-3B-Instruct     mixed         NAN   \n",
       "5                               Llama-3.2-3B-Instruct     mixed         NAN   \n",
       "6                               Llama-3.2-3B-Instruct     mixed         NAN   \n",
       "7                                          meta-llama  baseline         NAN   \n",
       "8                               Llama-3.2-3B-Instruct       SFT         NAN   \n",
       "0                                          meta-llama  baseline         NAN   \n",
       "1                                            patching  baseline         NAN   \n",
       "2                                            patching  baseline         NAN   \n",
       "3                                            patching  baseline         NAN   \n",
       "14  Llama-3.2-3B-Instruct_plus_e08a94c1-Llama-3.2-...     mixed         NAN   \n",
       "16  Llama-3.2-3B-Instruct_plus_75ec96b0-Llama-3.2-...     mixed         NAN   \n",
       "18  Llama-3.2-3B-Instruct_plus_b9ece787-Llama-3.2-...     mixed         NAN   \n",
       "\n",
       "   exact_match_stderr  prompt_level_strict_acc  \\\n",
       "0                 NAN                 0.670980   \n",
       "1                 NAN                 0.597043   \n",
       "2                 NAN                 0.628466   \n",
       "3                 NAN                 0.645102   \n",
       "4                 NAN                 0.613678   \n",
       "5                 NAN                 0.543438   \n",
       "6                 NAN                 0.608133   \n",
       "7                 NAN                 0.700555   \n",
       "8                 NAN                 0.659889   \n",
       "0                 NAN                 0.695009   \n",
       "1                 NAN                 0.615527   \n",
       "2                 NAN                 0.659889   \n",
       "3                 NAN                 0.680222   \n",
       "14                NAN                 0.404806   \n",
       "16                NAN                 0.534196   \n",
       "18                NAN                 0.467652   \n",
       "\n",
       "    prompt_level_strict_acc_stderr  inst_level_strict_acc  \\\n",
       "0                         0.020219               0.760192   \n",
       "1                         0.021107               0.696643   \n",
       "2                         0.020794               0.723022   \n",
       "3                         0.020591               0.733813   \n",
       "4                         0.020953               0.717026   \n",
       "5                         0.021435               0.655875   \n",
       "6                         0.021007               0.718225   \n",
       "7                         0.019710               0.780576   \n",
       "8                         0.020387               0.749400   \n",
       "0                         0.019813               0.779376   \n",
       "1                         0.020934               0.719424   \n",
       "2                         0.020387               0.750600   \n",
       "3                         0.020070               0.768585   \n",
       "14                        0.021123               0.532374   \n",
       "16                        0.021466               0.645084   \n",
       "18                        0.021471               0.592326   \n",
       "\n",
       "   inst_level_strict_acc_stderr  prompt_level_loose_acc  \\\n",
       "0                           NAN                0.730129   \n",
       "1                           NAN                0.683919   \n",
       "2                           NAN                0.689464   \n",
       "3                           NAN                0.720887   \n",
       "4                           NAN                0.687616   \n",
       "5                           NAN                0.617375   \n",
       "6                           NAN                0.670980   \n",
       "7                           NAN                0.752311   \n",
       "8                           NAN                0.719039   \n",
       "0                           NAN                0.748614   \n",
       "1                           NAN                0.656192   \n",
       "2                           NAN                0.711645   \n",
       "3                           NAN                0.735675   \n",
       "14                          NAN                0.478743   \n",
       "16                          NAN                0.589649   \n",
       "18                          NAN                0.528651   \n",
       "\n",
       "    prompt_level_loose_acc_stderr  inst_level_loose_acc  \\\n",
       "0                        0.019102              0.809353   \n",
       "1                        0.020008              0.774580   \n",
       "2                        0.019912              0.779376   \n",
       "3                        0.019303              0.802158   \n",
       "4                        0.019944              0.780576   \n",
       "5                        0.020915              0.726619   \n",
       "6                        0.020219              0.768585   \n",
       "7                        0.018576              0.820144   \n",
       "8                        0.019342              0.796163   \n",
       "0                        0.018668              0.818945   \n",
       "1                        0.020440              0.751799   \n",
       "2                        0.019494              0.790168   \n",
       "3                        0.018976              0.811751   \n",
       "14                       0.021497              0.611511   \n",
       "16                       0.021168              0.699041   \n",
       "18                       0.021481              0.647482   \n",
       "\n",
       "   inst_level_loose_acc_stderr  compression_ratio  learning_ratio  \\\n",
       "0                          NAN          -0.010802        0.012554   \n",
       "1                          NAN           0.035448        0.024583   \n",
       "2                          NAN          -0.000015        0.014769   \n",
       "3                          NAN           0.005254        0.024059   \n",
       "4                          NAN           0.020545        0.034478   \n",
       "5                          NAN           0.051198        0.076424   \n",
       "6                          NAN           0.023592        0.095626   \n",
       "7                          NAN          -0.010601        0.012159   \n",
       "8                          NAN          -0.011285        0.013064   \n",
       "0                          NAN          -0.010754        0.012367   \n",
       "1                          NAN           0.058259        0.106773   \n",
       "2                          NAN           0.006034        0.058326   \n",
       "3                          NAN          -0.006470        0.025475   \n",
       "14                         NAN           0.110800        0.032170   \n",
       "16                         NAN           0.001799        0.037306   \n",
       "18                         NAN           0.037431        0.025288   \n",
       "\n",
       "    theoretical_compression_ratio   alias limit  \\\n",
       "0                        0.000000  ifeval   NAN   \n",
       "1                        0.060531  ifeval   NAN   \n",
       "2                        0.014429  ifeval   NAN   \n",
       "3                        0.017551  ifeval   NAN   \n",
       "4                        0.055265  ifeval   NAN   \n",
       "5                        0.131187  ifeval   NAN   \n",
       "6                        0.120402  ifeval   NAN   \n",
       "7                        0.000000  ifeval   NAN   \n",
       "8                        0.000000  ifeval   NAN   \n",
       "0                        0.000000  ifeval   NAN   \n",
       "1                        0.169940  ifeval   NAN   \n",
       "2                        0.063345  ifeval   NAN   \n",
       "3                        0.017298  ifeval   NAN   \n",
       "14                       0.136866  ifeval   NAN   \n",
       "16                       0.014978  ifeval   NAN   \n",
       "18                       0.051094  ifeval   NAN   \n",
       "\n",
       "                                            json_path finetuning_params  \\\n",
       "0   eval_results/templating/output__templating__04...        embeddings   \n",
       "1   eval_results/templating/output__templating__b0...        embeddings   \n",
       "2   eval_results/templating/output__templating__f2...        embeddings   \n",
       "3   eval_results/templating/output__templating__eb...        embeddings   \n",
       "4   eval_results/templating/output__templating__ce...        embeddings   \n",
       "5   eval_results/templating/output__templating__74...        embeddings   \n",
       "6   eval_results/templating/output__templating__38...        embeddings   \n",
       "7   eval_results/templating/meta-llama__Llama-3.2-...               NAN   \n",
       "8   eval_results/templating/output__templating__52...        embeddings   \n",
       "0   eval_results/patching/meta-llama__Llama-3.2-3B...               NAN   \n",
       "1   eval_results/patching/output__patching__Llama-...               NAN   \n",
       "2   eval_results/patching/output__patching__Llama-...               NAN   \n",
       "3   eval_results/patching/output__patching__Llama-...               NAN   \n",
       "14  eval_results/baseline_embeddings/output__basel...              lora   \n",
       "16  eval_results/baseline_embeddings/output__basel...              lora   \n",
       "18  eval_results/baseline_embeddings/output__basel...              lora   \n",
       "\n",
       "   total_batch_size learning_rate main_loss_type embedding_init_strategy  \\\n",
       "0              32.0       0.00002            all                     NAN   \n",
       "1              32.0       0.00002          mixed                   merge   \n",
       "2              32.0       0.00002          mixed                   merge   \n",
       "3              32.0       0.00002          mixed                   merge   \n",
       "4              32.0       0.00002          mixed                   merge   \n",
       "5              32.0       0.00002          mixed                   merge   \n",
       "6              32.0       0.00002          mixed                   merge   \n",
       "7               NAN           NAN            NAN                     NAN   \n",
       "8              32.0       0.00002            all                     NAN   \n",
       "0               NAN           NAN            NAN                     NAN   \n",
       "1               NAN           NAN            NAN                     NAN   \n",
       "2               NAN           NAN            NAN                     NAN   \n",
       "3               NAN           NAN            NAN                     NAN   \n",
       "14             32.0        0.0005          mixed                   merge   \n",
       "16             32.0        0.0005          mixed                   merge   \n",
       "18             32.0        0.0005          mixed                   merge   \n",
       "\n",
       "   num_new_tokens unfreeze_params_steps finetune_params_after_unfreeze  \\\n",
       "0             0.0                 500.0                           lora   \n",
       "1           100.0                 500.0                           lora   \n",
       "2            10.0                 500.0                     first_last   \n",
       "3            10.0                 500.0                           lora   \n",
       "4           100.0                 500.0                     first_last   \n",
       "5          1000.0                 500.0                           lora   \n",
       "6          1000.0                 500.0                     first_last   \n",
       "7             NAN                   NAN                            NAN   \n",
       "8             0.0                 500.0                     first_last   \n",
       "0             NAN                   NAN                            NAN   \n",
       "1          1000.0                   NAN                            NAN   \n",
       "2           100.0                   NAN                            NAN   \n",
       "3            10.0                   NAN                            NAN   \n",
       "14         1000.0                  -1.0                            NAN   \n",
       "16           10.0                  -1.0                            NAN   \n",
       "18          100.0                  -1.0                            NAN   \n",
       "\n",
       "                                              dataset  \\\n",
       "0   magpie-default-tokenized_0,magpie-translation-...   \n",
       "1   magpie-default-tokenized_100,magpie-translatio...   \n",
       "2   magpie-default-tokenized_10,magpie-translation...   \n",
       "3   magpie-default-tokenized_10,magpie-translation...   \n",
       "4   magpie-default-tokenized_100,magpie-translatio...   \n",
       "5   magpie-default-tokenized_1000,magpie-translati...   \n",
       "6   magpie-default-tokenized_1000,magpie-translati...   \n",
       "7                                                 NAN   \n",
       "8   magpie-default-tokenized_0,magpie-translation-...   \n",
       "0                                                 NAN   \n",
       "1                                                 NAN   \n",
       "2                                                 NAN   \n",
       "3                                                 NAN   \n",
       "14  magpie-default-tokenized_1000,magpie-translati...   \n",
       "16  magpie-default-tokenized_10,magpie-translation...   \n",
       "18  magpie-default-tokenized_100,magpie-translatio...   \n",
       "\n",
       "                                       tokenizer_path    seed reset_optimizer  \\\n",
       "0                    meta-llama/Llama-3.2-3B-Instruct  1234.0            True   \n",
       "1   /cmlscratch/astein0/efficient_tokenization_for...  1234.0            True   \n",
       "2   /cmlscratch/astein0/efficient_tokenization_for...  1234.0            True   \n",
       "3   /cmlscratch/astein0/efficient_tokenization_for...  1234.0            True   \n",
       "4   /cmlscratch/astein0/efficient_tokenization_for...  1234.0            True   \n",
       "5   /cmlscratch/astein0/efficient_tokenization_for...  1234.0            True   \n",
       "6   /cmlscratch/astein0/efficient_tokenization_for...  1234.0            True   \n",
       "7                                                 NAN     NAN             NAN   \n",
       "8                    meta-llama/Llama-3.2-3B-Instruct  1234.0            True   \n",
       "0                                                 NAN     NAN             NAN   \n",
       "1                                                 NAN     NAN             NAN   \n",
       "2                                                 NAN     NAN             NAN   \n",
       "3                                                 NAN     NAN             NAN   \n",
       "14  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "16  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "18  /cmlscratch/astein0/efficient_tokenization_for...  1234.0           False   \n",
       "\n",
       "   warmup_steps lr_schedule target_modules    r lora_alpha lora_dropout  \\\n",
       "0         100.0      cosine         linear  8.0       16.0         0.05   \n",
       "1         100.0      cosine         linear  8.0       16.0         0.05   \n",
       "2         100.0      cosine            NAN  NAN        NAN          NAN   \n",
       "3         100.0      cosine         linear  8.0       16.0         0.05   \n",
       "4         100.0      cosine            NAN  NAN        NAN          NAN   \n",
       "5         100.0      cosine         linear  8.0       16.0         0.05   \n",
       "6         100.0      cosine            NAN  NAN        NAN          NAN   \n",
       "7           NAN         NAN            NAN  NAN        NAN          NAN   \n",
       "8         100.0      cosine            NAN  NAN        NAN          NAN   \n",
       "0           NAN         NAN            NAN  NAN        NAN          NAN   \n",
       "1           NAN         NAN            NAN  NAN        NAN          NAN   \n",
       "2           NAN         NAN            NAN  NAN        NAN          NAN   \n",
       "3           NAN         NAN            NAN  NAN        NAN          NAN   \n",
       "14        250.0      cosine         linear  8.0       16.0         0.05   \n",
       "16        250.0      cosine         linear  8.0       16.0         0.05   \n",
       "18        250.0      cosine         linear  8.0       16.0         0.05   \n",
       "\n",
       "   finetune_params_prefreeze warmup_steps_prefreeze lr_schedule_prefreeze  \n",
       "0                        NAN                    NAN                   NAN  \n",
       "1                        NAN                    NAN                   NAN  \n",
       "2                        NAN                    NAN                   NAN  \n",
       "3                        NAN                    NAN                   NAN  \n",
       "4                        NAN                    NAN                   NAN  \n",
       "5                        NAN                    NAN                   NAN  \n",
       "6                        NAN                    NAN                   NAN  \n",
       "7                        NAN                    NAN                   NAN  \n",
       "8                        NAN                    NAN                   NAN  \n",
       "0                        NAN                    NAN                   NAN  \n",
       "1                        NAN                    NAN                   NAN  \n",
       "2                        NAN                    NAN                   NAN  \n",
       "3                        NAN                    NAN                   NAN  \n",
       "14                       NAN                   -1.0                   NAN  \n",
       "16                       NAN                   -1.0                   NAN  \n",
       "18                       NAN                   -1.0                   NAN  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templating_results = get_results(results_file_name, \"templating\")\n",
    "# display(templating_results)\n",
    "patching_results = get_results(results_file_name, \"patching\")\n",
    "# display(patching_results)\n",
    "patching_plus_finetuning = get_results(results_file_name, \"baseline_embeddings\")\n",
    "patching_plus_finetuning = patching_plus_finetuning[patching_plus_finetuning[\"model_name\"].str.contains(\"plus\")]\n",
    "# display(patching_plus_finetuning)\n",
    "\n",
    "\n",
    "\n",
    "main_results = pd.concat([templating_results, patching_results, patching_plus_finetuning])\n",
    "main_results.sort_values(by=\"prompt_level_strict_acc\", ascending=True)\n",
    "main_results.fillna(\"NAN\", inplace=True)\n",
    "main_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>finetuning</th>\n",
       "      <th>num_new_tokens</th>\n",
       "      <th>prompt_level_strict_acc</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>theoretical_compression_ratio</th>\n",
       "      <th>learning_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>embeddings + lora</td>\n",
       "      <td>0</td>\n",
       "      <td>0.670980</td>\n",
       "      <td>-0.010802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>embeddings + first_last</td>\n",
       "      <td>0</td>\n",
       "      <td>0.659889</td>\n",
       "      <td>-0.011285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched</td>\n",
       "      <td>10</td>\n",
       "      <td>0.680222</td>\n",
       "      <td>-0.006470</td>\n",
       "      <td>0.017298</td>\n",
       "      <td>0.025475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>embeddings + lora</td>\n",
       "      <td>10</td>\n",
       "      <td>0.645102</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>0.017551</td>\n",
       "      <td>0.024059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>embeddings + first_last</td>\n",
       "      <td>10</td>\n",
       "      <td>0.628466</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.014429</td>\n",
       "      <td>0.014769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched + lora</td>\n",
       "      <td>10</td>\n",
       "      <td>0.534196</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.014978</td>\n",
       "      <td>0.037306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched</td>\n",
       "      <td>100</td>\n",
       "      <td>0.659889</td>\n",
       "      <td>0.006034</td>\n",
       "      <td>0.063345</td>\n",
       "      <td>0.058326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>embeddings + lora</td>\n",
       "      <td>100</td>\n",
       "      <td>0.597043</td>\n",
       "      <td>0.035448</td>\n",
       "      <td>0.060531</td>\n",
       "      <td>0.024583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>embeddings + first_last</td>\n",
       "      <td>100</td>\n",
       "      <td>0.613678</td>\n",
       "      <td>0.020545</td>\n",
       "      <td>0.055265</td>\n",
       "      <td>0.034478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched + lora</td>\n",
       "      <td>100</td>\n",
       "      <td>0.467652</td>\n",
       "      <td>0.037431</td>\n",
       "      <td>0.051094</td>\n",
       "      <td>0.025288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.615527</td>\n",
       "      <td>0.058259</td>\n",
       "      <td>0.169940</td>\n",
       "      <td>0.106773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>embeddings + first_last</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.608133</td>\n",
       "      <td>0.023592</td>\n",
       "      <td>0.120402</td>\n",
       "      <td>0.095626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>embeddings + lora</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.543438</td>\n",
       "      <td>0.051198</td>\n",
       "      <td>0.131187</td>\n",
       "      <td>0.076424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched + lora</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.404806</td>\n",
       "      <td>0.110800</td>\n",
       "      <td>0.136866</td>\n",
       "      <td>0.032170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>None</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.697782</td>\n",
       "      <td>-0.010677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name               finetuning num_new_tokens  \\\n",
       "0   Llama-3.2-3B-Instruct        embeddings + lora              0   \n",
       "1   Llama-3.2-3B-Instruct  embeddings + first_last              0   \n",
       "2   Llama-3.2-3B-Instruct                  patched             10   \n",
       "3   Llama-3.2-3B-Instruct        embeddings + lora             10   \n",
       "4   Llama-3.2-3B-Instruct  embeddings + first_last             10   \n",
       "5   Llama-3.2-3B-Instruct           patched + lora             10   \n",
       "6   Llama-3.2-3B-Instruct                  patched            100   \n",
       "7   Llama-3.2-3B-Instruct        embeddings + lora            100   \n",
       "8   Llama-3.2-3B-Instruct  embeddings + first_last            100   \n",
       "9   Llama-3.2-3B-Instruct           patched + lora            100   \n",
       "10  Llama-3.2-3B-Instruct                  patched           1000   \n",
       "11  Llama-3.2-3B-Instruct  embeddings + first_last           1000   \n",
       "12  Llama-3.2-3B-Instruct        embeddings + lora           1000   \n",
       "13  Llama-3.2-3B-Instruct           patched + lora           1000   \n",
       "14  Llama-3.2-3B-Instruct                     None       Baseline   \n",
       "\n",
       "    prompt_level_strict_acc  compression_ratio  theoretical_compression_ratio  \\\n",
       "0                  0.670980          -0.010802                       0.000000   \n",
       "1                  0.659889          -0.011285                       0.000000   \n",
       "2                  0.680222          -0.006470                       0.017298   \n",
       "3                  0.645102           0.005254                       0.017551   \n",
       "4                  0.628466          -0.000015                       0.014429   \n",
       "5                  0.534196           0.001799                       0.014978   \n",
       "6                  0.659889           0.006034                       0.063345   \n",
       "7                  0.597043           0.035448                       0.060531   \n",
       "8                  0.613678           0.020545                       0.055265   \n",
       "9                  0.467652           0.037431                       0.051094   \n",
       "10                 0.615527           0.058259                       0.169940   \n",
       "11                 0.608133           0.023592                       0.120402   \n",
       "12                 0.543438           0.051198                       0.131187   \n",
       "13                 0.404806           0.110800                       0.136866   \n",
       "14                 0.697782          -0.010677                       0.000000   \n",
       "\n",
       "    learning_ratio  \n",
       "0         0.012554  \n",
       "1         0.013064  \n",
       "2         0.025475  \n",
       "3         0.024059  \n",
       "4         0.014769  \n",
       "5         0.037306  \n",
       "6         0.058326  \n",
       "7         0.024583  \n",
       "8         0.034478  \n",
       "9         0.025288  \n",
       "10        0.106773  \n",
       "11        0.095626  \n",
       "12        0.076424  \n",
       "13        0.032170  \n",
       "14        0.012263  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this works for the baseline_embeddings and patched experiments\n",
    "agg_col = [\"num_new_tokens\", \"finetuning_params\", \"model_name\", \"model_name_sanitized\", \"finetune_params_after_unfreeze\"]\n",
    "agged = get_aggregated_results(main_results, agg_col)\n",
    "agged.reset_index(inplace=True)\n",
    "\n",
    "records = []\n",
    "agged.columns = ['_'.join(col).rstrip('_') if isinstance(col, tuple) else col for col in agged.columns]\n",
    "for _, row in agged.iterrows():\n",
    "    data_dict = {}\n",
    "    if not row[\"model_name_sanitized\"].startswith(\"output\"):\n",
    "        model_name = row[\"model_name_sanitized\"].split(\"__\")[-1]\n",
    "        # new_embeddings_init = \"None\"\n",
    "        finetuning = \"None\"\n",
    "        row[\"num_new_tokens\"] = \"Baseline\"\n",
    "    elif row[\"model_name\"] == \"patching\":\n",
    "        model_name = \"Llama-3.2-3B-Instruct\"\n",
    "        # new_embeddings_init = \"patched\"\n",
    "        finetuning = \"patched\"\n",
    "    elif row[\"model_name\"] == \"Llama-3.2-3B\" and row[\"finetuning_params\"] == \"embeddings\":\n",
    "        model_name = \"Llama-3.2-3B\"\n",
    "        # new_embeddings_init = \"None\"\n",
    "        finetuning = \"embeddings\"\n",
    "    elif \"plus\" in row[\"model_name\"]:\n",
    "        model_name = \"Llama-3.2-3B-Instruct\"\n",
    "        finetuning = f\"patched + {row['finetuning_params']}\"\n",
    "    elif row[\"model_name\"].startswith(\"Llama-3.2-3B-Instruct\") and row[\"num_new_tokens\"] != \"NAN\":\n",
    "        model_name = \"Llama-3.2-3B-Instruct\"\n",
    "        # new_embeddings_init = \"None\"\n",
    "        finetuning = f\"{row['finetuning_params']} + {row['finetune_params_after_unfreeze']}\"\n",
    "    else:  # loraed\n",
    "        model_name = \"Llama-3.2-3B\"\n",
    "        new_embeddings_init = \"learned\"\n",
    "        finetuning = \"lora\"\n",
    "\n",
    "    data_dict[\"model_name\"] = model_name\n",
    "    # data_dict[\"new_embeddings_init\"] = new_embeddings_init\n",
    "    data_dict[\"finetuning\"] = finetuning\n",
    "    try:\n",
    "        data_dict[\"num_new_tokens\"] = int(row[\"num_new_tokens\"])\n",
    "    except:\n",
    "        data_dict[\"num_new_tokens\"] = row[\"num_new_tokens\"]\n",
    "    data_dict[\"prompt_level_strict_acc\"] = row[\"prompt_level_strict_acc_mean\"]\n",
    "    data_dict[\"compression_ratio\"] = row[\"compression_ratio_mean\"]\n",
    "    data_dict[\"theoretical_compression_ratio\"] = row[\"theoretical_compression_ratio_mean\"]\n",
    "    data_dict[\"learning_ratio\"] = row[\"learning_ratio_mean\"]\n",
    "    records.append(data_dict)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               model_name               finetuning num_new_tokens  \\\n",
      "0   Llama-3.2-3B-Instruct        embeddings + lora              0   \n",
      "1   Llama-3.2-3B-Instruct  embeddings + first_last              0   \n",
      "2   Llama-3.2-3B-Instruct                  patched             10   \n",
      "3   Llama-3.2-3B-Instruct        embeddings + lora             10   \n",
      "4   Llama-3.2-3B-Instruct  embeddings + first_last             10   \n",
      "5   Llama-3.2-3B-Instruct           patched + lora             10   \n",
      "6   Llama-3.2-3B-Instruct                  patched            100   \n",
      "7   Llama-3.2-3B-Instruct        embeddings + lora            100   \n",
      "8   Llama-3.2-3B-Instruct  embeddings + first_last            100   \n",
      "9   Llama-3.2-3B-Instruct           patched + lora            100   \n",
      "10  Llama-3.2-3B-Instruct                  patched           1000   \n",
      "11  Llama-3.2-3B-Instruct  embeddings + first_last           1000   \n",
      "12  Llama-3.2-3B-Instruct        embeddings + lora           1000   \n",
      "13  Llama-3.2-3B-Instruct           patched + lora           1000   \n",
      "14  Llama-3.2-3B-Instruct                     None       Baseline   \n",
      "\n",
      "    prompt_level_strict_acc  compression_ratio  theoretical_compression_ratio  \\\n",
      "0                  0.670980          -0.010802                       0.000000   \n",
      "1                  0.659889          -0.011285                       0.000000   \n",
      "2                  0.680222          -0.006470                       0.017298   \n",
      "3                  0.645102           0.005254                       0.017551   \n",
      "4                  0.628466          -0.000015                       0.014429   \n",
      "5                  0.534196           0.001799                       0.014978   \n",
      "6                  0.659889           0.006034                       0.063345   \n",
      "7                  0.597043           0.035448                       0.060531   \n",
      "8                  0.613678           0.020545                       0.055265   \n",
      "9                  0.467652           0.037431                       0.051094   \n",
      "10                 0.615527           0.058259                       0.169940   \n",
      "11                 0.608133           0.023592                       0.120402   \n",
      "12                 0.543438           0.051198                       0.131187   \n",
      "13                 0.404806           0.110800                       0.136866   \n",
      "14                 0.697782          -0.010677                       0.000000   \n",
      "\n",
      "    learning_ratio  \n",
      "0         0.012554  \n",
      "1         0.013064  \n",
      "2         0.025475  \n",
      "3         0.024059  \n",
      "4         0.014769  \n",
      "5         0.037306  \n",
      "6         0.058326  \n",
      "7         0.024583  \n",
      "8         0.034478  \n",
      "9         0.025288  \n",
      "10        0.106773  \n",
      "11        0.095626  \n",
      "12        0.076424  \n",
      "13        0.032170  \n",
      "14        0.012263  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9gAAAJOCAYAAABMYq+bAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3xT1fvA8U+SpnsvWtqyyx5VkCFTVmWj7L1FhiB8UURF1B8bQYYMUaZMQaaibGQowwEyZBbKbOmge6XN/f0RGxqaDqDQwfN+vfKiuffck3PTm9DnnnOeo1IURUEIIYQQQgghhBBPRZ3fDRBCCCGEEEIIIYoCCbCFEEIIIYQQQog8IAG2EEIIIYQQQgiRByTAFkIIIYQQQggh8oAE2EIIIYQQQgghRB6QAFsIIYQQQgghhMgDEmALIYQQQgghhBB5QAJsIYQQQgghhBAiD0iALYQQQgghhBBC5AEJsIUQQogsHDp0CJVKxebNm/O7KbkSGhpK586dcXNzQ6VSMXfu3PxuUoGgUqkYOXJkfjcjV27cuIFKpeKLL74oEO1YuXJlvrYjL6V/ng8dOpTfTRFCFGESYAshXhiLFi1CpVJRp06d/G6KyGDlypWoVCqsra25c+dOpv1NmjShatWq+dCywmfMmDHs3r2bCRMm8N133/H6669nWValUqFSqZg9e3amfem/kz/++ONZNjdb6e3L6SHB0pNJ/x2be3zwwQfPrR2LFi0qUkG8EEJY5HcDhBDieVm7di2lSpXi5MmTXL16lXLlyuV3k0QGycnJTJ8+nQULFuR3UwqtAwcO0KFDB8aNG5frY2bNmsWwYcOwtbV9hi17fN99953J89WrV7N3795M2ytVqvQ8m1XkfP7555QuXdpkW9WqVSlZsiSJiYlotdpn+vqLFi3C3d2d/v37P9PXAWjUqBGJiYlYWlo+89cSQry4JMAWQrwQrl+/zm+//caWLVsYOnQoa9euZdKkSfndLLPi4+Oxs7PL72Y8dwEBAXzzzTdMmDCB4sWL53dznqu8+p3fv38fZ2fnXJcPCAjg9OnTLFmyhLFjxz716+el3r17mzw/fvw4e/fuzbRdPJ1WrVpRq1Yts/usra2fc2ueLbVaXeTOSQhR8MgQcSHEC2Ht2rW4uLjQpk0bOnfuzNq1a82Wi4qKYsyYMZQqVQorKyt8fX3p27cv4eHhxjJJSUl8+umnlC9fHmtra7y9vXnzzTe5du0akPU8P3NzGvv374+9vT3Xrl2jdevWODg40KtXLwCOHDlCly5dKFGiBFZWVvj5+TFmzBgSExMztfvixYt07doVDw8PbGxsqFChAh999BEABw8eRKVSsXXr1kzHrVu3DpVKxe+//272/fjjjz9QqVSsWrUq077du3ejUqn48ccfAYiNjeXdd981vneenp60aNGCv/76y2zdj/rwww9JS0tj+vTp2ZbLbm6oSqXi008/NT7/9NNPUalUXL58md69e+Pk5ISHhwcTJ05EURRu3bpFhw4dcHR0xMvLy+xwaYC0tDQ+/PBDvLy8sLOzo3379ty6dStTuRMnTvD666/j5OSEra0tjRs35tixYyZl0tt04cIFevbsiYuLCw0aNMj2nIOCgujSpQuurq7Y2tpSt25dfvrpJ+P+9OG+iqKwcOFC41DfnNSvX5+mTZsyc+ZMs9fVoy5evEjnzp1xdXXF2tqaWrVqsWPHDuP+qKgoNBoN8+fPN24LDw9HrVbj5uaGoijG7cOGDcPLyyvH18xOfHw8//vf//Dz88PKyooKFSrwxRdfmLxOViZPnoxarTYZMfHzzz/TsGFD7OzscHBwoE2bNpw/f97kuPTP7J07d+jYsSP29vZ4eHgwbtw40tLSTMpu2LCBmjVr4uDggKOjI9WqVWPevHm5Pr8vv/ySkiVLYmNjQ+PGjTl37pxx34oVK1CpVPz999+Zjps6dSoajcbslIvcyu77KjfnrtfrmTt3LlWqVMHa2ppixYoxdOhQHjx4YCxTqlQpzp8/z6+//mq8Zps0aQI8/Jw8Kv1av3Hjhkk9bdu25ejRo9SuXRtra2vKlCnD6tWrTY41992cPgXlwoULvPbaa9ja2uLj48PMmTMzvXZwcDDt27fHzs4OT09P45QMmaoghMhIAmwhxAth7dq1vPnmm1haWtKjRw+uXLnCqVOnTMrExcXRsGFDFixYQMuWLZk3bx5vv/02Fy9e5Pbt24Ah0Grbti2fffYZNWvWZPbs2YwePZro6GiTP34fR2pqKoGBgXh6evLFF1/QqVMnADZt2kRCQgLDhg1jwYIFBAYGsmDBAvr27Wty/D///EOdOnU4cOAAQ4YMYd68eXTs2JGdO3cChj8g/fz8zN5UWLt2LWXLlqVevXpm21arVi3KlCnD999/n2nfxo0bcXFxITAwEIC3336bxYsX06lTJxYtWsS4ceOwsbHh33//zdX7ULp0afr27cs333zD3bt3c3VMbnXr1g29Xs/06dOpU6cOkydPZu7cubRo0QIfHx9mzJhBuXLlGDduHIcPH850/JQpU/jpp58YP348o0aNYu/evTRv3twkKD1w4ACNGjUiJiaGSZMmMXXqVKKiomjatCknT57MVGeXLl1ISEhg6tSpDBkyJMu2h4aG8uqrr7J7926GDx/OlClTSEpKon379sabJo0aNTIOnW7RogXfffddpqHUWfn0008JDQ1l8eLF2ZY7f/48devW5d9//+WDDz5g9uzZ2NnZ0bFjR2M7nJ2dqVq1qsl7ePToUVQqFZGRkVy4cMG4/ciRIzRs2DBXbTRHURTat2/Pl19+yeuvv86cOXOoUKEC7733Xo698R9//DGffPIJX3/9Ne+88w5gGJLepk0b7O3tmTFjBhMnTuTChQs0aNDAJJgDw/dAYGAgbm5ufPHFFzRu3JjZs2ezdOlSY5m9e/fSo0cPXFxcmDFjBtOnT6dJkyaZbrhkZfXq1cyfP58RI0YwYcIEzp07R9OmTQkNDQWgc+fO2NjYZPm5btKkCT4+Pjm+TnR0NOHh4SaP7OTm3AGGDh3Ke++9R/369Zk3bx4DBgxg7dq1BAYGotPpAJg7dy6+vr5UrFjReM2m3xh8XFevXqVz5860aNGC2bNn4+LiQv/+/TPdIDHnwYMHvP7669SoUYPZs2dTsWJFxo8fz88//2wsEx8fT9OmTdm3bx+jRo3io48+4rfffmP8+PFP1F4hRBGmCCFEEffHH38ogLJ3715FURRFr9crvr6+yujRo03KffLJJwqgbNmyJVMder1eURRFWb58uQIoc+bMybLMwYMHFUA5ePCgyf7r168rgLJixQrjtn79+imA8sEHH2SqLyEhIdO2adOmKSqVSgkODjZua9SokeLg4GCyLWN7FEVRJkyYoFhZWSlRUVHGbffv31csLCyUSZMmZXqdjCZMmKBotVolMjLSuC05OVlxdnZWBg4caNzm5OSkjBgxItu6zFmxYoUCKKdOnVKuXbumWFhYKKNGjTLub9y4sVKlShXjc3PvYzrA5HwmTZqkAMpbb71l3Jaamqr4+voqKpVKmT59unH7gwcPFBsbG6Vfv37Gbem/Sx8fHyUmJsa4/fvvv1cAZd68eYqiGN5rf39/JTAw0OR9T0hIUEqXLq20aNEiU5t69OiRq/fn3XffVQDlyJEjxm2xsbFK6dKllVKlSilpaWkm55/b30HGsq+99pri5eVlvOYy/k7SNWvWTKlWrZqSlJRk3KbX65VXX31V8ff3N24bMWKEUqxYMePzsWPHKo0aNVI8PT2VxYsXK4qiKBEREYpKpTK+f7kxYsQIJeOfLdu2bVMAZfLkySblOnfurKhUKuXq1atmz/V///ufolarlZUrVxr3x8bGKs7OzsqQIUNM6goJCVGcnJxMtqd/Zj///HOTsi+99JJSs2ZN4/PRo0crjo6OSmpqaq7PUVEeXt82NjbK7du3jdtPnDihAMqYMWOM23r06KEUL17c5Br466+/svx8ZJT+Ozb3yNgOc99XOZ37kSNHFEBZu3atSblffvkl0/YqVaoojRs3ztS+9M9JVu2+fv26cVvJkiUVQDl8+LBx2/379xUrKyvlf//7n3Gbue/mxo0bK4CyevVq47bk5GTFy8tL6dSpk3Hb7NmzFUDZtm2bcVtiYqJSsWJFs9/3QogXl/RgCyGKvLVr11KsWDFee+01wDCMuFu3bmzYsMFkWOMPP/xAjRo1eOONNzLVkT5U8YcffsDd3d3Y62WuzJMYNmxYpm02NjbGn+Pj4wkPD+fVV19FURTjsNCwsDAOHz7MwIEDKVGiRJbt6du3L8nJySbLTW3cuJHU1NQc57R269YNnU7Hli1bjNv27NlDVFQU3bp1M25zdnbmxIkTT9X7XKZMGfr06cPSpUu5d+/eE9fzqMGDBxt/1mg01KpVC0VRGDRokHG7s7MzFSpUICgoKNPxffv2xcHBwfi8c+fOeHt7s2vXLgBOnz7NlStX6NmzJxEREcaewPj4eJo1a8bhw4fR6/Umdb799tu5avuuXbuoXbu2yTBye3t73nrrLW7cuGHSK/ykPv30U0JCQliyZInZ/ZGRkRw4cICuXbsSGxtrPL+IiAgCAwO5cuWKcThyw4YNCQ0N5dKlS4Chp7pRo0Y0bNiQI0eOAIZebUVRnqoHe9euXWg0GkaNGmWy/X//+x+Kopj0PoKhx3vkyJHMmzePNWvW0K9fP+O+vXv3EhUVRY8ePUx6cjUaDXXq1OHgwYOZXv/R31/Dhg1Nrh1nZ2fi4+PZu3fvE51fx44dTXqga9euTZ06dYzXHBiuy7t375q0b+3atdjY2BhHwuRk4cKF7N271+SRk5zOfdOmTTg5OdGiRQuT97NmzZrY29ubfT+fVuXKlU2uJw8Pjyw/z4+yt7c3+R60tLSkdu3aJsf+8ssv+Pj40L59e+M2a2vrbEefCCFeTBJgCyGKtLS0NDZs2MBrr73G9evXuXr1KlevXqVOnTqEhoayf/9+Y9lr167luBzUtWvXqFChAhYWeZcj0sLCAl9f30zbb968Sf/+/XF1dTXOdWzcuDFgGNYJGP8AzKndFStW5JVXXjEZTrp27Vrq1q2bYzb1GjVqULFiRTZu3GjctnHjRtzd3WnatKlx28yZMzl37hx+fn7Url2bTz/9NFd/3D7q448/JjU1Nce52I/j0ZsPTk5OWFtb4+7unml7xjmi6fz9/U2eq1QqypUrZxw6fOXKFQD69euHh4eHyePbb78lOTnZ+DtL92jm5qwEBwdToUKFTNvTs2cHBwfnqp7sNGrUiNdeey3LudhXr15FURQmTpyY6fzSkwXev38fwBjkHDlyhPj4eP7++28aNmxIo0aNjAH2kSNHcHR0pEaNGk/c5uDgYIoXL25y4wOyfl9Wr17NwoULWbBgAT169DDZl/77a9q0aabz27Nnj/Hc0llbW+Ph4WGyzcXFxeTaGT58OOXLl6dVq1b4+voycOBAfvnll1yf36PXHED58uVNhqu3aNECb29v4+dar9ezfv16OnTokOl9yUrt2rVp3ry5ySM7uTn3K1euEB0djaenZ6b3My4uLtP7mRce/Yyba1dWfH19M90gffTY4OBgypYtm6mcrEYhhHiUZBEXQhRpBw4c4N69e2zYsIENGzZk2r927VpatmyZp6+ZVU/2o0mA0llZWaFWqzOVbdGiBZGRkYwfP56KFStiZ2fHnTt36N+/f6be0Nzo27cvo0eP5vbt2yQnJ3P8+HG++uqrXB3brVs3pkyZQnh4OA4ODuzYsYMePXqY3Gjo2rUrDRs2ZOvWrezZs4dZs2YxY8YMtmzZQqtWrXLdzjJlytC7d2+WLl1qdj3ex31/wdBrnZttQK4SZD0q/fcxa9YsAgICzJaxt7c3eZ5xhEJBMGnSJJo0acLXX3+dKRN5+vmNGzfOOOf+UemBRvHixSldujSHDx+mVKlSKIpCvXr18PDwYPTo0QQHB3PkyBFeffXVTNf9s1S/fn1Onz7NV199RdeuXXF1dTXuSz+/7777zmzitUdvqGV17WTk6enJ6dOn2b17Nz///DM///wzK1asoG/fvmaTBj4JjUZDz549+eabb1i0aBHHjh3j7t27zzTTem7OXa/X4+npmWUyyUcDdHMe93P+NJ/nvPwuEEIICbCFEEXa2rVr8fT0ZOHChZn2bdmyha1bt7JkyRJsbGwoW7ZsjonKypYty4kTJ9DpdFmuD+vi4gIYMipn9Dg9jWfPnuXy5cusWrXKJKnZo8M3y5QpA5CrBGvdu3dn7NixrF+/3ri+bcYh3tnp1q0bn332GT/88APFihUjJiaG7t27Zyrn7e3N8OHDGT58OPfv3+fll19mypQpjxVgg6EXe82aNcyYMSPTvrx4fx9Xeg9nOkVRuHr1KtWrVwcM1wWAo6Njjj2Aj6tkyZLG4dYZXbx40bg/LzRu3JgmTZowY8YMPvnkE5N96deZVqvN1fk1bNiQw4cPU7p0aQICAnBwcKBGjRo4OTnxyy+/8Ndff/HZZ589VXtLlizJvn37iI2NNemtzep9KVeuHDNnzqRJkya8/vrr7N+/33hc+u/P09MzT39/lpaWtGvXjnbt2qHX6xk+fDhff/01EydOzLHn89FrDuDy5cuUKlXKZFvfvn2ZPXs2O3fu5Oeff8bDwyPLmyDPS9myZdm3bx/169fP8UZSVoF0xs95xhs+z/Jznp2SJUty4cIFFEUxafPVq1fzpT1CiIJLhogLIYqsxMREtmzZQtu2bencuXOmx8iRI4mNjTUuM9SpUyfOnDljdjmr9J6MTp06ER4ebrbnN71MyZIl0Wg0mbJRL1q0KNdtT+9RydiDoihKpiV+PDw8aNSoEcuXL+fmzZtm25PO3d2dVq1asWbNGtauXcvrr7+eaYh0VipVqkS1atXYuHEjGzduxNvbm0aNGhn3p6WlZRoC7enpSfHixUlOTs7Va2RUtmxZevfuzddff01ISIjJPkdHR9zd3Z/q/X1cq1evJjY21vh88+bN3Lt3z3jjoGbNmpQtW5YvvviCuLi4TMeHhYU98Wu3bt2akydPmiylFh8fz9KlSylVqhSVK1d+4roflT4X+9GM0J6ensbebXNz4x89v4YNG3Ljxg02btxoHDKuVqt59dVXmTNnDjqd7qnmX4PhfUlLS8v0Wfzyyy9RqVRmb+pUr16dXbt28e+//9KuXTvjcPjAwEAcHR2ZOnWqMcN1dueXGxERESbP1Wq18YZMbj4T27ZtM1lm6+TJk5w4cSLTeVWvXp3q1avz7bff8sMPP9C9e/c8ncLyJLp27UpaWhr/93//l2lfamqqyc0xOzu7TDfL4OFNj4yf8/j4+Dzr/X9cgYGB3Llzx2RZuqSkJL755pt8aY8QouCSHmwhRJG1Y8cOYmNjTZLSZFS3bl08PDxYu3Yt3bp147333mPz5s106dKFgQMHUrNmTSIjI9mxYwdLliyhRo0a9O3bl9WrVzN27FhOnjxJw4YNiY+PZ9++fQwfPpwOHTrg5OREly5dWLBgASqVirJly/Ljjz8+1rzDihUrUrZsWcaNG8edO3dwdHTkhx9+MDufcP78+TRo0ICXX36Zt956i9KlS3Pjxg1++uknTp8+bVK2b9++dO7cGcDsH7/Z6datG5988gnW1tYMGjTIZHhvbGwsvr6+dO7cmRo1amBvb8++ffs4depUlmtL5+Sjjz7iu+++49KlS1SpUsVk3+DBg5k+fTqDBw+mVq1aHD58mMuXLz/R6+SGq6srDRo0YMCAAYSGhjJ37lzKlStnTHCkVqv59ttvadWqFVWqVGHAgAH4+Phw584dDh48iKOjo3HZtMf1wQcfsH79elq1asWoUaNwdXVl1apVXL9+nR9++CFPh1k3btyYxo0b8+uvv2bat3DhQho0aEC1atUYMmQIZcqUITQ0lN9//53bt29z5swZY9n04PnSpUtMnTrVuL1Ro0b8/PPPWFlZ8corrzxVW9u1a8drr73GRx99xI0bN6hRowZ79uxh+/btvPvuu8YA7VF169Zl+/bttG7dms6dO7Nt2zYcHR1ZvHgxffr04eWXX6Z79+54eHhw8+ZNfvrpJ+rXr5/r6RTpBg8eTGRkJE2bNsXX15fg4GAWLFhAQECAcZ54dsqVK0eDBg0YNmwYycnJzJ07Fzc3N95///1MZfv27cu4ceMAnunw8Nxq3LgxQ4cOZdq0aZw+fZqWLVui1Wq5cuUKmzZtYt68ecbvoZo1a7J48WImT55MuXLl8PT0pGnTprRs2ZISJUowaNAg3nvvPTQaDcuXLzf+Xp63oUOH8tVXX9GjRw9Gjx5tnPtubW0NPF2SSyFEEfPc85YLIcRz0q5dO8Xa2lqJj4/Pskz//v0VrVarhIeHK4piWD5o5MiRio+Pj2Jpaan4+voq/fr1M+5XFMPSSx999JFSunRpRavVKl5eXkrnzp2Va9euGcuEhYUpnTp1UmxtbRUXFxdl6NChyrlz58wue2NnZ2e2bRcuXFCaN2+u2NvbK+7u7sqQIUOUM2fOmF2C59y5c8obb7yhODs7K9bW1kqFChWUiRMnZqozOTlZcXFxUZycnJTExMTcvI1GV65cMS7jc/To0Uz1vvfee0qNGjUUBwcHxc7OTqlRo4ayaNGiHOs1tyRUuvRlgTIu06Uoht/BoEGDFCcnJ8XBwUHp2rWrcv/+/SyX6QoLC8tUr7n3/dElwdKX9Vm/fr0yYcIExdPTU7GxsVHatGmTaVk0RVGUv//+W3nzzTcVNzc3xcrKSilZsqTStWtXZf/+/Tm2KTvXrl1TOnfubPz91q5dW/nxxx8zleMJl+nKKP2czf1Orl27pvTt21fx8vJStFqt4uPjo7Rt21bZvHlzpno8PT0VQAkNDTVuO3r0qAIoDRs2zFUbM3p0mS5FMSyvNWbMGKV48eKKVqtV/P39lVmzZpkslZbVuW7fvl2xsLBQunXrZlzm6uDBg0pgYKDi5OSkWFtbK2XLllX69++v/PHHH8bjsrp2Hl1WavPmzUrLli0VT09PxdLSUilRooQydOhQ5d69e9meZ/ryWLNmzVJmz56t+Pn5KVZWVkrDhg2VM2fOmD3m3r17ikajUcqXL59t3Rll97nL2I7cfF9ltaTW0qVLlZo1ayo2NjaKg4ODUq1aNeX9999X7t69aywTEhKitGnTRnFwcFAAkyW7/vzzT6VOnTrG92/OnDlZLtPVpk2bTK/fuHFjk/qyWqbr0e+X9HMtWbKkybagoCClTZs2io2NjeLh4aH873//U3744QcFUI4fP56pDiHEi0mlKJLBQQghXhSpqakUL16cdu3asWzZsvxujhAiD4SHh+Pt7c0nn3zCxIkT87s5L5S5c+cyZswYbt++bbKsmhDixSVzsIUQ4gWybds2wsLCTBKnCSEKt5UrV5KWlkafPn3yuylF2qNL2CUlJfH111/j7+8vwbUQwkjmYAshxAvgxIkT/PPPP/zf//0fL730knE9bSFE4XXgwAEuXLjAlClT6NixY6YM4yJvvfnmm5QoUYKAgACio6NZs2YNFy9ezHI5MiHEi0mGiAshxAugf//+rFmzhoCAAFauXEnVqlXzu0lCiKfUpEkTfvvtN+rXr8+aNWukF/UZmzt3Lt9++y03btwgLS2NypUr8/777+d6uUMhxItBAmwhhBBCCCGEECIPyBxsIYQQQgghhBAiD0iALYQQQgghhBBC5AFJcmaGXq/n7t27ODg4oFKp8rs5QgghhBBCCCGeA0VRiI2NpXjx4qjVj98fLQG2GXfv3sXPzy+/myGEEEIIIYQQIh/cunULX1/fxz5OAmwzHBwcAMOb6ujomM+tMU+n07Fnzx5atmyJVqvN7+aIF5hci6IgketRFBRyLYqCQq5FUZAUhusxJiYGPz8/Y0z4uCTANiN9WLijo2OBDrBtbW1xdHQssBeneDHItSgKErkeRUEh16IoKORaFAVJYboen3SqsCQ5E0IIIYQQQggh8oAE2EIIIYQQQgghRB6QAFsIIYQQQgghhMgDEmALIYQQQgghhBB5QAJsIYQQQgghhBAiD0iALYQQQgghhBBC5AEJsIUQQgghhBBCiDwgAbYQQgghhBBCCJEHJMAWQgghhBBCCCHygATYQgghhBBCCCFEHpAAWwghhBBCCCGEyAMSYAshhBBCCCGEEHmgQATYCxcupFSpUlhbW1OnTh1OnjyZZdkmTZqgUqkyPdq0aWMsoygKn3zyCd7e3tjY2NC8eXOuXLnyPE5FCCGEEEIIIcQLKt8D7I0bNzJ27FgmTZrEX3/9RY0aNQgMDOT+/ftmy2/ZsoV79+4ZH+fOnUOj0dClSxdjmZkzZzJ//nyWLFnCiRMnsLOzIzAwkKSkpOd1WkIIIYQQQgghXjD5HmDPmTOHIUOGMGDAACpXrsySJUuwtbVl+fLlZsu7urri5eVlfOzduxdbW1tjgK0oCnPnzuXjjz+mQ4cOVK9endWrV3P37l22bdv2HM9MCCGEEEIIIcSLJF8D7JSUFP7880+aN29u3KZWq2nevDm///57rupYtmwZ3bt3x87ODoDr168TEhJiUqeTkxN16tTJdZ1CCCGEEEIIIcTjssjPFw8PDyctLY1ixYqZbC9WrBgXL17M8fiTJ09y7tw5li1bZtwWEhJirOPROtP3PSo5OZnk5GTj85iYGAB0Oh06nS53J/OcpberoLZPvDjkWhQFiVyPoqCQa1EUFHItioKkMFyPT9u2fA2wn9ayZcuoVq0atWvXfqp6pk2bxmeffZZp+549e7C1tX2qup+1vXv35ncThADkWhQFi1yPoqCQa1EUFHItioKkIF+PCQkJT3V8vgbY7u7uaDQaQkNDTbaHhobi5eWV7bHx8fFs2LCBzz//3GR7+nGhoaF4e3ub1BkQEGC2rgkTJjB27Fjj85iYGPz8/GjSpAmOjo6ZyqvVaiwsHr51KSkpWbZTpVKh1WqfqKxOp0NRFLNlU1NTOXToEC1atECr1WZb9nHqBbC0tHyisqmpqej1+jwpq9VqUalUz7RsWloaaWlpeVLWwsICtVpdYMrq9XpSU1OzLKvRaNBoNHlSVqfTceDAAZo2bYq1tfUT1asoSrZ3Cx+nbMbP57MqC9l/lgvCd8SzKgsF+zsi4/WYfk7yHZG57PP8jsiLsoXxOyI+Pj7TtZhVWfmOMJC/I57N5z4tLY29e/fStGlT43E51VsQPvdF/TviRf07QqfT8euvv+YqhoH8+Y5IH838pPI1wLa0tKRmzZrs37+fjh07AoYP0/79+xk5cmS2x27atInk5GR69+5tsr106dJ4eXmxf/9+Y0AdExPDiRMnGDZsmNm6rKyssLKyyrR9/vz5WFtbZ9ru7+9Pz549jc9nzZqV5QeqZMmS9O/f3/h87ty5Wd4VKV68OEOGDDE+X7hwIdHR0WbLuru74+vri1arRavV8s033xAWFma2rJOTE++++67x+cqVK7l7967Zsra2trz33nvG52vXriU4ONhsWa1Wy4cffmh8vmnTpmyXQ5s0aZLx523btnHhwoUsy06YMMH4Qf3pp584c+ZMlmXHjRtnnIO/Z88e/vjjjyzLjh49GmdnZwAOHjyY7bz8YcOG4enpCcCxY8f49ddfsyw7ePBgfHx8AMPUhX379mVZtl+/fpQqVQqAv//+m59//jnLsj169KB8+fIAnD9/nu3bt2dZtnPnzlSpUsVYdvPmzVmW7dChg/HzcfnyZdavX59l2VatWhlHidy4cYNVq1aZLXf27FmaN29O/fr1Abhz5w7ffvttlvU2btyYJk2aAHD//n0WL16cZdl69erRsmVLAKKiopg3b16WZWvVqmVcti8+Pp4vvvgiy7I1atQwfvekpKRkW7Zy5comqxVMnTo1y7IF4TvCw8OD4cOHG5+/aN8RZ8+eNf4s3xEG+f0dAbxQ3xHpbch4LaaT7wgD+TvioWf5HVG6dGnA8Fn+8ccfsywr3xEG8neEwbP6jrCxsaFChQrGGKYgfkc87cpT+T5EfOzYsfTr149atWpRu3Zt5s6dS3x8PAMGDACgb9+++Pj4MG3aNJPjli1bRseOHXFzczPZrlKpePfdd5k8eTL+/v6ULl2aiRMnUrx4ceMHQAghhBBCCCGEyGsqJbs++efkq6++YtasWYSEhBAQEMD8+fOpU6cOAE2aNKFUqVKsXLnSWP7SpUtUrFiRPXv20KJFi0z1KYrCpEmTWLp0KVFRUTRo0IBFixYZ7/LnJCYmBicnJ8LCwgr0EPG9e/fSunVrGSJexId2FfThnzqdjt27dxMYGChDxJ+gbGEb2gUF+zsi4/UoQ8SzLlsQhnQW9e+I+Pj4TNdiVmXlO8JA/o54dkPEd+3aRWBgoAwRz0VZ+Tvi2ZbV6XTs27cvVzEM5N8QcQ8PD6Kjo83GgjkpEAF2QZMeYD/pm/o86HQ6du3aZbw4hcgvci2KgkSuR1FQyLUoCgq5FkVBUhiux6eNBfN1HWwhhBBCCCGEEKKokABbCCGEEEIIIYTIAxJgCyGEEEIIIYQQeUACbCGEEEIIIYQQIg9IgC2EEEIIIYQQQuQBCbCFEEIIIYQQQog8IAG2EEIIIYQQQgiRByTAFkIIIYQQQggh8oAE2EIIIYQQQgghRB6QAFuYOHbsGDVq1MDW1paAgAB+//33bMtHRUUxePBg3N3dcXR0pFatWiQkJABw7tw5qlevjqurKx988IHJcW+//TbLli17ZuchhBBCCCGEEM+bBNjCKDIykrZt2zJy5EgePHjAiBEjaNu2LVFRUWbL6/V62rZti1ar5fLly0RFRfHNN9+g1WoBGD9+PMOGDeP69et8//33/Pnnn4AhiL98+TIDBw58XqcmhBBCCCGEEM+cBNjCaOvWrfj4+DBkyBCsrKwYMmQIXl5ebN261Wz5n3/+mZs3b7JgwQJcXV1Rq9W89NJLxgA7KCiIpk2b4uTkRO3atbl27Ro6nY5Ro0axePFiVCrV8zw9IYQQQgghhHimJMAWRv/88w8BAQEm2wICAvjnn3/Mlv/1118pV64cffr0wc3NjSpVqrBq1Srj/mrVqrF3716ioqL4888/qVq1KjNnzqR9+/ZUqFDhWZ6KEEIIIYQQQjx3EmALo7i4OJydnU22OTs7Exsba7Z8ZGQkBw8epH79+ty7d4+lS5cycuRIDh8+DMDs2bPZvXs3TZo0YfTo0VhaWvLDDz/wv//9j5EjR9KoUSPeeecddDrdsz41IYQQQgghhHjmJMB+ga1duxZ7e3vs7e2pUqUK9vb2REdHm5SJjo7GwcHB7PH29vb4+voycuRILC0tqV+/Ph07duTHH38EwM/Pj507d3L69GlGjhzJsGHDmDdvHmvWrCEhIYHDhw8TGxvL8uXLn/m5CiGEEEIIIcSzJgH2C6xXr17ExcURFxfH+fPnqV69OqdPnzYpc/r0aapVq2b2+Bo1auT6tVavXk2pUqVo2LAhZ86coU6dOgDUq1ePM2fOPPE5CCGEEEIIIURBIQG2MHrjjTe4ffs2y5YtIyUlhWXLlnHv3j3eeOONLMsnJSWxZMkS0tLSOHHiBNu3b6d9+/Ym5SIiIpg5cyYzZ84EoEyZMhw4cACdTseBAwcoW7bsMz83IYQQQgghhHjWJMAWRq6uruzcuZN58+bh5OTE/Pnz2blzJy4uLgDcvHkTe3t7bt68CRjmZ//0008sW7YMR0dH+vbty8KFC2nQoIFJvf/73//4+OOPjfUMHTqU2NhY3N3diY+PZ+jQoc/3RIUQQgghhBDiGbDI7waIvBccHMw///yDoihUqlQJf3//XB/boEGDLLOGlyhRgri4OJNttWvX5tSpU9nWuXLlSpPnjo6O7Nq1K9dtEkIIIYQQQojCQHqwi5ATJ07Qpk0bSpcuTfv27enQoQPly5enadOm7N+/P7+bJ4QQQgghhBBFmgTYRcSOHTto1KgRt27d4uuvv+bGjRvcvHmT1atXEx8fT8uWLTP1JAshhBBCCCGEyDsyRLwICA4Opnv37rRq1Yq1a9diaWlp3Ne9e3e6dOnCiBEjGDx4MAEBAQQEBORfY4UQQgghhBCiiJIe7CJgyZIlWFpasmLFCpPgOp1Go+Grr77Cx8eH+fPn50MLhRBCCCGEEKLokwC7CFi5ciW9e/fG3t7euO2XG7/wb+S/xudhYWHY2NiwYsUKVCpVpvWuH3Xu3DkCAwNxd3dHpVIRFRVlsv/QoUOULVsWT09PFixYYLKvVatWMudbCCGEEEII8cKRALuQS01NJSQkhBo1ahi3xevimXpqKr1+6cXAvQPZd3MfevQ0btw41/VqtVq6du2a5bztESNG8NVXX/HXX3/x6aefEhoaCsD69evx9PSkWbNmT3VeQgghhBBCCFHYyBzsQk6j0WBhYUFsbKxxW0JqAg19GrI3eC+nw05zOuw0XrZeVKhfAc1aDWnxaTnWW6FCBSpUqMCNGzfM7g8KCqJp06ZYWVnh7+9PcHAwlpaWTJ48mV9//TWvTk8IIYQQQgghCg3pwS7kVCoVTZo0YdOmTcZtHjYeTHl1Cj91+IkhVYfgYuVCSEIIvyq/UmluJQBuxtx8qtetVq0ae/bs4fbt2wQHB1OuXDnef/993n//fdzd3Z+qbiGEEEIIIYQojCTALgKGDx/O8ePH2b17t8l2D1sPhlUfxq6Ou+hfrD+JwYmgNewbe2gsg/cM5tCtQ+gV/WO/5rJly/jyyy/p2LEj8+fP5/z589y4cYMOHTrQp08fGjVqxGeffZYHZyeEEEIIIYQQhYMMES8C2rdvT6tWrejWrRsLFy6kW7dufP/994wYMQIAFxcXEhIS8C/vz5fff8mrvIpapebEvROcuHcCPwc/elbsScdyHbG3tM/h1QyqVavGgQMHAEhJSaFu3bps3LiR6dOn4+/vz6pVq2jevDm7d+8mMDDwmZ27EEIIIYQQQhQU0oNdBGg0GjZv3kyrVq0YMGAA5cuX5+DBg3Tr1g0vLy9u377NSy+9xM+7fqaeXz0Avmr2FQOqDMDB0oFbsbeYcWoGzTY1Y9qJaQTHBD/W60+fPp1OnTrh7+/PmTNnqFOnDmq1mjp16nDmzJlnccpCCCGEEEIIUeBIgF1E2NrasnnzZv766y9atWrFv//+y9mzZ2nQoAG//fYbe/fuxcbGhqSkJACcLZwZXnU4e97cw8S6EynjVIaE1ATWXVxHu63tGL5vOAeDDhrLJycnk5SUhKIoJq97+fJlduzYwfvvvw9AmTJl2LdvH8nJyRw+fJiyZcs+3zdCCCGEEEIIIfKJDBEvYl566SWWLl1qdp+NjY3x5zp16gBw8OBBujbpileoF6/3ep0BOwZw+PZh9p/ez+IWi43lvby8ALh+/TqlSpUybh82bBjz589HqzVM7p4wYQLdunWjWLFidOjQgY4dO+bxGQohhBBCCCFEwSQB9gvk0d7njBo1akRCfAIAwTHBrL+4nq1rtpKQatjmYOlAZ//OWLpbmhy3f/9+k+e+vr4cO3Ysj1suhBBCCCGEEAWfDBEvRFJTU9m2bRtt27alVq1aAIwYMYJTp07l6euUdCzJB7U/YH+X/Yx/ZTx+Dn7EpsSy4vwKWm1pxZiDY/gj5I9sA3YhhBBCCCGEeNFIgF1IBAcH89JLL/HGG28QHh5uzMz966+/Urt2bbp162acL51X7C3t6V25Nzs77mRB0wXU8a6DXtGz7+Y+BuweQNcfu7Lt6jaS05Lz9HWFEEIIIYQQojCSALsQiIiIoFmzZsTHx3PixAmOHz/OlClTADhz5gxr1qxhx44d9OrV65n0KmvUGpr4NeHblt+ytf1WOpfvjLXGmouRF5l4bCItN7dkwd8LuJ9wP89fWwghhBBCCCEKCwmwC4F58+YRGhrK/v37qV27tsk+jUZDr169WL16NVu2bOHXX399pm0p51KOSfUmsa/LPt59+V287LyITIpk6T9LCdwcyPjD4zkbdvaZtkEIIYQQQgghCiIJsAs4nU7HN998Q58+fShdurTpzgy91Z07d6ZixYosXLjwubTLycqJQdUG8fObPzO78Wxe9nyZVCWVXdd30XNXT3rt6sWuoF3o9Lrn0h4hhBBCCCGEyG8SYBdw165dIyQkhC5dujzceO8MmhWBNLgyxbhJpVLRpUsXjh49+lzbZ6G2oGWplqxqtYqNbTfSvmx7tGot/4T9w/gj43l98+ss/WcpkUmRz7VdQgghhBBCCPG8SYBdwKWkpABga2v7cKOVI+q7f+KScA1SHyYYs7W1NZbPD5XdKjOlwRT2dN7D8IDhuNu4cz/xPgv+XkCLTS345NgnXIq8lG/tE0IIIYQQQohnSQLsAs7HxweNRsOff/75cKNLKRRrZ9RKGoT9a9z8xx9/ULJkyXxopSl3G3eG1RjGnk57mNpgKlXcqpCiT2Hr1a103tmZAb8MYH/wftL0afndVCGEEEIIIYTIMxJgF3Bubm60b9+eRYsWodfrDRtVKhTvAMOP984AcOfOHbZt28bAgQPN1nPv3j3at29P8eLFUalUnD59OtvXPXfuHIGBgbi7u6NSqYiKijLZf+jQIcqWLYunpycLFiww2deqVSv279+PVqOlXdl2rG+znu9afcfrpV5Ho9LwR+gfvHvoXVpvac3KcyuJTo5+7PfledLpdIwcORIXFxdcXV155513SE1NzbL8jh07CAgIwM7OjuLFi7NkyRLjvvfeew9XV1dq1KjBhQsXjNuDgoIICAjI86XWhBBCCCGEEM+PBNiFwNixY7lw4QLvvvuuMchWvGsAoL53mqioKLp06YKrqyt9+vQxW4dareb1119n27ZtuXpNrVZL165dWblypdn9I0aM4KuvvuKvv/7i008/JTQ0FID169fj6elJs2bNjGVVKhUBngHMajyLXzr9wuBqg3G2cuZu/F1m/zmbFptbMPn4ZIKig3L5jjxfkydP5ujRo1y4cIHz589z5MgRpk6darbsL7/8wvDhw5k7dy4xMTGcP3+eJk2aAHDq1Cm2bdvGjRs3GDRoEOPHjzceN3z4cObMmYO1tfXzOCUhhBBCCCHEMyABdiHQoEEDFi9ezFdffUXdunVZvXo1oeriANw/s4dq1arx77//8uOPP+Lk5GS2jmLFijF8+PBMy3xlpUKFCgwaNIiqVaua3R8UFETTpk3x9fXF39+f4OBgHjx4wOTJk5k9e3aW9XrZeTH65dHs7byXT+t9SjnnciSmJrLx0kY6bOvA0L1DOXz7MHpFn6t2Pg/Lly/n448/xtvbG29vbz766COWLVtmtuzEiRP55JNPaNKkCRqNBhcXFypWrAgY3rNatWrh6OhIy5YtuXbtGgDr1q3Dy8uLpk2bPrdzEkIIIYQQQuQ9CbALiaFDh7J3715cXFzo168fr/UeC4BraiitWjTl5MmTuQ6e80K1atXYs2cPt2/fJjg4mHLlyvH+++/z/vvv4+7unuPx1hbWdCrfiS3tt7Cs5TJe83sNFSp+u/sbI/aPoMO2Dqz7dx3xuvjncDZZe/DgAbdv3yYgIMC4LSAggJs3bxIdbTq0PT4+nj///JM7d+5Qvnx5vLy86NKlC/fu3QOgatWq/PHHH0RFRbFv3z6qVavGgwcPmDp1arY3JYQQQgghhBCFgwTYhUizZs3YvXs3N27cYNGaHSRr7LHUwNL/G4W/v/9zbcuyZcv48ssv6dixI/Pnz+f8+fPcuHGDDh060KdPHxo1asRnn32WYz0qlYra3rWZ33Q+P735E30q98Fea8+NmBtMOzmN5puaM+PkDG7F3noOZ5VZXFwcAM7OzsZt6T/HxsaalH3w4AGKorBt2zb27t3L1atXsbKyonfv3gBUqVKF0aNH06RJE3bv3s0XX3zBe++9x/jx47lw4QJNmzalWbNmz32pNSGEEEIIIUTesMjvBojHV7JkSYoXL07kmdIUiz0Ld0+DT02TMmvXrmXo0KHG8ufPn8/TNlSrVo0DBw4AhqXE6taty8aNG5k+fTr+/v6sWrWK5s2bs3v3bgIDA3NVp5+DH++/8j4jA0ay/dp21v27jhsxN1jz7xrW/ruWxn6N6V2pN7W9aqNSqfL0fLJib28PQHR0tLFnPr3n2sHBwWzZUaNGGbO5f/bZZ/j7+xMfH4+dnR0jR45k5MiRABw+fJibN2/Sq1cvSpYsya+//oqiKDRt2pQbN248t3MUQgghhBBC5A3pwS7EomxLG364+3emfb169SIuLo64uLg8D64fNX36dDp16oS/vz9nzpyhTp06qNVq6tSpw5kzZx67PlutLT0q9mB7x+0saraI+sXro6Bw6NYhBu8ZzJs73mTz5c0kpT77jNsuLi74+vqaZF0/ffo0fn5+mea7Ozs7U6JECbP1KIpi8jwlJYV3332XRYsWERYWRmpqKmXKlKFs2bKkpKQQFhaW5+cihBBCCCGEeLYkwC7Eom1LGX64ezpX5ZOSkozLQKWkpJCUlPRw6a9HKIpCUlISycnJACQnJ5OUlJQpULx8+TI7duzg/fffB6BMmTLs27eP5ORkDh8+TNmyZR//xP6jVqlp6NuQJS2WsL3jdrpV6IaNhQ1Xo67y2e+f0Xxzc+b+OZeQ+JAnfo3cGDBgAFOmTCEkJISQkBCmTp3K4MGDzZZ96623WLBgAXfu3CExMZHPP/+cZs2aGXu3002bNo0uXbpQrlw53N3dSU5O5syZM/zzzz+kpKTg5ub2TM9JCCGEEEIIkfckwC7EHqT3YIf9C7rEHMvb2NhgY2MDQJ06dbCxseHw4cMAHDlyxCQIDA4OxsbGxpgB28vLCxsbG4KDg03qHDZsGPPnz0er1QIwYcIEfvvtN4oVK0a5cuXo2LHj054mAGWcyvBx3Y/Z12Uf42qNw8feh+jkaJadW8brP7zOuF/H8ff9vzPdAMgLEydOpF69elSqVIlKlSpRv359PvzwQwDefvtt3n77bWPZDz74gGbNmlGjRg38/PxISEjgu+++M6nv0qVL7Ny5k3HjxgGg0WhYvHgxrVq1olWrVnz99ddoNJo8Pw8hhBBCCCHEs6VSnkVEUsjFxMTg5OREdHQ0jo6O+d0cE0lJSWzbto1r165RtUoVWp1/B0tdDAzeD7618rt5z02aPo1Dtw6x9uJaToWcMm6v7FaZ3pV6E1gqEEuNZZbH3717l61btxIeHo6joyNt27Z97oniigqdTseuXbto3bq18UaLEPlFrkdRUMi1KAoKuRZFQVIYrsenjQWlB7uQ0Ov1TJkyBV9fX3r06MHSpUtBpeLgxSgAzvy8Ml/b97xp1BqalWzG8sDlbG63mTfKvYGl2pILERf48OiHtNzckkWnFxGeGG5yXHh4ON27d6dEiRKMHTuWxYsX8+GHH1K+fHkCAwONa1MLIYQQQgghxOOSALsQUBSFwYMHM3HiRHr16sXly5e5evUqABWb9gDgrx+Xs2rVqvxsZr6p4FqBz+t/zt4ue3nnpXfwtPEkIimCxWcW02JzCz488iHnI84TFhZGgwYN2L9/P5MnT+bSpUtcuHCBq1evsnDhQq5cuUK9evW4fPlyfp+SEEIIIYQQohCSALsQ2LBhAytWrGDVqlXMmzfPZCizzyttAGhR1Z3Bgwdz8+bN/GpmvnO1duWt6m/xS+dfmNloJtU9qpOqT2Vn0E66/9idtmvaEu8bz4+7fuStt94yDvmwsbGhe/fu7N69G2dnZ3r37v1M5nILIYQQQgghijYJsAuBBQsW0Lx5c/r06ZNpn+L9EgA+lnG4OtgYho6/4LRqLa1Kt2Jt67Wsa72O1qVbo1FpSHBNwLmPMxOCJrD2ylqiU6JNjnNzc+Pzzz/n1KlTnDp1KovahRBCCCGEEMI8CbALuNu3b/P777+bLAt1PyaJVb8Hc+CuChy8wL4YKiWNsb1asnHjxnxsbcFTzaMaMxrNoGdcT8J3huOkdSIsKYylF5fSZW8XZp6Zyc7gnRwLOca/Uf9S7dVqFPcrLu+jEEIIIYQQ4rFZ5HcDRPbCww1JukqXLm3cdj82mcm7LmFnoQaVCrwD4MpuanqpmB4enkVNL7ak8CT0R/RsWryJA3cPsDloM1djrvLTzZ/46eZPJmVd/8+V/Sn7eWP7G7jZuOFu4467tbvhX1t3k+dOVk6oVKp8OishhBBCCCFEQSIBdgGXPk84NDTUuK2shz0qFcSnqoiIT8Gr+EtwZTeO8UEFblmxgsLR0ZGYmBj0KXpa+bXidd/X+SfyH/bf2c/9pPtEJkUSmWx4pClppFmmcTXqKlejrmZbr4XaAjdrNzxsPHC3cX8YkJt5WFtYP6ezFUIIIYQQQuQHCbALuNKlS1OlShVWrFhBu3btALCx1ODrbMOtB4lcvR9nCLABx7hrdOjQNz+bW2C1bduWMWPGsG3bNnr06IFKpaKGWw1quNUwKXfst2O80f0NVmxaQbka5QhPDCciMYLwxHDCk8IJTwg3/hydHE2qPpXQhFBCE0KzeOWH7LX2mYLujAG5h40HbjZuuFi5oFFrntVbIYQQQgghhHhGJMAu4FQqFSNGjGDkyJHs37+fZs2aAVDO084YYDeoEQCAv7Oe4V36519jC7By5crx+uuv88UXX9CyZUvc3NwylUlMTOT/Pv8/yvqUpVdgL9Tq7FMUpKSlEJkUSXhiOGEJYYYAPGNAnuGRnJZMnC6OOF0cN2JuZFuvWqXG1drVGICn944bA/L/hqd72Hpga2ErQ9SFEEIIIYQoICTALgQGDx7M9u3badeuHVOnTmXAgAH4e9pz8FI4fwWFcHLdWj6y1uPjqKaiU0p+N7fAWrhwIfXq1aNNmzZ89tlnNG/eHI1Gg6Io/P7773z++eecO3eOAwcO5BhcA1hqLPGy88LLzivbcoqiEKeLyxR0pz/SA/KwxDAeJD1Ar+iN+3JiY2GDm3XWw9LTg3I3Gze0am2u3yshhBBCCCHE45MAuxDQarVs27aNkSNHMm7cOD7++GPKB/YB/7Zs3nOMhB/XMe69SqC7BHf/hhJ187vJBVKZMmU4evQovXv3pmfPnnh7e+Pn58f9+/e5ceMGFSpU4MCBA9Stm7fvn0qlwsHSAQdLB0o7lc62bKo+lQdJD7IMxsMTw4lIMgTk8bp4ElMTuR13m9txt3Nsh4uVS7ZzxNMfjpaO0isuhBBCCCHEE5AAu5Cwtrbm22+/5fPPP2fVqlWcufWA44BzyYpcuXMHu1NfwaGpcPd0fje1QPP39+fEiROcOnWKjRs3Eh4eTp06dWjfvj3NmjXL98DSQm2Bh60HHrYeOZZN0CUYer+THgbfYQlhxgA8/RGZGEmqksqD5Ac8SH6QY+I2rVqb7RzxjMG4lcYqr05dCCGEEEKIQk8C7EKmePHiTJgwgej4RGr83wES0jQkKRbY/ZfojLt/528DC4lXXnmFV155Jb+b8VRstbbYam3xc/TLtpxe0ROVHJVpSHr6sPSMz2NSYtDpddyLv8e9+Hs5tsHB0gE3azdUCSqOHDuCh62HcX64u/XDAN3F2gW1Kudh90IIIYQQQhRmEmAXUraWFrhZKUQkq7hyPw634gGGHeGXITkWrBxyVY9Op2PMmDGsXbsWlUpFr169+PLLL7GwyHxp9O/fn3Xr1mFpaWnctnfvXurVqwfA3LlzmTp1Ko6OjixfvpxGjRoBEBUVRf369Tl06BAeHjn3zIq8lZ40zdXalfIu5bMtm5KWQkRiBGGJYWbniGcMxlP0KcSmxBKbEgvA9eDrWdarUWmMidvMZVHPmMjNVmubp+cvhBBCCCHE8yIBdiHmZftfgB0aS90ypcDRB2LuQMhZKPlqruqYPHkyR48e5cKFCwC0atWKqVOn8sknn5gtP3z4cObOnZtpe0hICJMnT+bs2bOcOnWKESNGcPbsWQDGjx/PuHHjJLguBCw1lnjbe+Nt751tOUVRiNXFEp4YTkhsCPt+34dfRT8epDwwBOj/ZVWPSIwgMsmwtnhYYhhhiWE5tsHGwibLIekZH67Wrlio5StMCCGEEEIUHPLXaSHmZQPnH8CV+3GGDcVfMgTYd//OdYC9fPlyvvzyS7y9DQHVRx99xLhx47IMsLMSHByMv78/3t7etGzZku7duwNw7Ngxrly5wtdff/1Y9YmCTaVS4WjpiKOlI362foRZhtG6Ymu02syZynV6XbaJ2zL2mCemJpKYmsit2Fvcir2VfRtQ4WLtYly6zMPWw2QZM+PD1h0HrUO+z68XQgghhBBFnwTYhZiXrQLA5VDDEF28A+Dij7meh/3gwQNu375NQECAcVtAQAA3b94kOjoaJyenTMesXr2a1atX4+3tzcCBAxkzZgxqtRp/f3+uX7/O7du3+fvvv6lWrRo6nY5Ro0axfv36pz1VUYhp1Vo8bT3xtPXMsWyCLsEk+H50WLoxcdt/veKRSZFEJkVyhSvZ1muptjQZlp4+JN1cD7mlxjLbuoQQQgghhMhKvgfYCxcuZNasWYSEhFCjRg0WLFhA7dq1sywfFRXFRx99xJYtW4iMjKRkyZLMnTuX1q1bA/Dpp5/y2WefmRxToUIFLl68+EzPIz942RgC7KsZe7Ah15nE4+IMxzk7Oxu3pf8cGxubKcAeNWoUs2bNwtXVlVOnTtG1a1fUajVjxozB1dWVBQsW0LFjRxwdHfn222+ZMWMGHTt2RKfT0apVKxITExk9ejRvvPHGE5+zKNpstbaU0JaghGOJbMul6dOMidvSM6mHJYSZPE8PxmNTYknRp3A3/i534+/m2AZHS0ezwfijAbmzlbMkbhNCCCGEECbyNcDeuHEjY8eOZcmSJdSpU4e5c+cSGBjIpUuX8PTM3NuVkpJCixYt8PT0ZPPmzfj4+BAcHGwSIAJUqVKFffv2GZ+bS9hVFBSzMfwbHpdCZHwKrumJziKuQFIMWDtme7y9vT0A0dHRuLu7G38GcHDInCTt5ZdfNv5ct25dPvjgA1avXs2YMWMA6NKlC126dAHgypUrbNmyhePHj9OoUSNmzpxJtWrVqF69Ok2aNMHFxeWJz1sIjVqDm40bbjZuOZZNSk0yWbosY9K2R7Oq6/Q6YlJiiEmJISg6KNt6LVQWuNqYJm5zs3YzZlJ3t3E3ZlKXxG1CCCGEEC+GfI0858yZw5AhQxgwYAAAS5Ys4aeffmL58uV88MEHmcovX76cyMhIfvvtN+Ncz1KlSmUqZ2FhgZeX1zNte0FgpQFfZ2tuRyVxJTSWOmXcwckPom9ByD9QqkG2x7u4uODr68vp06cpW7YsAKdPn8bPz8/s8PBHqdVZ994NGzaM+fPnY2lpyZkzZ6hTpw5WVlb4+vpy5cqVbEcpCJGXrC2s8bH3wcfeJ9tyiqIQkxKT7Rzx9OcPkh+QqqRyP+E+9xPu59gGWwtbwxxx66yTtqUvZyaJ24QQQgghCq98+0suJSWFP//8kwkTJhi3qdVqmjdvzu+//272mB07dlCvXj1GjBjB9u3b8fDwoGfPnowfPx6NRmMsd+XKFYoXL461tTX16tVj2rRplCiR9ZDT5ORkkpOTjc9jYmIAwxJWOp3uaU/1mUhvV1kPO25HJfHvvWhe9nNE41UDdfQt0m79gd6nTo719O3bl8mTJxsD3ilTpjBgwACz571p0yYCAwNxcHDgr7/+Yvr06bz99tuZyq5evZrSpUtTp04ddDodpUuX5ueff+all14y/m4K6vsqHl/677Io/E5t1baUsCtBCbvsh6ibJG77L1u6sZf8kedJaUkkpCYQHBNMcExwtvWmJ24zriH+379u1g+Tt6U/t9faS+I2M4rS9SgKN7kWRUEh16IoSArD9fi0bVMpiqLkUVsey927d/Hx8eG3334zrqMM8P777/Prr79y4sSJTMdUrFiRGzdu0KtXL4YPH87Vq1cZPnw4o0aNYtKkSQD8/PPPxMXFUaFCBe7du8dnn33GnTt3OHfunNlhz2B+3jbAunXrsLUt2EM7twerOXBXTUMvPZ1L6/EP2UHle5u57VyXP0sPz/H41NRUli1bxuHDhwFo3LgxgwYNQqPRsHjxYsDQGw3w4YcfcuPGDfR6Pa6urjRv3pyOHTua9GTHxMTw8ccfM3XqVOMQ9LNnz/LVV1+RlJREly5daNu2bV6/DUIUSIqikEIKsfpYYpVY4vRxxClxxOpjiVPiiNPHEavEEquPJV6JRyH3X8cWWGCvtsdB5YCD2gF7lb3xecZ/7VX2WKikV1wIIYQQIjcSEhLo2bMn0dHRODpmP+XWnEIVYJcvX56kpCSuX79u7LGeM2cOs2bN4t69e2ZfJyoqipIlSzJnzhwGDRpktoy5Hmw/Pz/Cw8Of6E19HnQ6HXv37iXesyofbr9I3dIufDfwFVRBB7FY34VYS092lZ1MxYoVqVy5cn43VxRh6ddiixYtzC7TJXInPXFbRJJhSHpEUoQxYVvGHvGIpAjidHGPVbeTpdPDnvAMc8Mffe5k6VToe8XlehQFhVyLoqCQa1EUJIXheoyJicHd3f2JA+x869Zwd3dHo9EQGhpqsj00NDTL+dPe3t5otVqT4eCVKlUiJCSElJQULC0zL6/j7OxM+fLluXr1apZtsbKywsrKKtN2rVZbYH/x6Sp4GeZKXw1L4Pjx48yZPIWt9cAh5T5D+3UnOhnq1avHxx9/bMy0LsSzUBg+LwWZFi1eVl54OeacPyIxNdGYnM3cHPGMS5ylKqlEp0QTnRKdc+I2tUWO88TTM6nbWNjk1ak/E3I9ioJCrkVRUMi1KAqSgnw9Pm278i3AtrS0pGbNmuzfv5+OHTsCoNfr2b9/PyNHjjR7TP369Vm3bh16vd44LPny5ct4e3ubDa7BsBTVtWvX6NOnzzM5j/xW1sMOgPC4ZJq37krV8qWJ1bjikBbJpYMb2XsthSVLltC2bVsWL17M0KFD87nFQoinZWNhg6+DL74OvtmW0yt6YpJjjHPDwxPDCU8IN3meHpBHJUeRqk8lNCGU0ITQbOsFsNfamwTcHjYeZtcVd7FyQaPW5FifEEIIIURRkK8T88aOHUu/fv2oVasWtWvXZu7cucTHxxuzivft2xcfHx+mTZsGGOYCf/XVV4wePZp33nmHK1euMHXqVEaNGmWsc9y4cbRr146SJUty9+5dJk2ahEajoUePHvlyjs+anZUFxey1hMbpaNy+O2vnfobV7lFwdRcO8UG0aTOUVq1aMWHCBIYPH06tWrWoWbNmfjdbCPEcqFVqnK2dcbZ2phzlsi2bkpZCZFJkpizq5h7JacnE6eKI08VxI+ZGjm1wtXZ9GIz/l6zNw9bD5Lm7jTt2WrtCP0RdCCGEEC+2fA2wu3XrRlhYGJ988gkhISEEBATwyy+/UKxYMQBu3rxpkkDLz8+P3bt3M2bMGKpXr46Pjw+jR49m/PjxxjK3b9+mR48eRERE4OHhQYMGDTh+/DgeHh7P/fyeF1VsCKjcaN9nKBYWFqQWq4bl1V1YhP5DMobs7FOnTmXPnj0sWLCAlStX5neThRAFjKXGEi87L7zssh+irigKcbq4TEPSMw5LT38emRSJXtEbn+fExsLGZIh6xt7xjNvcrN3QagrmsDIhhBBCvNjyPbXsyJEjsxwSfujQoUzb6tWrx/Hjx7Osb8OGDXnVtELj1rmTaKu14nZsKgBpntUA0Nw/Zyyj0Wjo168fM2bMYMmSJVhbW+dLW4UQhZtKpcLB0gEHSwdKO5XOtmyqPvXhcmbpAfl/ydrCEsJMnsfr4klMTeR23G1ux93OsR3OVs5m54g7a525prtGUHQQXg5eOFo6Sq+4EEIIIZ6bfA+wxdNJTU0l+uZF3Ku1Iig8EcgQYEcHo736C7pyrwNQuXJlkpOTiYyMpHjx4vnWZiHEi8FCbYGHrQcetjmPIErQJTxcSzxjQJ4haVt4YjiRiZGkKqlEJUcRlRzF1SjzCSxX/LTC2Ib0TOnuthmC8fSh6f9tc7N2w9pCbjwKIYQQ4ulIgF3IaTQalGjDEmVB4QkAKNZOJFftgdW59dj9/A7xbZagK9OM6OhoAGxsCnb2XyHEi8dWa4ut1hY/B79sy+kVPdHJ0WaHpKf3jN8Iu0GyRTIxKTGk6lMJiQ8hJD4EIrJvg4PWIcekbe427rhYu6BWqbOvTAghhBAvJAmwCzmVSsWrVctwBQiP1xGdqMPJRkvCa5NRpcRheXkndruGEdfuG3744QdeeuklXFxc8rvZQgjxRNQqNS7WLrhYu+Dv4p9pv06nY9euXbRu3RpFrWQ7Rzx9jfGwhDBS9CnE6mKJ1cXmmLhNo9KYJG5LnyP+aEDuYeOBrdb2Gb0TQgghhCiIJMAuAkYNe4thu+5j4eRJUEQiL/lqQa0hPvBL0KdiefVnbLcPJu1qNMMnLM7v5gohxHNhqbHE294bb3vvbMspikKsLtZ0SHpCmDEAz9hD/iDpAWlKGmGJYYQlhuXYBhsLG9N1xK3d8LA1Tdrmbu2Oq40rWrUkbhNCCCEKOwmwi4A2bdrgunspMcDanQeoPLAVVlZWoLYguvkcEu7comTiOX7saYemcdn8bq4QQhQoKpUKR0tHHC0dKeNUJtuyOr3OJHHbo3PEMwbkCakJJKYmciv2Frdib2XfBlS4WLtkWrrs0YebjZskbhNCCCEKMAmwiwC1Wk2nFvVZ8ftNth44zs6542nUqBFqtZrffvuNsJA7HH2nFK84RcL3vaDPFihRN7+bLYQQhY5WrcXT1hNPW88cyyboEnJcUzwiMYKIpAjSlDQikyKJTIrkCleyrddSbZlpKbOsgnErjVVenboQQgghckEC7CKiko8zcJNG7bpRsoo1Z86cQVEU2rZty1tvvcXL1SrD+u4QdBDWdIa+28C3Vj63Wgghii5brS0ltCUo4Vgi23J6RU9UchRhCf/NEU/KEIQnhJs8j02JJUWfwt34u9yNv5tjGxwsHbKcI26yvJmVsyRuE0IIIfKABNhFRPliDgDcjdOzbd4884W6r4N1XeHGEfjuTUOQ7fPy82ukEEKITNQqNa7Wrrhau+ZYNjkt2ZCYzcyQ9EcfOr2O2JRYYlNiCYoOyrZejUqDm7WbyVJmbtYPA3IPWw/crQ1BuiRuE0IIIbImAXYRUc7THoD7sclEJ+hwsjWTLMfSFnpuNPRg3/wNvnsD+u0E7+rPubVCCCGehJXGiuL2xSluXzzbcoqiEJMSk+0c8fSs6g+SDYnb7ife537i/RzbYGthm+2w9PQM6i7WLlio5c8MIYQQLxb5n6+IsLeywMfZhjtRiVy+H8srpbLoCbG0g17fG3qwb5+E1R2g/09QrPLzbbAQQohnRqVS4WTlhJOVE2Wcc07cFpkYabJsmXGOeJJpD3liaiIJqQncjL3Jzdib2bfhv8RtWQXjGZc5s9faS+I2IYQQRYIE2EVIOU977kQlciU0LusAG8DKAXpvhtUd4e5fsLq9Icj2qPDc2iqEEKJg0Kq1FLMrRjG7YtmWUxSFhNTsE7el95BHJEWgV/TGxG2XH1zOtm4rjZXJsmXuNu4Ph6s/klVdq5HlzIQQQhRcEmAXIeWL2fPr5TAuh8bmXNjayZBNfHUHuHcGVrWD/rs4dimU4cOHc+XKFcqXL8/ixYupV69eltUcO3Ysy/J37tyha9eunD9/ng4dOrBixQrUakMSnenTpxMfH8///d//5cm5CyGEeLZUKhV2WjvstHaUdCyZbdk0fRoPkh+YHZL+aEAeq4slOS2ZO3F3uBN3J8d2OFk5GYPujEPSH03i5mTlJInbhBBCPHcSYBch/p6GRGdX78fl7gAbF+izzRBch54jcnFr2s4OZeasL+jbty+rV6+mbdu2XLt2DWdn50yHR0ZG0rZtW2bOnGm2/NSpU2nYsCH79++nadOmbN26lU6dOhEUFMSGDRs4ceJE3p28EEKIAkOj1hgD3QpkPzoqMTXxYc93egCeFP4wq3riw0zqqfpUopOjiU6O5lr0tWzrtVBbmCRqyzgk/dE54zYWNnl5+kIIIV5gEmAXIf7FDInOctWDnc7WFfpuh5Vt2Lr7H3xsFIZ0bglWVgwZMoS5c+eydetWBgwYkOnQrVu34uPjw5AhQwAylQ8KCuLdd9/F2tqaRo0ace2a4Y+hYcOG8eWXX2JlJeuzCiHEi87GwgZfB198HXyzLZeeuC0sIcwYcGeVxC0qOYpUfSqhCaGEJoTm2AY7rZ3ZXnA3azdDBvX/nrtYuaBRa/Lq1IUQQhRBEmAXIbnKJG6OnTv03cE/m6oT4PHA0KM9YBc4+RIQEMA///xj9rB//vmHgIAAk20Zy1erVo19+/bRqFEjjhw5wscff8zatWspXrw4r7322hOfpxBCiBdPxsRt5SiXbVldmi5TgraMw9LTA/LwxHCS05KJ18UTr4vnRsyNbOtVq9S4WLngYethOl/czMMSyzw8eyGEEIWFBNhFiIO1luJO1tyNTuLK/VhqZZfoLNPBxYjza4qzfh9EBf83J/snnJ2diY013yMeFxeXaeh4xvITJkxg5MiR1KlThw4dOlCnTh0aNWrEr7/+yqRJkzh48CClSpXiq6++wtHR8UlPWwghhDCh1WjxsvPCy84r23KKohCvi886cVtSOOEJhp8jkyLRK3oikiKISIrIsQ3WGmtsFBu+3/M9nraemXrHM/aSS+I2IYQoOiTALmLKFXP4L8COyzHAXrt2LUOHDgWgZMmSNG/enMiURuB8BSKDYFV7osNL4VHcz+zx9vb2REZGmmyLjo7Gw8MDABcXF9auXWvcN2jQID744ANOnTrFsWPHOHToEJ9//jnTpk1j2rRpT3PaQgghxGNTqVTYW9pjb2lPKadS2ZZN1acSlRz1MGlbQliWveTxuniS0pJIIokH4Q9ybIezlXO2c8Tdrd3xsPXA0dJRljMTQogCTgLsIqa8pz2Hc5lJvFevXvTq1cv4fNmyZcw9cACW/Agr20DEFU4fucDYieaD3+rVqzN37lyTbadPn2bs2LGZyv7666/cvn2b3r17M2PGDF555RXUajX16tVj3rx5j3eSQgghxHNmobYwBr45SdAlEBoXyo8HfsT/JX8epDwwTeKWYbh6qmII3KOSo7gadTV3bciQRd3D1iNTVnV3G3esLazz6tSFEEI8Bgmwi5j0RGe5ziSewRtvvMG4ceNYtmUffXps5rtRr3EvKpQ3EtZBQh9DQjRz5Zcto0+fPnz33Xfcu3ePN954w6RccnIyY8aMYfPmzQCUKVOGuXPnkpyczL59+yhbtuwTnq0QQghR8NhqbfG196WERQma+TVDqzU/BFyv6IlOjs5yjnjGLOrRydGk6lMJiQ8hJD4kxzY4aB2yHJaesYdcErcJIUTekgC7iPEvZliq67Eyif/H1dWVnTt3Mnz4cEaOHEn5sqXYOdACl9iLsLoDN5suovLL9bhw4QIlSpTIXL58eXbu3ImLi4tJvdOmTaNr166UKVMGgDfffJPt27fj6elJtWrV+P7775/+xIUQQohCRq1S42Ltgou1C/4u/tmWTUlLMe0B/y+Tevoc8fCk/4LzhDBS9CnE6mKJ1cXmmLhNo9Lgau2aqQfc3MPWwlaGqAshRA4kwC5i0jOJh8Yks3HrTmpWq0S5ctlnW82oQYMGplnDwy4ZhouH/EOJQ6OIC78D1k5Zlzfj008/NXmu0WhYs2ZNrtskhBBCvOgsNZZ423vjbe+dbTlFUYjTxRGWGJZpSPqjjwdJD0hT0ghLDCMsMSzHNthY2GRK0Jb+c8bM6q42rmjVkrhNCPFikgC7CDl58iSff/45qSU6Y+HoQb933if5zkWaNm3KRx99RNOmTR+/Uo8K0HeHIci++xes6Qx9toCVQ96fgBBCCCGeikqlwsHSAQdLB8o4lcm2bKo+lQdJDzIPSTfzSEhNIDE1kVuxt7gVeyvHdrhYuWSZtC19m5uNmyRuE0IUORJgFxE7d+6kc+fO+Pv7U66WPTeSYObXa3C4/w+LFi2iRYsWLF++nH79+j1+5cUqQ9/thqW7bp+EtV2h92awtMv7ExFCCCHEc2GhtsDD1gMPW48cyyboEgwBeJIhg7pxzvh/mdTDEgw95hFJEaQpaTxIfsCD5Ac5Jm7TqrXZzhFPD8jdbNyw0ljl1akLIcQzIwF2ERAcHEy3bt0IDAxk1apVfHnoJjdO3CYsxYJBXbvSqVMnRo0axaBBg6hRowYBAQGP/yLe1aHvNljVAW7+Buu6Qc/vwdI2r09HCCGEEAWMrdYWW60tfo7ml+5Mp1f0JsuZPfpI7yUPSwwjNiUWnV7Hvfh73Iu/l2MbHCwdsp4jbv0wq7qzlTNqlTqvTl0IIR6LBNhFwJIlS7C0tOSbb77B0tKSsh6GoPdaWDxgmPM8b9489u/fz/z581m+fPmTvVDxlwzDw1d3hBtHYENP6LEBtLIUiBBCCCEMidtcrV1xtXalvEv5bMsmpyVnOyw9YzCu0+uITYklNiWW69HXs61Xo9LgZu32sAfc1sNkvrhJ4jatdBQIIfKWBNhFwMqVK+nRowf29oYEZ+U8DEO3r4bFoygKKpUKCwsLBg4cyKxZs1i8eDFWVk84zMq3lmF4+HdvQtBB+L4PdFsDFjJsSwghhBC5Z6Wxorh9cYrbF8+2nKIoxKTEZJ+07b8s6pFJkaQpadxPvM/9xPs5tsHWwjbbOeLp+1ytXbFQy5/NQoicyTdFIZeamkpISAjVq1c3bivrbgiw78em0Hz+cZpWcKdZBXcqV6lGYmIiERERFC+e/X9m2SpRF3p9b0h4dmUPbOoPXVaBheVTno0QQgghhCmVSoWTlRNOVk6Ucc4+cZtOryMyMdIYcKcH4GEJYcb54umPxNREElITuBl7k5uxN7NvAypcrF0yzRE3l8TNQesgiduEeIFJgF3IaTQaLCwsiI19uO61g7UFg171Y92pO4TEJLPu1B3WnbqDtdoKt9Zj+O1mPG3c07Cx1Dz5C5dqAD03GOZiX9oFPwyCzstBI8tyCCGEECJ/aNVaitkVo5hdsRzLJugSjBnUzQ1LT/85IikCvaInMimSyKRILj+4nG29VhqrhwG3dfYBuaVGOieEKGokwC7kVCoVTZo0YcuWLYwYMcK4fWyzsoxoXIrfgx5w4FI4By9HEJmgw75aM8Ztu8zHP12lob8HgVW8aFbRExe7J/iCL9MEuq2FDT3g3x2wdSi8sRQ0clkJIYQQomCz1dpSUluSko4lsy2Xpk/LMnFbRGKEyTJnsbpYktOSuRN3hztxd3Jsg6Olo0mm9KySuDlZOUniNiEKCYmEioDhw4fz5ptvsm/fPpo3b27cbmWhoUl5d5qUd+fUH3/Qqs//aPv2R9zTeHL7QSJ7L4Sy90IoGrWK2qVcCaxSjBZVvPBxtsn9i/s3h67fwcbecO4HUGuh4yJQP0XvuBBCCCFEAaFRa3CzMSRNq0CFbMsmpSY9HIqe8HB+uDEoT3j4PFWfSkxKDDEpMQRFB2Vbr4XKAlcb1yzniGcM0G0sHuPvOCFEnpMAuwho3749r7/+Or169WL+/Pl06tQJCwvDr1av1/PLL7/w9ttvE1C+PN+N7YiVlRX/3otl9/kQ9lwI5d97MfweFMHvQRF8uvMC1XycCKxSjJZVvPD3tM95HlGF16HLStjUD/7ZYOjBbrcA1HKnVQghhBAvDmsLa3zsffCx98m2XHriNuMc8QxD0h99RCVHkaqkcj/hPvcTck7cZqe1MwTg1oZly1wtXQlPCkd3TUcx+2LGzOouVi5opENEiDwnAXYRoNFo2Lx5M3379mXQoEFMmjSJpk2bolarOXLkCNeuXaNp06Zs3rwZa2vDklqViztSubgjY1qU52ZEAnsuhLDnfCingiM5eyeas3ei+WLPZUq729GysiHYfsnPGbU6i2C7Ulvo9C1sHgh/rzH0ZLf9EiTJhxBCCCGEiYyJ28o6l822rC5NR0RShMn88EfnjKc/ktKSiNfFE6+LJzgm2KSevSf2mjxXq9S4WLnkmLTNw8YDO62dJG4Tz03//v1Zt24dlpYPp7Du3buXevXqmS1/584dRowYwZEjR1CpVDRt2pSFCxfi4eEBwNy5c5k6dSqOjo4sX76cRo0aARAVFUX9+vU5dOiQsWxekAC7iLCzs+OHH37gr7/+YvHixZw5cwZFUahfvz6rV6+mXr16WX4xlnCzZXDDMgxuWIbwuGT2/xvK7vOhHL0azvXweL4+HMTXh4PwcLCiReViBFbxol4ZNywtHumhrvIGpKXCliHw5wpDwrNWMyXIFkIIIYR4QlqNFi87L7zsvLItpygK8br4TEuXhcaFcvrqaWzcbIhMjiQ8MZzIpEj0it4QuCdFcOnBpWzrttZYZztH3BiUW7uhlYS3Ig8MHz6cuXPn5qpseh6q4OBgFEWhV69ejBo1ivXr1xMSEsLkyZM5e/Ysp06dYsSIEZw9exaA8ePHM27cuDwNrkEC7CLn5Zdf5ptvvnni493trej2Sgm6vVKCuORUfr0Uxp4LIRz49z5hscmsO3GTdSdu4mBlwWsVPWlZpRhNKnhib/XfpVS9C+h1sG04nFxq6MkOnCJBthBCCCHEM6RSqbC3tMfe0p5STqWM23U6Hbvu7qL1a63Rag3Bb5o+jQfJD7JeVzxD73icLo6ktKRcJ25ztnI2nSOenknd1t3kuZOVk/SKizwRFBTEBx98gL29PQDdunVj2rRpgCHo9vf3x9vbm5YtW9K9e3cAjh07xpUrV/j666/zvD0SYIss2VtZ0Ka6N22qe5OSquf3oAj2/DdvOyw2mR1n7rLjzF0sLdQ0KOdOy8rFaF65GO4BPSFNBztHwfGFhp7s5p8WyiA7OTmZkSNHsm/fPsLDw/Hx8eH9999n4MCBZstPnDiRbdu28e+//zJy5EiTO29paWn079+fnTt3UrVqVb7//nvjeuS//fYbH374IQcPHpT/bIQQQgjxTGnUGmOvc04SUxPNDkc3F5CnKqlEJUcRlRzF1air2dZrobYwCbizStrmbuOOtYV1Xp26KCRWr17N6tWr8fb2ZuDAgYwZMwZ1Fvmdxo4dy6ZNm2jTpg2KorB+/XratWsHgL+/P9evX+f27dv8/fffVKtWDZ1OZ+zhfhYkwBa5YmmhpnF5DxqX9+D/OlTl9O0oQ5K086FcD4/nwMX7HLh4H/XWs9Qq6UrLKg15s8l0XA99AMfmgsYSmn6U36fx2FJTU/H29mbfvn2UKVOGEydO0KpVK3x9fWnZsmWm8uXKlWPmzJlmRxFs2bKFGzduEBoayocffsi0adNYsGABOp2Od955h7Vr10pwLYQQQogCxcbCBj8HP/wc/LItp1f0xCTHmJ8jnmSaVT06OZpUfSoh8SGExIfk2AZ7rX3m4egZ5oinP5fEbUXDqFGjmDVrFq6urpw6dYquXbuiVqsZM2aM2fL169fnm2++wcXFBYB69eoxYcIEAFxdXVmwYAEdO3bE0dGRb7/9lhkzZtCxY0d0Oh2tWrUiMTGR0aNH88Ybb+RJ+yXAFo9NrVbxcgkXXi7hwgevV+Tq/Th2nw9h9/lQzt6J5uSNSE7eiGQyJRjvMoRhid/A4ZkoagtUTcbnd/Mfi52dHZ9//rnxed26dXnttdc4evSo2QC7X79+AGzcuDHTvqCgIBo0aICVlRUtWrRg/vz5AMyaNYt27dpRsWLFZ3QWQgghhBDPllqlxtnaGWdrZ/xd/LMtm5KWQmRSJGEJYSZLmWVM5Jb+c3JaMnG6OOJ0cdyIuZFjG1ytXc3PD38kiZutha10bBRQL7/8svHnunXr8sEHH7B69WqzAbZer6dFixZ07dqVvXsNifw+/fRTWrZsyfHjxwHo0qULXbp0AeDKlSts2bKF48eP06hRI2bOnEm1atWoXr06TZo0MQbpT0MCbPFUVCoV/sUc8C/mwMim/tyNSjQOIz9xPZIZD14jUhPPR9p1qA5NZd+VKByajaNWKVc0WWUkL8CSkpI4efIkPXv2fOxjq1WrxtSpU0lMTGT//v1Uq1aNq1evsmnTJuMXgBBCCCFEUWepscx14rY4XVyO88TDEsN4kPQAvaI37suJjYUNbtZuxmXL0n9+9OFq44pWLYnb8lNWQ8MBIiMjCQ4OZtSoUdja2gLwzjvvMGvWLMLDw3F3N50GMWzYMObPn4+lpSVnzpyhTp06WFlZ4evry5UrV6hdu/ZTt1cCbJGnijvb0L9+afrXL82D+BQOXLzP7vPFmHNVz1j1BprfWcT/LYthmHVHmlfyJLCKF/XLuWOtLfjDeRRFYfDgwfj7+/Pmm28+9vGtW7fmt99+o06dOlStWpWFCxfSrVs35s2bx48//siCBQuwsbFhzpw5VKpU6RmcgRBCCCFE4aFSqXCwdMDB0oHSTqWzLZuqT+VBUg6J25IiCEsIIyE1gcTURG7H3eZ23O0c2+Fi5ZLtHPH0h6Olo/SK54Hvv/+e119/HQcHB/7880+mT59uzBT+KHd3d8qVK8fChQuZNGkSAAsXLsTX1zdTcL1q1SrKli1LgwYNAChTpgx79+7l5Zdf5sqVK5QsWTJP2i8BtnhmXOws6VTTl041fUlMeYmr2zwod2EBE7Vr0CVpWP1HIN//cRtbSw1NKnjQsrIXr1X0xMmm4N0lVBSF4cOHc+nSJfbt25ftnbTsTJ48mcmTJwPw3XffUaJECapWrUr16tU5e/YsZ86cYeDAgfz+++952XwhhBBCiCLNQm2Bh60HHrY5L7mUoEsw9H4nPQy+wxLCiEgyTeQWmRhJqpLKg+QHPEh+kGPiNq1am+0c8YzBuJXGKq9Ovcj56quveOutt0hNTcXHx4fhw4fzv//9z7j/7bffBmDJkiUAbN++nTFjxuDj44Ner+ell15ix44dJnWGh4cza9Ysjh49aty2cOFCBg4cSFxcHJMmTaJYsWJ50n4JsMVzYWOpoVyX/4MDlnBkNp9rV/FKGU+m3q/Hvegkdp0NYdfZECzUKuqVdaNlFS9aVi5GMcf8zxqpKAojRozgxIkT7N+/Hycnp6euMyIighkzZnDkyBEuX76Mn58fLi4u1KtXjzNnzuRBq4UQQgghhDm2Wltstbb4OeacuC0qOcrskPRHM6vHpMSg0+u4F3+Pe/H3cmyDg6XDw4DbOkNAbuth8tzF2gW16sk6dgoavV5PTExMjuUOHz6c7f70wDpd5cqV2b17d7bHuLu7c+7cOZNtTZo0ISgoKMf2PC4JsMXzo1JB04mQlgK/LaDdrVm0bb+As57t2XM+lN3nQ7hyP44jV8I5ciWcidvO8VIJZ1pW9iKwSjHKeNjnS7NHjhzJsWPHOHDgQI6JD3Q6HWlpacZHUlISGo3GuO5kunHjxvHRRx/h4uJCyZIluXz5Mnfu3OHvv/+mbNmyz/J0hBBCCCFELqQnTXO1dqW8S/lsyyanJWdaysxc0rbwxHBS9CnEpsQSmxLL9ejr2darUWlws3bL1AP+aNI2dxt3bLW2eXn6eSYoKIhFixaxfPlykpKSWL9+PQ0aNGDIkCH07t0ba+v871DLSxJgi+dLpYIW/wdpqXBiMaodo6j+hhXVA7sxLrACQWFx7LkQyp7zIfx1M4q//3vM+OUi5TztCaxSjJaVvaju6/Rc5rgEBwezaNEirKysTOZl9O7dmyVLltCqVSsaNmzIhx9+CMCQIUNYtWqVsdxXX31Fv379WLlypXHboUOHCAkJoUePHgB4eXkxceJEAgICcHR0ZMWKFc/8vIQQQgghRN6x0lhR3L44xe2LZ1tOURRidbEPly1LfLh8WUSiYY54+s+RSZGkKWncT7zP/cT7ObbBxsImyyHpJonbrF2xUD+fMHDXrl106tQJjUZDQECA8e/p5ORk3nrrLZYsWcIvv/ySab50YaZSFEXJ70YUNDExMTg5OREdHY2jo2N+N8csnU7Hrl27aN26dabe0UJBUeCn/8Efy0Clhk7fQtVOJkXuxyQZgu0Lofx+LRxd2sNL1dvJmhaVixFYxYvapV3Rah5/6ExISAjh4eE4Ojri5+cnSSmeUKG/FkWRItejKCjkWhQFhVyLhZdOr8sxcVv6IzE1Mdf1qlDhYu1i0gvuZuOGu3WGQNzW8K+D1uGJ/0Y+ffo0devWpXTp0nTp0gVLS0vUajVVqlTh/Pnz3Lp1i7Vr11K1alWOHj1qzHGUnJzMyJEj2bdvH+Hh4fj4+PD+++8zcODAbF8vNDSUSpUqUaJECU6fPg1AWloa/fv3Z+fOnVStWpXvv/+e4sUNN0F+++03PvzwQw4ePGhyjk8bC0oPtsgfKhW0/gL0OvhrNfwwBNQWULmDsYinozW965akd92SRCfqOHTpPnvOh3Lw0n3uRSex+vdgVv8ejJONlmYVPWlZxYvG5T2wscw6I7miKGzevJn58+ebJDmoXr06I0aMYODAgVhYyMdCCCGEEELkL61ai6etJ562njmWTdAlPEzYZmaOuDFx23+94pFJkUQmRXLlwZVs67VUW2a5rvijWdUtNZYmx06bNg1HR0e6du1q9uaOj48PnTp1YuXKlezbt4+WLVsCkJqaire3N/v27aNMmTKcOHGCVq1a4evrayxjzsiRI3nppZeIiIgwbtuyZQs3btwgNDSUDz/8kGnTprFgwQJ0Oh3vvPMOa9euzfNONokkRP5Rq6HtPMNw8TPrYPNA6PodVGydqaiTjZYOAT50CPAhSZfGb9fC2X0ulH3/hhIRn8KWv++w5e87WGvVNPT3ILCKF80qeuJi9/CDrtfrefvtt/nmm2+oXbs2M2fOxNfXl/DwcLZt28awYcPYtm0bW7ZsKXJzQYQQQgghRNFlq7WlhLYEJRxLZFsuTZ9mTNyWnkk9LCHM5Hl4YjjhCeHE6mJJ0adwN/4ud+Pv5tgGR0tHY9Btr7LnqMVRKvSuwB2nO1inWWOtt8ZWsUWv6I3HlClThuLFi7N48WJj8GxnZ8fnn39uLFO3bl1ee+01jh49mmWAvX37diIjI+nTpw9z5841bg8KCqJBgwZYWVnRokUL5s+fD8CsWbNo164dFStWzPG8HpcE2CJ/qdXQ4StDT/bZTfB9X+i+DspnfXfKWquhacViNK1YjDS9wp/BD9h9PoTd50O4/SCRvRdC2XshFI1aRe1SrrSsUoyWVbz4bsk8vvnmG6ZMmZJpHesWLVpw7NgxRowYwTvvvMM333zzrM9cCCGEEEKI50qj1uBmY0ialpOk1CSTpcsiEiOMveOP9pDr9DpiUmKISYkhKNqQmdu1pStRRHGCEyb17o/ZTxvaAIa1zkuXLs0///yTdTuSkjh58iQ9e/Y0uz86OpqxY8fyyy+/cOzYMZN91apVY+rUqSQmJrJ//36qVavG1atX2bRpE8ePH8/xPXgSEmCL/KfWQMclkKaDC9tgY2/ouQHKNs3xUI1aRe3SrtQu7crHbSrx771Y9lwIYff5UP69F8PvQRH8HhTBZzsvkBbuQMO3p/Dya21RFCXTcJD69evz7rvvMnv2bD777DPj/AwhhBBCCCFeNNYW1vjY++Bj75NtOUVRiEmJMRmKfurCKZasWULV2lXBDpLUSSRpkkjWJGOntst1GxRFYfDgwfj7+2fqIEv3/vvv079/f/z9/TMF2K1bt+a3336jTp06VK1alYULF9KtWzfmzZvHjz/+yIIFC7CxsWHOnDlUqlQp1+3KjgTYomDQWBgSnelT4eKPsL4H9NoEpRvlugqVSkXl4o5ULu7Iu83LczMigT0XQthzPpRTNyLQuJfiJvD29lsUd9Dyakk76vrZ4mKjQa1SoVZBk1Yd+erb1SxZsYYx776LWq3CQq1Co1ahVqX/iyREE0IIIYQQAsPfxU5WTjhZOVHW2bDcbG2H2kx5YwpWsVbUq1fvYWE1lK1UlusYlidTFIWgoCDq1KmTqV5FURg+fDiXLl1i3759xiRoGR05coRjx47x119/Zdm+yZMnM3nyZAC+++47SpQoQdWqValevTpnz57lzJkzDBw4kN9///1p3gYjCbBFwaHRQucVhh7sK7thXTfo/QOUfPWJqivhZsvghmUY3LAMH30+nWW/nKRxz3f4624id2N1bD4XxeZzUZmOcx/8DatjYfX/7c2ybrUKk6Bbo1KhNgnEMdlm/Nm4jczb0gN4tQrNI/Wb7E+v3yTo/+91Hnl9jVqNRo2Z4zPWiZnjM9ZJNuepQklL43Y8XAqJxcpSa+ZcVKiN7TF3LnLTQgghhBCiKPHw8ODNN9/k4MGD1KpVy5jkTI0aW/XD9bqDgoK4d+8ew4YNMzleURRGjBjBiRMn2L9/P05OTmZfZ//+/QQFBRlHniYnJ5OYmIi7uztnz57F29vbWDYiIoIZM2Zw5MgRLl++jJ+fHy4uLtSrV48zZ87k2blLgC0KFgtL6LoaNvSEa/thbRfosxX8aj9VtQ6WkHh+P582m0JiqsKfdxL47WY8p+8mkpymJ00BvR70ikKaXm9YOiwbegX0aQogq9wZWDDrn6e762fupoVGY+4GBJm3ZXNzIvNNgyxujmRx88P0RkGGmxI53LQw3tww8zqPc/MhYx3mzslYh5lzEkIIIYTILx9++CHbt2/n+++/p3PnzlhZWZnsv3PnDps3b+bVV1+lefPmJvtGjhzJsWPHOHDgAC4uLlm+xtixYxk8eLDx+aZNm/j222/ZvXs3np6m2dfHjRvHRx99hIuLCyVLluTy5cvcuXOHv//+m7Jly+bBGRtIgC0KHq01dF9r6MG+/ius6QR9t4FPzSeu8pVXXiEmJoa//vqLmjVr0rCUPQ1L2Wcqd/fuXVq0aMGiRYsZNHgwaYqCXg9pikKaXkGvV/7bpmTYlmH/f/9m/NnwL6b70+vQP3yesaw+07ZH9pt5nYd1Yv51jO3GzLZH2pzjOfGw7Xo9CYlJWFhaGrc/fJ8e1pMTuWmR9zSPBPHmRlRkddPicQL5R0dfmLuRkPEGh/kbCRn2Z3kjIYsRFRleC30aV6JVnLrxACtLi1zfnMh8oyXzDRAZZSGEEELkXo0aNdi6dSudOnVizpw5BAQEULJkSapUqcKGDRs4d+4cNWvWZMeOHSbDv4ODg1m0aBFWVlaULFnSuL13794sWbKEVq1a0bBhQz788EMcHR1N1qp2cXFBq9Xi6+tr0pZDhw4REhJCjx49APDy8mLixIkEBATg6OjIihUr8uy8VYqiyF+zj3jaxcWfB51Ox65du2jdurXZdeWKhJQEQw928FGwdoK+O6B4wBNVpdfrqVixonEZgKz+UJ4yZQo7duzgzp072NtnDsBFZrm9Fp8skM/h5kSGID77mwbZ37RI1T+8OZHxePN1Yv5Gi7FOHmn/ozdCHt7gyHRO6edj9kbO49+0EHlLpTI3eoJspleoMo/MyOamRaZ8D084NcT8jYIMozuyGVFhMmLiMUZUaMzU9aJPDXkh/p8WhYJciyK/Xb9+nfnz5/Ptt9+SlpbG+vXrmThxIqNHj6ZXr14Fbnncp40FpQdbFFyWttBzo6EH+9Zx+K4j9PsRvKo+dlVqtZrp06fTqVMnPvvsM8aPH4+NjY1xv06nY9myZaxZs4YvvvhCgutnQK1WoUaFVpPfLSk6MgbgT3TzwcwNDnM3JzLf9Hhkf4bXMDeiIjc3LdL0+ixHX5i7OZGpzvR60vREx8Zia2uHHh45J7I5T8PIjOwoCqQqCjkWFLlmbmpIfuSzyH4aR4abD+ZuFJi5OaFRq1D0es5EqLC4EIqlVptjPousRlTk5qaFTA0RQhRUqampLFu2jBUrVhAXF2ecK3316lXmzp1LyZIlMw0PL+wkwBYFm5W9IZv4d2/AnT9gdXvo/xN4Pn4a/TfffJNly5YxdOhQfv75Z9q2bYufnx9hYWH8+OOP3L9/n08++YSxY8c+gxMRIu/JTYvMHvbUNHjsnhpFeTj94UlGTKSm5X70hbmbE+ZvRJDFSIbMNw3M3wh5vJsWmUZcZDE1xPwNl8yjL3IaI6cv8lNDNKy4nHeJc3L1io8E448zNeR557Ow0OTN1BDJZyFEwZSWlkaPHj3YunUrffr0oWvXrnh7exMcHMzcuXP59ttvef3119m0aRNvvPFGfjc3z0iALQo+a0dDNvHVHeDeaVjVHgbsAnf/x65q4MCBvPbaa3z99dds2LCBiIgIHBwcePPNNxk+fDjVqlXL+/YLIQoFlephUCHyhmIS3D/bfBbp0z0KSj6L1DQ94eGROLm4GG4k5PKmhUleDjPvXU7S9AppKJD2HH7BLwhzNy0yL+GZMWjP/dSQ55HPAkXPhRAVsX/cxtLC4mE5M6MvnvbmhOSzEBktXbqULVu2MGfOHF577TXAMG0ToHbt2tSqVYsJEybQu3dvbt68iZubW342N89IgC0KBxtnQzbxVe0h9CysamfoyXZ7/Ix/pUuXZvr06UyfPj3v2ymEEMJI9V8v5Yv4x8bD0RS183Tea0HOZ5F+UyI1LasRGYUzn0XRuGmhYdP1C/ndCODp8lmkr9DxPJY6zet8Fhbqh3XlxdSQgp7PQlEU5s+fT7NmzYzB9aMsLCyYMGECgYGBrFixgnHjxj3nVj4bL+L/eaKwsnWFvtthVVu4f8EQZA/YBS6l8rtlQgghxHMhU0PyXqbRB/rHvPlQgPJZmMvNkZqaxt17IXh4FkOBXOWzMD/KJMP+LM9T8lnkh0yjJp5JPgv1Y00NeRARQYhXPWo0ac6aM9GoVYZ2qgBdvIr05OCurq689tprbNiwQQJsIfKFnZshyF7ZBsIvw8r/gmxnv/xumRBCCCEKoaJ+0+LhaIqXnksW8afNZ2G8OZCL0RcFNZ+FXv/fjZpcTg0pqvksnOp04ngMHD8fY7Ldw1pNzwzPfX19uXjx4vNt3DMkAbYofOw9od9OWNEaIq8ZerQH/AyOxfO7ZUIIIYQQLzTJZ5H3nnc+izRjHfpcTw159HXu3gth1XffERj4Ou4enia5KEiOMzm/8PDwArs08pOQAFsUTg5ehiB7ZWt4cANWtjX0ZDt45flLvfPOO2zbto3o6GgcHBzo0qULM2fOxNLSMlPZJk2a8Pvvv5vcIb58+bJxSYL33nuPZcuW4efnx/r166lcuTIAQUFBvPnmmxw/frzArQUohBBCCCHyT2HMZ5GaWo4fPulDivMD3vr0U+N2vV5PcPDDHu34+Hj27dvHmDFj8qGVz4Y6vxsgxBNz8jEE2U4l/uvJbgdx9/P8ZYYPH87FixeJiYnhzJkznDlzhpkzZ2ZZfsaMGcTFxRkf6cH1qVOn2LZtGzdu3GDQoEGMHz/e5DXmzJkjwbUQQgghhCj0LCwsGDZsGD/99BP//POP2TKKorBo0SKSkpJ46623nnMLnx0JsEXh5lwC+u0ARx/DnOzVHSA+Ik9folKlStjZ2QGGLwK1Ws2VK1ceu56goCBq1aqFo6MjLVu25Nq1awCsW7cOLy8vmjZtmqftFkIIIYQQIr+MGTOGmjVr8vbbb7Np0yYSEhKM+27evMknn3zCmjVr+PLLL/HzKzr5lCTAFoWfa2lDT7a9lyG7+HcdICEyT19i+vTp2Nvb4+npyZkzZ3jnnXeyLDt58mRcXV156aWXWL16tXF71apV+eOPP4iKimLfvn1Uq1aNBw8eMHXqVGbPnp2n7RVCCCGEECI/2djYsHv3btq2bcuUKVNo0aIFQ4cOBaB79+4cPXqUb7/9Ntu/qwsjCbBF0eBW1hBk23lCyFn47g1IjMqz6j/44APi4uK4cOECb7/9Nl5e5ud6T5s2jWvXrhEaGsr06dN555132Lp1KwBVqlRh9OjRNGnShN27d/PFF1/w3nvvMX78eC5cuEDTpk1p1qwZR48ezbN2CyGEEEIIkV8cHBzYsGEDQUFBjBs3jurVqwOwePFibt++zaBBg/K5hXlPAmxRdHiUNwwXt3WDe6dhTSdIisnxsMdRqVIlatSoQf/+/c3ur1evHk5OTmi1WgIDAxk6dCgbN2407h85ciSnT59m586dXL9+nZs3b9KrVy969uzJt99+y9KlS+nVqxdKTusxCCGEEEIIUUiUKlWKSZMmsWjRIgB69uyJjY1NPrfq2ZAAWxQtnpUM62TbuMCdP2Btl0xLATwtnU6X6znYarX5j1hKSgrvvvsuixYtIiwsjNTUVMqUKUPZsmVJSUkhLCwsL5sshBBCCCGEeA7yPcBeuHAhpUqVwtramjp16nDy5Mlsy0dFRTFixAi8vb2xsrKifPny7Nq166nqFEWMVzXosw2sneDWcVjXDVIScjzMnLi4OFasWEFUVBSKonD27FkmT55MYGBgprJRUVHs2rWLhIQE0tLS2L9/P0uWLKFTp06Zyk6bNo0uXbpQrlw53N3dSU5O5syZM/zzzz+kpKTg5ub2RO0VQgghhBBC5J98DbA3btzI2LFjmTRpEn/99Rc1atQgMDCQ+/fNL7WUkpJCixYtuHHjBps3b+bSpUt88803+Pj4PHGdoogqHgC9t4KVIwQfhfXdQZf42NWoVCrWrVtH2bJlcXBwoEOHDrRp04a5c+cC0KpVK6ZOnQoYerY/++wzvLy8cHFxYcyYMcyZM4cuXbqY1Hnp0iV27tzJuHHjANBoNCxevJhWrVrRqlUrvv76azQazVOdvhBCCCGEEOL5y9f1yufMmcOQIUMYMGAAAEuWLOGnn35i+fLlfPDBB5nKL1++nMjISH777Te0Wi1gGM//NHWKIsy3JvTaDGvehOu/woZe0H0daA1rTUdFRREZGYmjoyPu7u5mq7Czs2Pv3r1ZvsTPP/9s/NnDw4MTJ07k2KwKFSrwxx9/mGzr1q0b3bp1y81ZCSGEEEIIIQqofAuwU1JS+PPPP5kwYYJxm1qtpnnz5vz+++9mj9mxYwf16tVjxIgRbN++HQ8PD3r27Mn48ePRaDRPVCdAcnIyycnJxucxMYbEWDqdDp1O97Sn+kykt6ugtq/A8H4ZVbd1aDZ0R3VtP/qNffjZsReLl37LgQMHjMVq1arFW2+9RefOnaX3+DHJtSgKErkeRUEh16IoKORaFAVJYbgen7Zt+RZgh4eHk5aWRrFixUy2FytWjIsXL5o9JigoiAMHDtCrVy927drF1atXGT58ODqdjkmTJj1RnWCYD/vZZ59l2r5nzx5sbW2f4Oyen+x6V8VD7iVHUffabDRX9/CSUwRvDR7BkCFDMpXbvXt3PrSuaJBrURQkcj2KgkKuRVFQyLUoCpKCfD0mJDxZ7qZ0+TpE/HHp9Xo8PT1ZunQpGo2GmjVrcufOHWbNmsWkSZOeuN4JEyYwduxY4/OYmBj8/Pxo2bIljo6OedH0PKfT6di7dy8tWrQwDpcX2WnN9rl3aB29muLRf1L71jecq/Auiuphb/XJkyeZMWMG/fv3Z/bs2fnY1sJFrkVRkMj1KAoKuRZFQSHXoihICsP1mD6a+UnlW4Dt7u6ORqMhNDTUZHtoaCheXl5mj/H29kar1ZoM4a1UqRIhISGkpKQ8UZ0AVlZWWFlZZdqu1WoL7C8+XWFoY0GQkpLC27O+Z3iLqnxS/hJeEb+jXLHgfPl34L8gu06dOnTr1o0lS5bw0UcfZRoJIbIn16IoSOR6FAWFXIuioJBrURQkBfl6fNp25VsWcUtLS2rWrMn+/fuN2/R6Pfv376devXpmj6lfvz5Xr15Fr9cbt12+fBlvb28sLS2fqE7xYtixYwdhYWH4NR3E2Yr/Q6/S4B12hMpXFqHSP5xn0bp1azQaDatWrcrH1gohhBBCCCEKo3wdIj527Fj69etHrVq1qF27NnPnziU+Pt6YAbxv3774+Pgwbdo0AIYNG8ZXX33F6NGjeeedd7hy5QpTp05l1KhRua5TvJguXbqEs7MzJUuWJIySnKswhqoX51D8/iGK3z9EstaZJCt3kq3c+PoNF6zu7IBz5cDJD5x8wL4YqCX5mRBCCCGEECJr+Rpgd+vWjbCwMD755BNCQkIICAjgl19+MQ7NvXnzJmr1w052Pz8/du/ezZgxY6hevTo+Pj6MHj2a8ePH57pO8WJSq9Xo9XoURUGlUnHfvR7nKrxLpWtL0abGYaWLwkoXBXFX6Vse4CxsznBTRm0BDt7g5AuOPoag29HX8K+Tr+FnW1dQqfLpDIUQQgghhBD5Ld+TnI0cOZKRI0ea3Xfo0KFM2+rVq8fx48efuE7xYnr55ZeJiYnh33//pXLlygDc96jPffdX0abGYJ0cgXVyOKmRwRzYupqOr9XC39MWYu5AzF3Qp0L0LcMjKxY24Fj8v6Dbz3wgbuXwnM5YCCGEEEII8bzle4AtxPPQokULSpcuzffff8+kSZNQpfc0q1TotE7otE7E2pfh681/88vvFgzd/BOkZ5DXp0FsiCHYjr793793DMF2+s/x9yE1ESKvGR5ZsXL6L+j+L+DOGIA7/vfQWj/7N0QIIYQQQgiR5yTAFi8EtVrN1KlT6dGjB0uWLGHQoEFYWloa96elpbF161Y2b97MlClTTJdnU2v+64H2Ab/a5l8gNdnQ020MwDMG4rch5jYkRUNyNNyPhvsXsm6sncfDANxcIG7vBRr56AohhBBCCFHQyF/p4oXRvXt3IiMjeeeddzh48CAtWrTAy8uLyMhI9u/fz71793jvvfeYMGHC41duYQWupQ2PrCTH/Rd03zIE3ukBeMxtQxAefcfQCx4fZnjcO22+HpXmv/ngPo8MQ88QiNu5y3xwIYQQQgghnjMJsMULZfjw4TRt2pTFixezfv16IiIicHBwoGPHjgwfPpzatbPooc4LVvbgUcHwMEdRIPGBaS94xp7wmNsP54PH/NcrnhWNlelQ9PRAPOPccGunZ3OeQgghhBBCvKAkwBYvnIoVKzJv3jzmzZuX300xpVIZMpHbuoJ3dfNl9GkQd99MAJ4hEI8LhbRkiAwyPLJi6ZAhC/qjQ9J9DQnbtDbP5lyFEEIIIYQogiTAFqIwUWvA0dvw8K1lvkxqCsTezTAM3cyQ9MQHkBILYRcNj6zYuv0XdPuZ6RH3NQxVF0IIIYQQQgASYAtR9FhYgkspwyMrKfEZ5n9nEYjr4iEhwvAI+cd8PSo1FvbFaJhmiybpB3D2yzwk3dYdMqxnL4QQQgghRFElAbYQLyJLO/Aob3iYoyiQFPUw+Vp6IG4yJP0u6HWoYu/hCvBvFsuTaSwNw80zrgf+6Nxwa2dJyiaEEEIIIQo9CbCFEJmpVGDjYnh4VTNfRq+H+DBSI27w1687qVmuGJq4ENO54bEhkJYCD24YHlmxtM+QET3jPPAMgbil7bM4UyGEEEIIIfKMBNhCiCejVoNDMRRrV+45h6Cv3RqNVmtaJk0HsfdM1wN/dEh6YiSkxEH4JcMjKzaupuuBPzo33LE4aLRZHy+EEEIIIcQzJgG2EOLZ0WjBuYThkZWUBMNw84zrgZsE4rcNAXhipOERcjaLilRgX8x0PXDjkPT/frbzlPngQgghhBDimZEAWwiRvyxtwb2c4WGOokBS9MPka9G3MmREzzAvPC0F4kIMjzt/mK9LrTVkYM+4HvijQ9JtXGQ+uBBCCCGEeCISYAshCjaVCmycDY9iVcyX0eshIdx0PfBHA/HYe6DXQdRNwyMrWtsMWdB9zQ9Jt7R7FmcqhBBCCCEKOQmwhRCFn1oN9p6Gh8/L5sukpRqC7JhHsqFnHJKeEA66BIi4YnhkxdrZNAnbo3PDHX0My6UJIYQQQogXigTYQogXg8bCsE63s1/WZXSJhvngGXvCTeaG34HkGMMSZklREHoui4pUhmA/43rgxiHp/wXi9sVArXkGJ/r/7N13eFRl2sfx75nJpHdCekINTaUjKrAq0l0UbKjsQhQRC4uIFBVQmuIiCmJ/XQR0Ley6K2tBXUCxIlaUFURpJiEJoaYnM5mZ949JhkwyQQJpwO9zXedK5pTn3CcegTv3U0RERESksSjBFhGpYAmAZm1cW01K8ipVv2vokm4vhYL9ri3zO+/tmHwgJL5S93MviXhgpMaDi4iIiJxGlGCLiNSGf6hri+7o/bjTCUWHqiTglRPxjPLx4GWQm+baauIT4DkRm7cu6X4h9fOcIiIiIlJrSrBFROqSYUBQlGuL7+r9HHuZq7pdeT3wqmPDCw9AWTEc2unaauIXVinprtwNvdI+H796eVQRERER8aQEW0SkoZl9yruEJ0DS+d7PsZVAfmal5LtyIl4+NrwkF0pzIScXcn6q+X5BzasvR1a5S3pIrMaDi4iIiNQBJdgiIk2RxR8iW7u2mpTme86C7q1LelmxqxpeeACytnhvxzBDSFylpckSqifigc00HlxERETkdyjBFhE5XfmFQHQH1+aN0wlFh48l4N4mZ8vPdI0Hz8twbembvbfl4w+h8Z7rgVcdG+4fWn/PKiIiInIaUIItInKmMgwIauba4rp4P8dhh4Kc8qQ7w3uX9IL9UFYCh3e7tpr4hVYaB15DIm7xr59nFREREWkClGCLiJzNTGYIjXNt9PJ+Tpm1fDx4hmeX9MoV8ZKjrjXCD+TBge013y8wynMW9Kpd0kPiXGPURURERE5D+leMiIgcn48vRLR0bTUpLYC8TM/1wKuODbcVQdFB15b1g/d2DJMryfaohCd6JuKBUWAy1ceTioiIiJwSJdgiInLq/IKheTvX5o3TCcVHKq0HXjkRr/icBQ6b63PePsio4V5mX9d48IpZ0Ct3SQ+Kwaes0HU/ERERkQamBFtEROqfYUBgpGuLPc/7OQ4HFOZUqn576ZKenw12KxzZ69qqsACXA84dU6usB55YfWy4JaAeH1hERETORkqwRUSkaTCZXGtyh8QCPbyfU2aF/KwaKuEZOHP3YRQfxrAWwsEdrq0mAZGe64FXHRseEgdmS708qoiIiJyZlGCLiMjpw8cXIlq4Ni/KbDY+eOdNBl94LpbC/TV3SbcWQPFh15a9tYabGa5kv/J64FUT8aBojQcXERERNyXYIiJyRrGb/KBZCsR28n6C0wkluZXWA6/0taJLel6mqyt6fpZr2/eN97ZMlvLx4JUmYavaJT0gwtVFXkRERM54SrBFROTsYhgQEO7aYs/1fo7D4ZrtvPJ64FUT8YJs16RsR39zbTWxBFUa/53gfWy4b1B9PKmIiIg0MCXYIiIiVZlMEBzt2hJqGA9ut7kmXau8HnjVLulFh8BWCAd/cW018Q8/Nvmaty7pIfGu7vEiIiLSpCnBFhERORlmC4Qnubaa2IrL1wfP8NIlvfx7az6UHHVt+48zHjw42rMretVEPDhG48FFREQamRJsERGR+mIJgGZtXFtNSnI9u6F7S8TtpVCw37Xt+9Z7OyYfV6U7rPKSZJXHhidqPLiIiEg9U4ItIiLSmPzDXFvMcSZlKzzouR64x9jwfa6J2BxlkJvm2mriE1B9PfCqM6P7hdTPc4qIiJwFlGCLiIg0ZYYBwc1dW3w37+fYy1yTrlXMgp67r3olvPAAlBXDoZ2urSb+YZ7rgVftkh6aAD5+9fOsIiIipzkl2CIiIqc7s8+xbuD09n6OrcSVaLvXA6+aiO+D0lxXl/WSXMj5qeb7BUV7dkOvmoiHxILJXC+PKiIi0pQpwRYRETkbWPx/fzx4aX6lpLuGLullJVCY49oyv/fejmF2rQ9euert0SU9EQKbaTy4iIiccZRgi4iIiItfCER3cG3eOJ1QdLg8+c7wrIS7u6NngtNenpSnQ3oN9/Lxrz7+u2oi7h9ab48qIiJSH5Rgi4iIyIkxDAhq5triung/x2F3zXbuTr4zqndJL8xxVcIP73JtNfELrTL+u8rY8NAEV2VeRESkiVCCLSIiInXHVNE9PB7o5f2cslJXpdu9Hnh6pbHh5Ul4yVEozYOcba6tJoFRnuuBVx0bHhzrGqMuIiLSAGr9N07Lli25+eabSU1NJTk5uT5iEhERkTOZjx9EtnJtNSktqLIeuJcu6bYiKDro2rJ+8N6OYYKQuEpJt5cu6UFRGg8uIiJ1otYJ9uTJk1m5ciXz5s3j0ksvZdy4cYwcORI/Py3ZISIiInXELxiat3dt3jidUHyk+nJklRPxvEzX+uAVs6dnfOW9LbOfq+JeeTb0ql3S/cPq71lFROSMcVIJ9uTJk/nuu+9YuXIlf/nLX7jjjju48cYbufnmm+nevXt9xCkiIiJyjGFAYKRri+vs/RyHwzXeOzfDczmyyl3SC/aDvRSO7HFtNfENqVT1rtolvTwRtwTUz7OKiMhp46QHJXXv3p3u3bvz2GOP8cwzzzBjxgyeffZZzjvvPCZNmsRNN92Eoe5WIiIi0lhMJtea3CGxkNjT+zllVsjP8lIJr7RcWfERsObDgZ9dW00Cm7kr4KbgONruz8f4qRgiWrgS8JA4MFvq51lFRKRJOOkE22az8eabb7JixQrWrVvHBRdcwLhx48jIyOD+++9n/fr1vPrqq3UZq4iIiEjd8vF1JcARLWo+x1ro6m7uUQmv0iXdVghFh1xb9o+YgXMA1vzjWDuGyTXpWuWJ2Kp2SQ9q7vrFgIiInJZqnWB/9913rFixgtdeew2TycSYMWNYsmQJHTocWzNz5MiR9OpVw8yhIiIiIqcT3yCISnFt3jidrlnP3bOgp2M/kk7m9q9ICAFT3r7y8eA2yM90bXztvS2zr6vSXXk9cHc39PJE3D9ck7KJiDRRtU6we/XqxcCBA3n22WcZMWIEFkv1rk6tWrXi+uuvr5MARURERJo0w4CACNcWey4ADpuN74rXEjtsGCaLpXw8+AHP9cCrdknPzwK7FY7+5tpqYgnyXA+88lrhFWPDfQMb6OFFRKSyWifYu3fvpkWL43SjAoKCglixYsVJByUiIiJyRjGZICTGtSX08H6O3VY+HrzSeuBVE/GiQ67u6Ad/cW01CYjwnAW9aiIeEu/qHi8iInWq1gl2Tk4O2dnZ9O7d22P/5s2bMZvN9OxZwyQiIiIiIlIzswXCk11bTaxFru7mldcDz02v9P0+14RsxUdc2/6tNTRkQHCM53rgVceGB8doPLiISC3VOsG+8847mT59erUEe9++ffz1r39l8+bNdRaciIiIiFTiGwhRbV1bTUpyPdcDr9YlPdO1NFlBtmvb9633dkwWCI07Vgl3T8hWqSIeEKHx4CIildQ6wd62bZvXta67devGtm3b6iQoERERETlJ/mGuLeYc78edTig86LkeeNVEPD/LNSnb0TTXVhNLYPX1wKt2SfcLrp/nFBFpgmqdYPv5+bF//35at27tsT8rKwsfn5Ne9UtEREREGoJhQHBz15ZQvWgCgL3MVd3O3VclEa/UJb3oINiK4NCvrq0m/mHHJl/zVgkPjQcfv/p5VhGRBlbrjHjQoEHcd999/Oc//yEsLAyAo0ePcv/99zNw4MA6D1BEREREGpjZ51gSTG/v59hKXAl35fXAPcaGZ0BpnqvLekku7P9fzfcLivZcD7zq2PDgGDCZ6+VRRUTqUq0T7MWLF/OHP/yBFi1a0K1bNwC2bNlCTEwML7/8cp0HKCIiIiJNkMUfmrVxbTUpyavSDd1LIl5WAoU5ri3zO+/tmHxc64O7q99eEvHASI0HF5FGV+sEOyEhgR9//JFXXnmFH374gYCAAG666SZuuOEGr2tii4iIiMhZyj/UtUV39H7c6XQtPVZ5PfCqXdLzMsFRVt41PR3Sa7iXT4Cru3nl9cCrJuL+ofX2qCIicBIJNrjWub711lvrOhYREREROZsYBgRFubb4rt7PcdghP9tzPfCqiXhhDpQVw+Fdrq0mfmGVqt4VE7FVSsBDE1yVeRGRk3TSs5Jt27aNtLQ0rFarx/4rrrjilIMSEREREQFcY6/DyhPipPO9n1NWWmUitozqY8NLcqE0F3JyIec4K98ENfecBb1ql/TgWNcYdRERL2r9p8Pu3bsZOXIkW7duxTAMnE4nAEb5mBe73V63EYqIiIiIHI+PH0S2dm01KS3wnAXdWyW8rBgKD7i2rC3e2zHMrvHglSvhoYmeiXhQlMaDi5ylap1g33XXXbRq1YoNGzbQqlUrvvrqKw4dOsQ999zD4sWL6yNGEREREZFT4xcMzdu7Nm+cTig+Ul71zvDSJT0D8svHg+dluLaamP08lyRzd0mvNDbcP6x+nlNEGlWtE+xNmzbx4YcfEhUVhclkwmQy0bdvXxYuXMikSZP4/vvv6yNOEREREZH6YxiumcgDIyGus/dzHHYoyKnU/bxqIp7hOm4vhcO7XVtNfEMqjQP3lojHgyWgfp5VROpNrRNsu91OSEgIAFFRUWRmZtK+fXtatGjBjh076jxAEREREZEmwWSG0DjXltjT+zllVlel290NvUqX9LwMV6Xcmg8HfnZtNQls5rkeeNWx4SFxGg8u0sTU+v/Ic889lx9++IFWrVrRu3dvFi1ahK+vL//3f/9H69bHGfciIiIiInKm8/GFiJaurSbWwurrgVdNxG2FriXMig5B9o/e2zFMrknXKiffVSvhvuqKLtKQap1gz5o1i8LCQgDmzZvHH//4R/r160ezZs1YvXp1nQcoIiIiInJG8Q2C5u1cmzcV48ErV71zq3RFz8sEh81VLc/PBL722pSP2ZcB5nDMh56D8CTvXdL9wzUpm0gdqXWCPXjwYPf3bdu25eeff+bw4cNERES4ZxIXEREREZGTVHk8eOx53s9xOFyznedm1FwJz8/GsFsJsudAWg6k1XA/3+BKM6JXrYSXf/UNrLfHFTmT1CrBttlsBAQEsGXLFs4991z3/sjIyDoPTEREREREamAyQUiMa6OH93PsNmxH0vnygze4sFMSPoXZ1RPx4sNgLYCDO1xbTQIiPdcDrzo2PDQezJZ6eVSR00mtEmyLxUJycrLWuhYRERERaerMFghL4nBwe5znDgOLlwTYWuTqbp5XPiu6ty7p1gJXIl58GLK31nAzA4JjPNcDr5qIB0W7fjEgcgardRfxmTNncv/99/Pyyy+rci0iIiIicjrzDYSotq7NG6cTSnIrrQeeXmlseMXnTLBboSDbte37xntbJotrBvbK64FX7ZIeEKHx4Ge51NRUXn31VXx9fd371q1bx4UXXuj1/KeeeoqVK1eydetWhg4dypo1azyOT5s2jeXLl5OUlMRrr71Gp06dANi9ezdXXXUVX375Jf7+/nUWf60T7KeeeoqdO3cSHx9PixYtCAoK8jj+3Xff1TqIp59+mkcffZTs7Gy6dOnCk08+yfnnn+/13JUrV3LTTTd57PPz86OkpMT9OTU1lVWrVnmcM3jwYN5///1axyYiIiIictYyDAgId20x53g/x+GAooOek7C5v68YD57lmpTtaJprq4klsNIs6IneK+G+QTVfL2eEO+64g6VLl57QufHx8cyaNYv169eTkZHhcezrr79mzZo17N27l5UrVzJjxgzefvtt9z0ef/zxOk2u4SQS7BEjRtRpAKtXr2bKlCk899xz9O7dm6VLlzJ48GB27NhBdHS012tCQ0M91tz2NrnakCFDWLFihfuzn59fncYtIiIiIiK4un0HR7u2hO7ez7GXuZLsvCqzoVfukl50EGxFcOhX11YT/3DPSdiqdkkPTXAtlyZnhauuugqALVu2VEuwd+/eTc+ePQkNDWXQoEE899xzALz66qvExsbSv3//Oo+n1gn2gw8+WKcBPP7444wfP95dlX7uued49913efHFF7n33nu9XmMYBrGxscdt18/P73fPERERERGRBmD2cS0TFp5U8zm2Yld388rV76pd0kvzoOSoa9v/vxoaMlzJvrsbemL1RDw4BkzmenhQqQsvvfQSL730EnFxcdx8883cfffdmE5i/P65557LrFmzOHr0KOvXr+e8887jyJEjPPzww3z88cf1EPlJJNh1yWq18u2333Lfffe595lMJgYMGMCmTZtqvK6goIAWLVrgcDjo3r07Dz/8MOec49llZePGjURHRxMREUH//v1ZsGABzZo1q7dnERERERGRU2AJgGZtXFtNSnIrjf+uIRG3l0LBfteWWcPwVZMPhMRX6n6eUGVseKJrmTSNB29wkyZN4tFHHyUyMpKvv/6a6667DpPJxN13313rts455xzuuusuLrnkEpKSknjmmWeYNm0aM2bMYNu2bTz44IMYhsHcuXPp27dvncRf6wTbZDIdd73r2swwfvDgQex2OzExMR77Y2Ji+Pnnn71e0759e1588UU6d+5Mbm4uixcv5qKLLuKnn34iMTERcHUPv+qqq2jVqhW7du3i/vvvZ+jQoWzatAmzufpvqkpLSyktLXV/zsvLA1zLktlsthN+noZUEVdTjU/OHnoXpSnR+yhNhd5FaSrOuHfRHAiRKa7NG6cTig5BXgZGXiZGnisZ9/ian43hKIPcNNdWA6dPAITG4yzvdu50f59Y/jUe/ELq6UHPTCfyPp53nmvtd4fDQY8ePZg2bRp///vfmThx4nHbttvtOByOam1PmDCBCRMmAPDpp5/y22+/cd1119G2bVvWr1+P0+lk8ODB/PrrrxiGccr/r9Q6wX7zzTc9PttsNr7//ntWrVrF3LlzTymYE3HhhRd6zCB30UUX0bFjR55//nnmz58PwPXXX+8+ft5559G5c2fatGnDxo0bueyyy6q1uXDhQq+x//e//yUwMLAenqLurFu3rrFDEAH0LkrTovdRmgq9i9JUnL3vYoJr8wOauzbDacfPlkuA7RAB1kMEWA8TYDtMgPUQgdZD+NsO41+Wh1FWDId3YRzeVWPrNnMgRZZmFPtGUmyJpMS3GcWWSIp8XftKLJE4TFofvKravI/btm3j6NGjrF279rjn/frrr+zfv7/G82w2GzNmzGDatGm8/vrrFBYWuou6+fn5vPbaa4SHh1NUVHTiD+JFrRPsK6+8stq+a665hnPOOYfVq1czbty4E24rKioKs9nM/v37Pfbv37//hMdPWywWunXrxs6dO2s8p3Xr1kRFRbFz506vCfZ9993HlClT3J/z8vJISkpi0KBBhIaGnuDTNCybzca6desYOHAgFm9rGoo0EL2L0pTofZSmQu+iNBV6F0+OrawE8rNcFe9czwq4ke8aJ26U5mGxFxFmLyKsJL3GtpxBzXGGxEPYscr3sap44lk1HvxE3sd//vOfDB48mJCQEL777jvee+89brvtNoYNG+b1/LKyMsrKyvjyyy8pKCigf//+mEwmj2W+AObPn89NN93EuHHjsNvt3H333SQkJGAYBoZhMGrUKMxms7s388mqszHYF1xwAbfeemutrvH19aVHjx5s2LDBPTu5w+Fgw4YNv9sFoILdbmfr1q01/sABMjIyOHToEHFxcV6P+/n5eZ1l3GKxNPk/iE6HGOXsoHdRmhK9j9JU6F2UpkLvYi1ZLBAQAtHtaj6nNN9zFvTKs6NXfC0rwSg8gFF4ALJ/8N6OYYaQuEpLk1X+Wj42PLDZGTUe/Hjv43PPPccdd9xBWVkZCQkJ3HHHHUydOtU9ydltt93mPg/goYce8uiNHBoaysUXX8zGjRvd+3bs2MHatWvZtGmT+97PPvssV1xxBYZh8Pzzz7uX6zrV/0/qJMEuLi5m2bJlJCQk1PraKVOmMHbsWHr27Mn555/P0qVLKSwsdM8qPmbMGBISEli4cCEA8+bN44ILLqBt27YcPXqURx99lN9++41bbrkFcE2ANnfuXK6++mpiY2PZtWsX06dPp23btgwePLguHldERERERM52fiEQ3cG1eeN0QtHhYwl4XvlkbO7v90F+JjjKXOfkZUD6Zu9t+fi7xny7lyarmogngn/T7HlbIT09nT179gBQVFREWFiY1/M++eST47ZTkVhXmDNnDnPmzDnuNe3bt+ebb77x2Ddq1ChGjRr1O1HXXq0T7IiICI9JzpxOJ/n5+QQGBvL3v/+91gGMGjWKAwcO8MADD5CdnU3Xrl15//333ROfpaWleUzJfuTIEcaPH092djYRERH06NGDL774gk6dOgFgNpv58ccfWbVqFUePHiU+Pp5BgwYxf/58rYUtIiIiIiINwzAgqJlri+vi/RyH3TXbeeVKeNVEvGA/lJXA4d2urSZ+oZVmQS+vfLu/T3Ql6JaA+nnW41i3bh2PPvoo69atIyAggNdee42UlBRuuOEGZsyYQXx8fIPHVJ9qnWAvWbLEI8E2mUw0b96c3r17ExERcVJBTJw4scYu4ZVL+xX3X7JkSY1tBQQE8MEHH5xUHCIiIiIiIg3GZC6vTMcDvbyfU1bqWh/cvR64ly7pJUdda4QfyIMD22u+X2CU53rgVSviIXGuNcvryBNPPMHkyZNp2bIlo0ePplWrVgD06tWLFStW8M9//pMPP/yQDh1q6AVwGqr1Ty81NbUewhAREREREZFqfPwgspVrq0lpgSsJr7weeNVE3FYERQddW1ZN48FNriTboxJepUt6YBRU6mFck//+979MnjyZ/v37c+WVV7onEwMYOnQoF110Ec888wxDhw7l559/PmN6G9c6wV6xYgXBwcFce+21Hvv/+c9/UlRUxNixY+ssOBEREREREfkdfsHQvJ1r88bphOIjxyre7up3pS7peVngsLk+5+2DjBruZfZ1VdzDkmpOxP3DWLRoEa1atXIn11WFhoYyduxYFi5cyL///W9uuOGGuvt5NKJaJ9gLFy7k+eefr7Y/OjqaW2+9VQn2acZms3H33XfzyiuvYBgGo0ePZsmSJfj4eL4apaWlTJw4kfXr13Pw4EESEhKYPn06N998s/ucadOmsXz5cpKSknjttdfc4+J3797NVVddxZdffumenU9ERERERBqIYUBgpGuLPc/7OQ4HFOZUqn5nVK+EF+wHuxWO7HVtNXD4BLKsfT5l3WMxF64h3xRGgSmMAnM4QSXH8oy4uDjatWvHCy+8cPYm2Glpae6+85W1aNGCtLS0OglKGs6CBQv47LPP2LZtG+DqrvHwww/zwAMPeJxXVlZGXFwc69evp3Xr1mzevJmhQ4eSmJjIoEGD+Prrr1mzZg179+5l5cqVzJgxg7fffhuAO+64g8cff1zJtYiIiIhIU2UyQUisa6OH93PKrJCfdaz67a1LevFhTGVFdGpuBg5A6QGPJoqLI/g5bKr7c3JyMjt37qy/52pgtU6wo6Oj+fHHH2nZsqXH/h9++IFmzZrVVVzSQF588UWWLFniXiN85syZTJ06tVqCHRQUxLx589yfL7jgAi699FI+++wzBg0axO7du+nZsyehoaEMGjTIPX3+q6++SmxsLP3792+4hxIRERERkbrn4wsRLVxbTaxFfP/JO0y79UYmXH85SWEmQhy5BDtyCXHkYvdr7nG6zWbD19e3ngNvOLVOsG+44QYmTZpESEgIf/jDHwD4+OOPueuuu7j++uvrPECpP0eOHCEjI4OuXbu693Xt2pW0tDRyc3NrXJsOoKSkhK+++oobb7wRgHPPPZdZs2Zx9OhR1q9fz3nnnceRI0d4+OGH+fjjj+v7UUREREREpCnwDaRt76F8meOP5Xsrw4YNcx8yDIOkpCRITwfA4XDwv//9j+HDhzdWtHXu96d/q2L+/Pn07t2byy67jICAAAICAhg0aBD9+/fn4Ycfro8YpZ4UFBQAEB4e7t5X8X1+fn6N1zmdTm655RZSUlK46qqrADjnnHO46667uOSSS/jggw9YvHgx06ZNY8aMGWzbto3+/ftz2WWX8dlnn9Xb84iIiIiISOMLCQlh7NixfPHFF8fNK77++msOHTpU45LNp6NaV7B9fX1ZvXo1CxYsYMuWLQQEBHDeeefRosVxuglIkxQcHAxAbm4uUVFR7u/B9T+FN06nkzvuuIMdO3awfv16TJWm6K+8nvknn3xCWloao0ePpkWLFnz88cc4nU769+/P3r17vc4kKCIiIiIiZ4Z7772XN954g2eeeYbU1FRiYmLcxxwOB1999RWrV69m9OjR9OhRw5jv09BJryKekpJCSkpKXcYiDSwiIoLExES2bNlCmzZtANiyZQtJSUleu4c7nU7uvPNONm/ezIYNG2rsQm61Wpk8eTL/+Mc/OHDgAGVlZbRu3dp97MCBA0RHR9ffg4mIiIiISKNKSkriww8/ZOjQoTz00EN06NCBNm3aMH78eBYtWsS+ffu48cYbefHFFxs71DpV6y7iV199NX/961+r7V+0aFG1tbGl6bvpppt46KGHyM7OJjs7m4cffphbbrnF67kTJ07k888/Z926dURERNTY5sKFC7n22mtp27YtUVFRlJaW8sMPP/Djjz9itVo1GZ6IiIiIyFngnHPO4ZdffuGll14iOjqaX375BYAhQ4bw1Vdf8corr5xRE5zBSVSwP/nkE+bMmVNt/9ChQ3nsscfqIiZpQLNnz+bQoUN07NgRgD/96U/cf//9ANx2220APPfcc/z2228888wz+Pn5eQwH+NOf/uSeMRxgx44dvP3222zatAkAs9nMs88+y9ChQzEMg+effx6z2dxQjyciIiIiIo3I39+fP//5z/z5z3/GZrOxdu1ann32WSwWS2OHVi9qnWAXFBR4/S2DxWIhLy+vToKShmOxWHj66ad5+umnqx2rnDi3aNECp9P5u+21b9+eb775xmPfqFGjGDVq1KkHKyIiIiIi0oTVuov4eeedx+rVq6vtf/311+nUqVOdBCUn76effmLSpEn84Q9/oF+/fkyYMIFvv/22scMSERERERE549W6gj179myuuuoqdu3aRf/+/QHYsGEDr776Km+88UadBygnpqSkhFtuuYVXXnmF6OhoLr74YkwmE2vXruX//u//uPzyy3n11VcJDQ1t7FBFRERERETOSLVOsIcPH86aNWt4+OGHeeONNwgICKBLly58+OGHREZG1keM8jscDgfXX389H3zwAU8//TSjRo1yd+MvKyvj7bffZtKkSfzxj39k3bp1+Pn5NXLEIiIiIiIiZ55adxEHuPzyy/n8888pLCxk9+7dXHfddUydOpUuXbrUdXxyAt555x3+85//sGrVKv785z97jJH38fFh5MiRvPHGG3z22We8/PLLjRipiIiIiIjImeukEmxwzSY+duxY4uPjeeyxx+jfvz9ffvllXcYmJ+iZZ56hR48eDB061GO/s7TU/X3v3r0ZNGgQzzzzTEOHJyIiIiIiclaoVYKdnZ3NI488QkpKCtdeey2hoaGUlpayZs0aHnnkEXr16lVfcUoNnE4nH374IVdffbV7n6OoiCMLHuLAXybhtNvd+6+55hq+//57jhw50hihioiIiIiInNFOOMEePnw47du358cff2Tp0qVkZmby5JNP1mdscgLsdjs2m43w8HD3PsfhIxStW4f1++/Jf/nv7v1hYWEAFBcXN3SYIiIiIiIiZ7wTTrDfe+89xo0bx9y5c7n88ssxm831GZecIB8fH6Kjo/npp5+O7UtMIPyeewDIe+45rD//DMC2bdvw8/PTZHQiIiIiIiL14IQT7M8++4z8/Hx69OhB7969eeqppzh48GB9xiYnaMyYMbz66qsUFRW59wUO/yMB/fuD3c7hWbMpKyxkxYoVXH/99fj7+zditCIiIiIiImemE06wL7jgAl544QWysrKYMGECr7/+OvHx8TgcDtatW0d+fn59xinHcccdd1BUVMQdd9yBzWYDwDAMwu+7F1Pz5pT99hsf3HAj6enp/OUvf2nkaEVERERERM5MtZ5FPCgoiJtvvpnPPvuMrVu3cs899/DII48QHR3NFVdcUR8xyu9o1aoVr7zyCm+99RYDBgxg9erVHD58mHxgx6CBAHTJymL1tGn06NGjcYMVERERERE5Q530Ml0A7du3Z9GiRWRkZPDaa6/VVUxyEq6++mo+/PBDIiMjGT9+PC1btiQ5OZkrH3iA//r7AdD5k08pU7d+ERERERGReuFTF42YzWZGjBjBiBEj6qI5OUn9+vVj/fr1/PLLL/z44484nU46duxIp5QU9l57HaW//ELWzFkkPvcshmE0drgiIiIiIiJnlDpJsKVpadeuHe3atfPYF//oo+y99loKPv6Yo6+/TsQNNzRSdCIiIiIiImemU+oiLqcP//btiL5nCgD7/7qI0t27GzkiERERERGRM4sS7LNIxJ//TNBFF+EsKSFz6jScVmtjhyQiIiIiInLGUIJ9FjFMJuIWLsQcFkbJtm0cePKpxg5JRERERETkjKEE+yxjiYkmdv48AA797W8UfvVVI0ckIiIiIiJyZlCCfRYKHTSIsKuvAqeTzHvvxZ6X19ghiYiIiIiInPaUYJ+lYu+/H0tyMmWZWWTPm9/Y4YiIiIiIiJz2lGCfpUxBQSQs+iuYzeS98w65b7/T2CGJiIiIiIic1pRgn8UCunYl6vbbAcieOxfbvn2NHJGIiIiIiMjpSwn2WS7qtgkEdO2Ko6CAzBn34rTbGzskERERERGR05IS7LOc4eND/KOLMAUGUvTNNxxa/mJjhyQiIiIiInJaUoIt+CYlETNrFgAHli2j+H8/NXJEIiIiIiIipx8l2AJA2MgRhAwaBGVlZE6bhqO4uLFDEhEREREROa0owRYADMMgdu4cfKKjse7Zw/5Fixo7JBERERERkdOKEmxx84mIIP6RhQAcfe118j/6qJEjEhEREREROX0owRYPQRddRGRqKgBZM2dRdvBg4wYkIiIiIiJymlCCLdU0v3syfu3aYT98mKyZs3A6nY0dkoiIiIiISJOnBFuqMfn5Eb/4UQxfXwo+/pijr7/e2CGJiIiIiIg0eUqwxSv/du2InnoPAPv/uojS3bsbOSIREREREZGmTQm21CjiT38i6KKLcJaUkDl1Gk6rtbFDEhERERERabKUYEuNDJOJuIULMYeHU7JtGweefKqxQxIREREREWmylGDLcVlioomdPw+AQ3/7G4VffdXIEYmIiIiIiDRNSrDld4UOHEjYNVeD00nmjHux5+U1dkgiIiIiIiJNjhJsOSGx992HJTmZsqwssufOa+xwREREREREmhwl2HJCTEFBJDy6CMxm8t59l9y332nskERERERERJoUJdhywgK6dCHqjtsByJ47F9u+fY0ckYiIiIiISNOhBFtqJWrCBAK6dsVRUMC+GTNw2u2NHZKIiIiIiEiToARbasXw8SH+0UWYAgMp/uZbDi1/sbFDEhERERERaRKUYEut+SYlETNrFgAHli2j+H8/NXJEIiIiIiIijU8JtpyUsJEjCBk8GMrKyJw2DUdxcWOHJCIiIiIi0qiUYMtJMQyD2DkP4hMdjXXPHvb/9a+NHZKIiIiIiEijUoItJ80nIoL4RxYCcPT11eR/9FEjRyQiIiIiItJ4lGDLKQm66CIiU1MByJo5i7KDBxs3IBERERERkUaiBFtOWfMpd+PXrh32w4fJnDkTp9PZ2CGJiIiIiIg0OCXYcspMvr7EL34Uw9eXwo8/4chrrzV2SCIiIiIiIg1OCbbUCf927Yieeg8AOX9dROmuXY0ckYiIiIiISMNSgi11JuJPfyKoTx+cpaVkTpuO02pt7JBEREREREQajBJsqTOGyUTcww9jDg+nZNs2Djz5ZGOHJCIiIiIi0mCUYEudssREEzt/HgCH/racws1fNXJEIiIiIiIiDUMJttS50IEDCbvmanA6ybz3Xux5eY0dkoiIiIiISL1Tgi31Iva++7C0SKYsK4vsufMaOxwREREREZF6pwRb6oUpKIiERYvAbCbv3XfJffvtxg5JRERERESkXjWJBPvpp5+mZcuW+Pv707t3b776quZxuytXrsQwDI/N39/f4xyn08kDDzxAXFwcAQEBDBgwgF9//bW+H0OqCOjShag7bgcge+48bPv2NXJEIiIiIiIi9afRE+zVq1czZcoUHnzwQb777ju6dOnC4MGDycnJqfGa0NBQsrKy3Ntvv/3mcXzRokUsW7aM5557js2bNxMUFMTgwYMpKSmp78eRKqImTCCgWzccBQXsmzEDp93e2CGJiIiIiIjUi0ZPsB9//HHGjx/PTTfdRKdOnXjuuecIDAzkxRdfrPEawzCIjY11bzExMe5jTqeTpUuXMmvWLK688ko6d+7MSy+9RGZmJmvWrGmAJ5LKDB8f4hf9FVNgIMXffMuhvy1v7JBERERERETqRaMm2FarlW+//ZYBAwa495lMJgYMGMCmTZtqvK6goIAWLVqQlJTElVdeyU8//eQ+tmfPHrKzsz3aDAsLo3fv3sdtU+qPb1ISMbNmAXDgyScp3vq/Ro5IRERERESk7vk05s0PHjyI3W73qEADxMTE8PPPP3u9pn379rz44ot07tyZ3NxcFi9ezEUXXcRPP/1EYmIi2dnZ7jaqtllxrKrS0lJKS0vdn/PKl5Wy2WzYbLaTfr76VBFXU42vqsA/Xk7QRx9RuG4d+6ZNJWn1akyBgY0dltSB0+1dlDOb3kdpKvQuSlOhd1GaktPhfTzV2Bo1wT4ZF154IRdeeKH780UXXUTHjh15/vnnmT9//km1uXDhQubOnVtt/3//+18Cm3gSuG7dusYO4YSZLrqQFps3w97f+O6uu8gZObKxQ5I6dDq9i3Lm0/soTYXeRWkq9C5KU9KU38eioqJTur5RE+yoqCjMZjP79+/32L9//35iY2NPqA2LxUK3bt3YuXMngPu6/fv3ExcX59Fm165dvbZx3333MWXKFPfnvLw8kpKSGDRoEKGhobV5pAZjs9lYt24dAwcOxGKxNHY4J6woMZHM8bcS/uVmOo4eTdAllzR2SHKKTtd3Uc5Meh+lqdC7KE2F3kVpSk6H97GiN/PJatQE29fXlx49erBhwwZGjBgBgMPhYMOGDUycOPGE2rDb7WzdupVhw4YB0KpVK2JjY9mwYYM7oc7Ly2Pz5s3cfvvtXtvw8/PDz8+v2n6LxdJk/8NXOB1irCysXz9KUlM5vHIlOQ/OofVb/8EnKqqxw5I6cLq9i3Jm0/soTYXeRWkq9C5KU9KU38dTjavRZxGfMmUKL7zwAqtWrWL79u3cfvvtFBYWctNNNwEwZswY7rvvPvf58+bN47///S+7d+/mu+++409/+hO//fYbt9xyC+CaYXzy5MksWLCAt956i61btzJmzBji4+PdSbw0ruZT7savfXvshw+TOXMmTqezsUMSERERERE5ZY0+BnvUqFEcOHCABx54gOzsbLp27cr777/vnqQsLS0Nk+nY7wGOHDnC+PHjyc7OJiIigh49evDFF1/QqVMn9znTp0+nsLCQW2+9laNHj9K3b1/ef/99/P39G/z5pDqTry8Jix9lz9XXUPjxJxx57TUib7yxscMSERERERE5JY2eYANMnDixxi7hGzdu9Pi8ZMkSlixZctz2DMNg3rx5zJs3r65ClDrml5JC9NSp7H/4YXL+uoig3r3xa9OmscMSERERERE5aY3eRVzOXhF/Gk1Qnz44S0vZN20aTqu1sUMSERERERE5aUqwpdEYJhNxCx/GHB5O6bbtHHjyycYOSURERERE5KQpwZZGZYmOJna+qyv/ob8tp3DzV40ckYiIiIiIyMlRgi2NLnTgQMKuuRqcTjLvvRd7bm5jhyQiIiIiIlJrSrClSYi97z4sLZIpy8oie978xg5HRERERESk1pRgS5NgCgoiYdEiMJvJe/ddct9+u7FDEhERERERqRUl2NJkBHTpQtSddwCQPXce1ox9jRyRiIiIiIjIiVOCLU1K1K23EtCtG46CAjLvnYHTbm/skERERERERE6IEmxpUgwfH+IX/RVTUBDF33zLob8tb+yQRERERERETogSbGlyfJOSiJk1C4ADTz5J8db/NXJEIiIiIiIiv08JtjRJYSOuJGTIECgrI3PaNBxFRY0dkoiIiIiIyHEpwZYmyTAM4uY8iE9MDNa9e9n/10WNHZKIiIiIiMhxKcGWJsscHk78IwsBOLp6NfkfftTIEYmIiIiIiNRMCbY0aUEXXkjkTTcBkDVrFmUHDzZyRCIiIiIiIt4pwZYmr/ndk/Fr3x774cNkzpyJ0+ls7JBERERERESqUYItTZ7J15eExY9i+PpS+PEnHHnttcYOSUREREREpBol2HJa8EtJIXrqVABy/rqI0l27GjkiERERERERT0qw5bQR8afRBPXti7O0lH3TpuG0Whs7JBERERERETcl2HLaMEwm4h5+CHN4OKXbtnNg2bLGDklERERERMRNCbacVizR0cQtmA/AoeUvUrj5q0aOSERERERExEUJtpx2QgYMIPzaa8DpJPPee7Hn5jZ2SCIiIiIiIkqw5fQUc++9WFokU5aVRfbceVq6S0REREREGp0SbDktmYKCSFi0CMxm8tauJe/ttxs7JBEREREROcspwZbTVkCXLkTdeQcA2fPmY83Y18gRiYiIiIjI2UwJtpzWom69lYBu3XAUFJB57wycdntjhyQiIiIiImcpJdhyWjN8fIh/dBGmoCCKv/mWQy/8rbFDEhERERGRs5QSbDnt+SYmEjNrFgAHnnqK4q3/a+SIRERERETkbKQEW84IYSOuJGTIECgrI3PaNBxFRY0dkoiIiIiInGV8GjsAkbpgGAZxcx6k+Pvvse7dy/6/LiJu7pw6afsvf/kLa9asITc3l5CQEK699loWLVqEr6+vx3lpaWl06tTJY19JSQnDhg3jrbfeAmDatGksX76cpKQkXnvtNff5u3fv5qqrruLLL7/E39+/TuIWaYocDgdWq7Xe2rfZbPj4+FBSUoJdczJII2qId9FisWA2m+ulbREROTlKsOWMYQ4PJ/6RhaTddDNHV68m+OI/ENK//ym3e8cdd/DII48QFBTEwYMH3Qn2rPJu6RWSk5MpKChwf7ZarcTHx3P99dcD8PXXX7NmzRr27t3LypUrmTFjBm+XLy92xx138Pjjjyu5ljOa1Wplz549OByOeruH0+kkNjaW9PR0DMOot/uI/J6GehfDw8OJjY3V+y4i0kQowZYzStCFFxJ5000cXrGCrJmzCHjrP/g0b35KbXbs2NH9vdPpxGQy8euvv/7udWvWrMHhcHDVVVcBrip1z549CQ0NZdCgQTz33HMAvPrqq8TGxtK/Dn4ZINJUOZ1OsrKyMJvNJCUlYTLVzwglh8NBQUEBwcHB9XYPkRNR3++i0+mkqKiInJwcAOLi4ur8HiIiUntKsOWM0/zuyRRu2kTpzz+TOWsWSc89d8q/2X/kkUdYsGABhYWFNGvWjL/+9a+/e83y5csZPXq0uyp97rnnMmvWLI4ePcr69es577zzOHLkCA8//DAff/zxKcUn0tSVlZVRVFREfHw8gYGB9Xafii7o/v7+SrClUTXEuxgQEABATk4O0dHR6i4uItIE6F8fcsYx+fqS8OgiDF9fCj/+hCOvvnrKbd57770UFBSwbds2brvtNmJjY497/m+//cb69eu55ZZb3PvOOecc7rrrLi655BI++OADFi9ezLRp05gxYwbbtm2jf//+XHbZZXz22WenHK9IU1MxBrXq3AUicmoqfmFls9kaORIREQEl2HKG8ktJIXrqVAByFj1K6c6dddJux44d6dKlC6mpqcc9b8WKFXTr1o0uXbp47J84cSJbtmzh7bffZs+ePaSlpTF69GhuvPFG/va3v/F///d/jB49GqfTWSfxijQ1GicqUrf0/5SISNOiBFvOWBF//hNBffviLC1l37TpOOto5mKbzXbcMdgOh4MVK1Z4VK+rslqtTJ48mWeeeYYDBw5QVlZG69atadOmDVarlQMHDtRJrCJyenE6ndx6661ERkZiGAbh4eFMnjy5scM6YampqYwYMaKxwxAREWk0GoMtZyzDMIh7+CH2XDmC0u3bObBsmbuqfaIKCgr45z//yciRIwkLC+N///sfCxYsYPDgwTVes27dOg4ePMgNN9xQ4zkLFy7k2muvpW3bttjtdkpLS/nhhx8wDAOr1UqzZs1qFaeInBnef/99Vq5cycaNG2ndujUmk8k9zraupKamcvToUdasWVOn7QI88cQT6oEjIiJnNVWw5YxmiY4mbv48AA4tf5HCzV/V6nrDMHj11Vdp06YNISEhXHnllVx++eUsXboUgKFDh/Lwww97XLN8+XKuueYawsLCvLa5Y8cO3n77baaWJ/tms5lnn32WoUOHMnToUJ5//nlNVCNyltq1axdxcXFcdNFFxMbGEh0dTUhISGOHdcLCwsIIDw9v7DBEREQajRJsOeOFDBhA+LXXgNNJ5owZ2HNzT/jaoKAg1q1bx6FDhygoKGD37t08+uij7kll3nvvPe6//36Pa/7xj3+watWqGtts374933zzDRaLxb1v1KhRZGZmsm/fPq655ppaPqGInAlSU1P5y1/+QlpaGoZh0LJlSy655BKPLuItW7bk4Ycf5uabbyYkJITk5GT+7//+z6Od9PR0rrvuOsLDw4mMjOTKK69k7969AMyZM4dVq1bxn//8B8MwMAyDjRs3snHjRgzD4OjRo+52tmzZgmEY7mtXrlxJeHg4H3zwAR07diQ4OJghQ4aQlZXl8QyVu4hfcsklTJo0ienTpxMZGUlsbCxz5szxiPfnn3+mb9+++Pv706lTJ9avX49hGPVSYRcREalvSrDlrBBz771YWiRTlp1N1py5fLV5M/fccw+pqalMmjSJdevW4XA4GjtMETmLPfHEE8ybN4/ExESysrL4+uuvvZ732GOP0bNnT77//nvuuOMObr/9dnbs2AG45ogYPHgwISEhfPrpp3z++efuRNhqtTJ16lSuu+46d2KclZXFRRdddMIxFhUVsXjxYl5++WU++eQT0tLS3L1xarJq1SqCgoLYvHkzixYtYt68eaxbtw5wzS4/YsQIAgMD2bx5M//3f//HzJkzTzgeERGRpkYJtpwVTEFBJDz6KJhM5L/3HgsGDeLll1/m66+/5l//+heDBg2iY8eOfPnll40dqoicpcLCwggJCcFsNhMbG0vz5s29njds2DDuuOMO2rZty4wZM4iKiuKjjz4CYPXq1TgcDv72t79x3nnn0bFjR1asWEFaWhobN24kODiYgIAA/Pz8iI2NJTY2tlZLp9lsNp577jl69uxJ9+7dmThxIhs2bDjuNZ07d+bBBx8kJSWFMWPG0LNnT/c169atY9euXbz00kt06dKFvn378tBDD51wPCIiIk2NJjmTs0ZmUBDLCwoYFxjI/OQWZE69B2fz5jidTn766SdWrlxJ//792bBhAxdeeGFjhysi4lXnzp3d3xuGQWxsLDk5OQD88MMP7Ny5s9q47ZKSEnbt2nXK9w4MDKRNmzbuz3Fxce57n0i8Va/ZsWMHSUlJxMbGuo+ff/75pxyniIhIY1EFW84ad955J6+X2ShISsTXZiP29dXgcGAYBueeey4PPfQQLVu2ZOzYseouLiJNVuX5G8CVZFf8mVVQUECPHj3YsmWLx/bLL79w44031timyeT650DlGcBtNtsJ3fv3Zg0/XrwiIiJnGiXYclbYuXMnH3zwAdddfz1HxozB4eeH/549hH34ofscPz8/UlNT+fXXX/mw0n4RkdNF9+7d+fXXX4mOjqZt27YeW8XKBr6+vtjtdo/rKrqjV56wbMuWLfUeb/v27UlPT2f//v3ufTWNPRcRETkdKMGWs8I777yDn58f/fr1o6xZMw5ddRUAEe9/QPh77xH8zTf47dlD56Qk4uPj+c9//tPIEYuI1N7o0aOJioriyiuv5NNPP2XPnj1s3LiRSZMmkZGRAbhmIv/xxx/ZsWMHBw8exGaz0bZtW5KSkpgzZw6//vor7777Lo899li9xztw4EDatGnD2LFj+fHHH/n888+ZNWsW4Kp0i4iInG40BlvOCnl5eQQFBeHn5wdAQc8eBGzbRvAPPxCxbr3Hue+EhJL/2eek33EnvkmJWBKTsCQl4pucjCUhAVN5GyIiTU1gYCCffPIJM2bM4KqrriI/P5+EhAQuu+wyQkNDARg/fjwbN26kZ8+eFBQU8NFHH3HJJZfw2muvcfvtt9O5c2d69erFggULuPbaa+s1XrPZzJo1a7jlllvo1asXrVu35tFHH2X48OH4+/vX671FRETqg+H8vcFTZ6G8vDzCwsLIzc11/4OkqbHZbKxdu5Zhw4ZVG98m1T399NNMnjyZV199laCgIAAMq42QLzfhm5WFz8FDWA4fxnz0KMbv/C/hExPjSriTksu/JmFJdCXg5sjIs67qondRTkRJSQl79uyhVatW9Zo4ORwO8vLyCA0NdY8rltPL559/Tt++fdm5c6fHhGqnm4Z6Fxvq/y05fenvaWlKTof38VRzQVWw5awwcuRIJk+ezIYNG7jiiisAcPpayPvDHzzO+2bTJl7861/51zPP0CowEFt6BraMdKxp6djS03EUFVG2fz9l+/dT/M231e5jBAbim5iIJSnJlXi7E/AkLIkJmGqxHI6IyNngzTffJDg4mJSUFHbu3Mldd91Fnz59TuvkWkREzl5KsOWsEB8fz1VXXcXq1avp2bMn8fHx1c7Jzc1l+UsvEd2lC93HjatWiXY6ndiPHMGWXp5wZ6RjTc9wfU5Pp2z/fpxFRZT+8gulv/xSPQjDwCc29lgCnuxKvH2TXJ/NERFnXfVbRCQ/P58ZM2aQlpZGVFQUAwYMaJDx3yIiIvVBCbacNZ5++mn69OnDtGnTGDVqFJdddhlBQUGUlpby6aefsnr1aqxWK6+88orXRNcwDHwiI/GJjCSgS5dqxx2lpdj2ZXpUvK0ZxxJwZ3ExZVlZlGVlgZdZck1BQeWVb9e4b48EPD4eQ9VvETkDjRkzhjFjxjR2GCIiInVCCbacNaKiovj888+58847Wb58OcuXLycsLIyCggJKS0sZNGgQzzzzzEl3SzT5+eHXuhV+rVtVO+Z0OrEfOoQ1PR1bRobra1o61ox0bOkZlO3fj6OwkNKff6b055+9NG7CEhuLpaLbeUUCXj7+2xweruq3iIiIiEgjU4ItZ5WoqChWr15NVlYW//73vzl48CChoaEMHz6ctm3b1tt9DcPAJyoKn6go6Nat2nFHSQm2fftciXflcd/l3dCdJSXYMjOxZWbC5s3VrjcFB2NJTsI3sdK474px4HFxGE10EgkRERERkTOJEmw5K8XFxXHnnXc2dhhuJn9//Nq0wc9L9dzpdGI/eLA8+fYc921LT6fswAEcBQWUbttO6bbtXho3YYmLq5SAV4z7TsY3KRFzWFgDPKGIiIiIyJlPCbZIE2cYBj7Nm+PTvDl0717tuKO42FX99jLxmi0jA2dpKbZ9+7Dt20cRX1a73hQa6pp4LTn52PjvJNdnS2wsho/+mBARERERORH6l7PIac4UEIBf27b4eeni7nQ4KDtwsMrEa65u6Nb0dOwHD+LIy6Nk2zZKtm2r3rjZjCU+3qPibanohp6cjDkkpAGeUERERETk9KAEW+QMZphMWGKiscREE9ijR7XjjqIi10znGRlY09JciXdGxTjwDJxWK7byruiwqdr15rAwfBITiTObOfTrr/i1aHFs3e+4WAyzuQGeUkRERESkaVCCLXIWMwUG4t+uHf7t2lU75nQ4KMvJcY/7tqa7EvCK5cfshw5hz83FnptLCHDkxx89G/DxwZIQX2nitWSPCdjMwcEN85AiTdAll1xC165dWbp0aZ22u3LlSiZPnszRo0drPGfOnDmsWbOGLVu2AJCamsrRo0dZs2ZNncbS0ObMmcOzzz5LTk4Ob775JmvWrGnw5zqRn7+IiJzZlGCLiFdGxdJgsbEE9upV7bi9oBDbvgyK9+zlx3XrSAkOpiwz05WA79sHNhu239Kw/ZbmtX1zRIRrwrXERNfXSut++8TEqPot0kCeeOIJnE5nY4dxSrZv387cuXN58803ueCCC4iIiODSSy895edq2bIlkydPZvLkyXUTaC3V1y9iRESk/ijBFpGTYg4Owty+PebWrTlaWkLzYcOwlC8H5rTbKdu/3zXhWkZ6pXW/XRVw+5Ej7q2kauUbMCwWLAkJx2Y8r7zud0Ii5uCghn5ckTNWWBNcSSA1NZWWLVsyZ86cEzp/165dAFx55ZUYhgGAn5/fca+xWq34+vqeUpwiIiJVmRo7ABE58xjlk6MF9T6f8KuvJnryZBIef4xW/1hNu01f0O6br2n15r9JWPYE0dOmEX7D9QT17YulRTL4+OC02bDu3Uvhp59y5NXXyFm0iIyJf2HPlSP4pWdPfrmoD3tHXc++qdPIeeIJjv7r3xR9/TW27GycDkdjP76cQRwOBwsXLqRVq1YEBATQpUsX3njjDffxjRs3YhgGH3zwAd26dSMgIID+/fuTk5PDe++9R8eOHQkNDeXGG2+kqKjIo+2ysjImTpxIWFgYUVFRzJ4926PiWlpaytSpU0lISCAoKIjevXuzceNGjzZWrlxJcnIygYGBjBw5kkOHDlV7hkceeYSYmBhCQkIYN24cJSUlHsdTU1MZMWKE+/Mll1zCpEmTmD59OpGRkcTGxlZLdH/++Wf69u2Lv78/nTp1Yv369RiG4e6ObbVamThxInFxcfj7+9OiRQsWLlxYi5/8iZszZw7Dhw8HwGQyuRNsb881ceJEJk+eTFRUFIMHD8bpdDJnzhySk5Px8/MjPj6eSZMmuc//7bffuPvuuzEMw91ubezZs4cRI0YQExNDcHAwvXr1Yv369R7nPPPMM6SkpODv709MTAzXXHONO/6PP/6YJ554wn3/vXv3nsRPSEREGpIq2CLS4MzBwZg7dsS/Y8dqx5x2O2XZ2Z7jvivNgm7PzcV++DDFhw9T/MMP1a43LBYsiYmV1v0+Nu7bNzERU2BgQzyi/A6n00mxzV7n7TocDoqtdnysZZhM3n+HHGAxn3CytHDhQv7+97/z3HPPkZKSwieffMKf/vQnmjdvzsUXX+w+b86cOTz11FMEBgZy3XXXcd111+Hn58err75KQUEBI0eO5Mknn2TGjBnua1atWsW4ceP46quv+Oabb7j11ltJTk5m/PjxAEycOJFt27bx+uuvEx8fz5tvvsmQIUPYunUrKSkpbN68mXHjxrFw4UJGjBjB+++/z4MPPugR/z/+8Q/mzJnD008/Td++fXn55ZdZtmwZrVu3Pu5zr1q1iilTprB582Y2bdpEamoqffr0YeDAgdjtdkaMGEFycjKbN28mPz+fe+65x+P6ZcuW8dZbb/GPf/yD5ORk0tPTSU9PP6GfeW1NnTqVli1bctNNN5GVlfW7z3X77bfz+eefA/Cvf/2LJUuW8Prrr3POOeeQnZ3ND+V/rvz73/+mS5cu3Hrrre7/JrVVUFDA0KFDefjhh/Hz8+Oll15i+PDh7Nixg+TkZL755hsmTZrEyy+/zEUXXcThw4f59NNPAVfX/V9++YVzzz2XefPmAdC8efOTikNERBqOEmwRaVIMs9nVPTwhgaALelc7bs/Lc816np6BLT3t2LrfGRnYMjNd1e89e7Du2UOhl/bNUVHl635XJOBJ7mXIfJpHYdSQlEndKrbZ6fTAB41y723zBhPo+/t//ZWWlvLwww+zfv16LrzwQgBat27NZ599xvPPP++RYC9YsIA+ffoAMG7cOO677z527drlTmSvueYaPvroI48EOykpiSVLlmAYBu3bt2fr1q0sWbKE8ePHk5aWxooVK0hLSyM+Ph5wJZLvv/8+K1as4OGHH+aJJ55gyJAhTJ8+HYB27drxxRdf8P7777vvsXTpUsaNG8e4cePcca5fv75aFbuqzp07u5P1lJQUnnrqKTZs2MDAgQNZt24du3btYuPGjcTGxgLw0EMPMXDgQPf1aWlppKSk0LdvXwzDoEWLFr/78z5ZwcHBhIeHA7jjqUlKSgqLFi1yf3733XeJjY1lwIABWCwWkpOTOf/88wGIjIzEbDYTEhLyu+3W5LzzzqNPnz7uX/bMnz+fN998k7feeouJEyeSlpZGUFAQf/zjHwkJCaFFixZ069YNcHXd9/X1JTAw8KTvLyIiDU8JtoicVsyhoZg7dcK/U6dqx5xlZdiys8tnPk93z4BekYA7cnOxHzxI8cGDFJfPoFyZ4eeHJTGxPAE/tu6362sipoCABnhCaSp27txJUVGRR+IIru7PFUlQhc6dO7u/j4mJITAw0KNKHBMTw1dffeVxzQUXXOBRSb/wwgt57LHHsNvtbN26FbvdTrsqM/yXlpbSrFkzwDWx18iRIz2OX3jhhR4J9vbt27ntttuqnfPRRx8d99krPw9AXFwcOTk5AOzYsYOkpCSPpK8iKa2QmprKwIEDad++PUOGDOGPf/wjgwYNqvF+r7zyChMmTPB4TsMwWLx4sXvfe++9R79+/Y4b9+/pUWW5wmuvvZalS5fSunVrhgwZwrBhwxg+fDg+PnXzz6OCggLmz5/P2rVrycrKoqysjOLiYtLSXJM/Dhw4kBYtWrjvP2TIEEaOHEmgetqIiJy2lGCLyBnD8PHBtzxBDiqvOFZmz831MvFa+brfWVk4S0ux7tqFtXzCpKp8mjf3qHi7viZhSUzEp3nzkxqjebYKsJjZNm9wnbfrcDjIz8snJDTkuF3ET0RBQQHgqnImJCR4HKs6gVbFBH8AhmF4fK7Y56jF/AAFBQWYzWa+/fZbzFVm1A9ugCXuTjX+7t27s2fPHt577z3Wr1/Pddddx4ABAzzGr1d2xRVX0Lv3sR4rM2bMICEhwT0eGqj23+BkBAV5TpCYlJTEjh07WL9+PevWreOOO+7g0Ucf5eOPP672MzgZs2fP5pNPPmHx4sW0bduWgIAArrnmGqxWKwAhISF89913bNy4kf/+97888MADzJkzh6+//tpdlRcRkdOLEmwROWuYw8IICAsj4Nxzqh1z2mzYsrLKK9/Hxn1bM1yJuKOggLIDByg7cIDi776rdr3h7++ueFdb9zshAZO/f0M84mnDMIwT6qZdWw6HgzJfM4G+PjUm2CeqU6dO+Pn5kZaW5tEdvK5s3rzZ4/OXX35JSkoKZrOZbt26YbfbycnJqbFq27FjR69teDtnzJgxNZ5TW+3btyc9PZ39+/cTExMDwNdff13tvNDQUEaNGsWoUaO45pprGDJkCIcPHyYyMrLauSEhIYSEhHh8joyMpG3btqcU64kICAhg+PDhDB8+nDvvvJMOHTqwdetWunfvjq+vL3b7yc8VsHnzZsaOHevuaVBQUFBtojIfHx8GDBjAgAEDePDBBwkPD+fDDz/kqquuOuX7i4hIw1OCLSKCa3I03+RkfJOTqx1zOp04cnM9u51XmnjNlp2Ns6SE0l93UvrrTq/t+0RHe068lpzs6o6elIS5WTNVv5ugkJAQpk6dyt13343D4aBv377k5uby+eefExoaytixY0+p/bS0NKZMmcKECRP47rvvePLJJ3nssccA13jq0aNHM2bMGB577DG6devGgQMH2LBhA507d+byyy9n0qRJ9OnTh8WLF3PllVfywQcfeHQPB7jrrrtITU2lZ8+e9OnTh1deeYWffvrpdyc5O56BAwfSpk0bxo4dy6JFi8jPz2fWrFkA7vf48ccfJy4ujm7dumEymfjnP/9JbGxsk6vKrly5ErvdTu/evQkMDOTvf/87AQEB7jHjLVu25JNPPuH666/Hz8+PqKioWrXfpk0b3nzzTa644goMw2D27NkePQHeeecddu/ezR/+8AciIiJYu3YtDoeD9u3bu++/efNm9u7dS3BwMJGRkaf8iyMREalfSrBFRH6HYRiYw8MJCA8n4Lzzqh13Wq2u6ndaenn384xj48DT0nAUFVGWk0NZTg7F33xbvf3AQNe47/KZzit3Q7ckJmDSWr2NZv78+TRv3pyFCxeye/duwsPD6d69O/fff/8ptz1mzBiKi4s5//zzMZvN3HXXXdx6663u4ytWrGDBggXcc8897Nu3j6ioKC644AL++Mc/Aq4x3C+88AIPPvggDzzwAAMGDGDWrFnMnz/f3caoUaPYtWsX06dPp6SkhKuvvprbb7+dDz44+QnmzGYza9as4ZZbbqFXr160bt2aRx99lOHDh+Nf3lMjJCSERYsW8euvv2I2m+nVqxdr165tcslheHg4jzzyCFOmTMFut3Peeefx9ttvu8e5z5s3jwkTJtCmTRtKS0s9llE7EQ899BCTJ0/moosuIioqihkzZpCXl+dx/3//+9/MmTOHkpISUlJSeO211zjnHFcvm6lTpzJ27Fg6depEcXExe/bsoWXLlnX2/CIiUvcMZ23/tjgL5OXlERYWRm5uLqGhoY0djlc2m421a9cybNiwOhknJnKy9C4en9PpxH70qCvhdifgrm7o1vR0yrKz4Xh/DBsGPjEx1SdeS3bNgG6OiDgtqt8lJSXs2bOHVq1auZOw+uBwOMjLyyM0NLTJJXNnss8//5y+ffuyc+dO2rRp09jhNAkN9S421P9bcvrS39PSlJwO7+Op5oKqYIuI1CPDMPCJiMAnIoKAKjMzAzisVmz79rmWHktzrftdMfGaNT0dZ1ERZdnZrkT8m2+qXW8KDHRVvJOTKo3/dk285puQgKHqt9SDN998k+DgYFJSUti5cyd33XUXffr0UXItIiJnvSaRYD/99NM8+uijZGdn06VLF5588slqS3548/rrr3PDDTdw5ZVXsmbNGvf+1NRUVq1a5XHu4MGDq41NExFpbCZfX/xatcKvVatqx5xOJ/bDh93jvq3prgS8YtmxsuxsHEVFlO7YQemOHdUbNwx84mKPjfuuPPFaUhLm8PDTovotTU9+fj4zZswgLS2NqKgoBgwY4B4/fiYbOnQon376qddj999/f50MHRARkdNboyfYq1evZsqUKTz33HP07t2bpUuXMnjwYHbs2EF0dHSN1+3du5epU6fWOLvqkCFDWLFihftz1SVVRESaOsMw8GnWDJ9mzQjo2rXacUdpqav6nV5l3Hd5Au4sLqYsM4uyzCyosgYzgCk42HPcd3kV3DcpEUtcnKrfUqMxY8Z4zEx+tvjb3/5GcXGx12PeZkcXEZGzT6Mn2I8//jjjx4/npptuAuC5557j3Xff5cUXX+Tee+/1eo3dbmf06NHMnTuXTz/9lKNHj1Y7x8/Pj9jY2PoMXUSkUZn8/PBr3Ro/LzNCO51O7IcOeR33bUtPpywnB0dBAaXbt1O6fbuXxk1Y4uKOTbiW6JmAm5vYbNAiDaEu1uIWEZEzW6Mm2FarlW+//Zb77rvPvc9kMjFgwAA2bdpU43Xz5s0jOjqacePG1dhVa+PGjURHRxMREUH//v1ZsGCBe1bQqkpLSyktLXV/rpjh02azYbPZTubR6l1FXE01Pjl76F1swsLCsJwXhuW8cwmscshRUkJZZqZrmbGMDPdWlu766qyoju/bR5GXZZNNISFYEhPxSUzEUrGVj/32iY3BqDJxic1mcy135nB4LFNU1yrm7ay4l0hjaah30eFw4HQ6sdlsmM3meruPnL7097Q0JafD+3iqsTVqgn3w4EHsdjsxMTEe+2NiYvj555+9XvPZZ5+xfPlytmzZUmO7Q4YM4aqrrqJVq1bs2rWL+++/n6FDh7Jp0yavf/ksXLiQuXPnVtv/3//+l8DAqv8sbVrWrVvX2CGIAHoXT2sREa6tYgkypxNzfj6Ww4exHDqE7+HDru8PH8Zy6DA++fk48vNrrH47TSZs4eHYIiNdW7NI7ElJhPXtS0FeHlartd4fKT8/v97vIXIi6vtdtFqtFBcX88knn1BWVlav95LTm/6elqakKb+PRUVFp3R9o3cRr438/Hz+/Oc/88ILLxAVFVXjeddff737+/POO4/OnTvTpk0bNm7cyGWXXVbt/Pvuu48pU6a4P+fl5ZGUlMSgQYOa9DJd69atY+DAgU12ins5O+hdPPs4ioqwZWZSlpFRXgHfd6wCnpEBViu+hw/je/jwsWvi4rB36YJvTg7+FguGxeIa4+3r6/7eKP/+VDidTvLz8wkJCdEEbtKoGupdLCkpISAggD/84Q9apku80t/T0pScDu9jRW/mk9WoCXZUVBRms5n9+/d77N+/f7/X8dO7du1i7969DB8+3L2votuVj48PO3bs8LpESOvWrYmKimLnzp1eE2w/Pz+vk6BZLJYm+x++wukQo5wd9C6eRcLC8AsLg44dqx1yOhyUHThQaeK1NKzpGZQUFlJYvhaw027HabdDSUn1tg3DI+E2VU6+fX0xfqcLbMXfCYZhaB1saVQN9S6aTCYMw9CfwfK79I5IU9KU38dTjatRE2xfX1969OjBhg0bGDFiBOD6C2nDhg1MnDix2vkdOnRg69atHvtmzZpFfn4+TzzxBElJSV7vk5GRwaFDh4iLi6vzZxARkWMMkwlLTAyWmBgCe/Z07y8pKWHPnj34JSfjZzbjtFrdm8NqxWm14bRZwel07wewV23fbPZMuOuw+i0iIiJyqhq9i/iUKVMYO3YsPXv25Pzzz2fp0qUUFha6ZxUfM2YMCQkJLFy4EH9/f84991yP68PLZ7Kt2F9QUMDcuXO5+uqriY2NZdeuXUyfPp22bdsyePDgBn02ERHxZJjNmPz9wUtXVqfTidNmc22VEnBnRQJuL3NVv4uLwdtSSeXVb4thoqywEJOfn2cS3oQmgLrkkkvo2rUrS5curdN2V65cyeTJk72urlFhzpw5rFmzxj2XSWpqKkePHmXNmjV1GktDOhOeQUREzgyNnmCPGjWKAwcO8MADD5CdnU3Xrl15//333ROfpaWl1aprldls5scff2TVqlUcPXqU+Ph4Bg0axPz587UWtohIE2YYhntcNkFB1Y477XZXsl0pAXdUJOA2m7v6bQLspSVeqt8+GL5VupxXJOAWy1k7ZvuJJ55wz3gtIiIip6bRE2yAiRMneu0SDq7lto5n5cqVHp8DAgL44IMP6igyERFpKgyzGSMgAAICqh2rqH47rFaKcnMJMJs9K+F2u6sCXlx2nOq3L4avBZO3BLwJVb/rWlhYWGOHUE1qaiotW7Zkzpw5DXI/p9OJ3W7Hx6dJ/LNIREROY5oBRkRETnuGYWDy9cUUGIgjKAhzdDS+SUn4tWmDf8eO+HfsiF+bNvgmJWGJicEcGYkpONhVMTeM8up3KY6CAsoOH8aWnY01LY3iX35h/j330DIxkQB/fzp36sTrf/sbZUeOYC8s5MP16zEMgw8++IBu3boREBBA//79ycnJ4b333qNjx46EhoZy4403Vlv2o6ysjIkTJxIWFkZUVBSzZ8/2qCSXlpYydepUEhISCAoKonfv3tV+6bxy5UqSk5MJDAxk5MiRHDp0qNrP5pFHHiEmJoaQkBDGjRtHSZXJ5VJTU93zoICr+/qkSZOYPn06kZGRxMbGVkt0f/75Z/r27Yu/vz+dOnViffnPoaKLttVqZeLEicTFxeHv70+LFi1YuHBh7f/DnqTS0lImTZpEdHQ0/v7+9O3bl6+//tp9fOPGjRiGwXvvvUePHj3w8/Pjs88+Y9euXVx55ZXExMQQHBxMr169WL9+fYPFLSIipz/9qlZERM54J1L99hjzXf550bJlvPb22yybPZu2ycl89u23pN55J5FmM/169cKWng7AA/fey5JZswgMCWH0bbdx7dVX4+fvzyt//zuFRUWMHDmSJ598khkzZrjvu2rVKsaNG8dXX33FN998w6233kpycjLjx48HXL27tm3bxuuvv058fDxvvvkmQ4YMYevWraSkpLB582bGjRvHwoULGTFiBO+//z4PPvigx7P94x//YM6cOTz99NP07duXl19+mWXLltG6devj/rxWrVrFlClT2Lx5M5s2bSI1NZU+ffowcOBA7HY7I0aMIDk5mc2bN5Ofn88999zjcf2yZct46623+Mc//kFycjLp6emkl/+sGsL06dP517/+xapVq2jRogWLFi1i8ODB7Ny5k8jISPd59957L4sXL6Z169ZERESQnp7OsGHDeOihh/Dz8+Oll15i+PDh7Nixg+Tk5AaLX0RETl9KsEVEpOE5nWAr+v3zasvhcLVrNUNN83dYAl1V63IeY78rKS0tZdHf/sa6Dz7ggh49cFqttOvZky9/+onla9bwhz593Oc+eOed9G7fHoAxV1zBA088wU9r19LKzw8jKIiRgwax4b33mJJ6E4avBafdTlJiIo8//jgmk4n27duzdetWlixZwvjx40lLS2PFihWkpaURHx8PwNSpU3n//fdZsWIFDz/8ME888QRDhgxh+vTpALRr144vvviC999/3x3X0qVLGTduHOPGjQNgwYIFrF+/vloVu6rOnTu7k/WUlBSeeuopNmzYwMCBA1m3bh27du1i48aN7iU1H3roIQYOHOi+Pi0tjZSUFPr27YthGLRo0eK496tLhYWFPPvss6xcuZKhQ4cC8MILL7Bu3TqWL1/OtGnT3OfOmzfPI+7IyEi6dOni/jx//nzefPNN3nrrrRqHsomIiFSmBFtERBqerQgejq/zZk1A+O+ddH8m+FafRK2qnTt3UlRUxKAhQzz2W61WunXrhn+7dvju2wdA98suwxIWhtNqJTYxkcCAAFq3aInTYcdZVkbzsDC+3rKFsgM5ADhLSujZoQOl2392j/vu1b49j/36K9YjR/jh22+x2+20a9fO496lpaU0a9YMgO3btzNy5EiP4xdeeKFHgr19+3Zuu+22aud89NFHx332zp07e3yOi4sjJ8cV+44dO0hKSnIn1wDnn3++x/mpqakMHDiQ9u3bM2TIEP74xz8yaNCgGu/3yiuvMGHCBI/nNAyDxYsXu/e999579OvX77hxA+zatQubzUafSr8AsVgsnH/++Wzfvt3j3J6VlpID10okc+bM4d133yUrK4uysjKKi4tJS0v73fuKiIiAEmwRERGvCgoKAHj33XdJSEjwOFaxKkXFzOP+ERH4lC8baYmKwuLri3+njjjLynBabfiEheE0mTBHRLjW+K6ooDsdOEtLsZeWYs/PB8CalsbRXbswm8188c9/4uPnBxVLjVkshISHu2ZNr0eWKmuKG4aBw+E44eu7d+/Onj17eO+991i/fj3XXXcdAwYM4I033vB6/hVXXEHv3r3dn2fMmEFCQgKTJk1y76v636AuBFWZrX7q1KmsW7eOxYsX07ZtWwICArjmmmuwlq/LLiIi8nuUYIuISMOzBLoqyXXM4XCQl59PaEhIzUs8WgJPqK1OnTrh5+dHWloaF1988UnFY/j4YPj4YPL3x/Dxwbc8STQFBvLtL7/gl5LiWuPbZuWbX3+lbcuWWIKC6NqpE3a7nZycHPr06OHZaFERJTt20C4+nk0ffoh17Fj3jOebPv0UAKfDgWEy0bFjRzZv3syYMWPcl3/55Zcn9SwV2rdvT3p6Ovv373cvqVl5ArEKoaGhjBo1ilGjRnHNNdcwZMgQDh8+7DEGukJISAghISEenyMjI2nbtm2t42vTpg2+vr58/vnn7q7pNpuNr7/+msmTJx/32s8//5zU1FR3z4CCggL27t1b6xhEROTspQRbREQanmGcUDftWnM4wGJ3tV1Tgn2CQkJCmDp1KnfffTcOh4O+ffuSm5vL559/TmhoKGPHjj2l9tPS0ph6331MmDCB7777jmdWrOCxxx7Dr21bzm3ThtE33MD4Bx9k0fz5dOnQkQP7s/nw4485t21bhvTpwx033kj/MWN47Mkn+eOll7Lu88/54L//BaBk2zYMi4U7rr+e8dOn061dOy7q04fX//1vfvrpp9+d5Ox4Bg4cSJs2bRg7diyLFi0iPz+fWbNmAccq+o8//jhxcXF069YNk8nEP//5T2JjYwkvr/LXp6CgIG6//XamTZtGZGQkycnJLFq0iKKiIvdY9JqkpKTw73//m+HDh2MYBrNnz65V5V5EREQJtoiISA3mz59P8+bNWbhwIbt37yY8PJzu3btz//33n3LbY8aMobi4mPPPPx+z2cxdd93FrbfeCrgS1RWrVrFgwQKmP/AA+/btIyoqigsuuIARY8bg36kT/VJSeD43l7kPPcT8p5+mf9++3HvnnSx8+mkAnDYbV/fvz64JE7hv7lxKSksZMWAA46+5hvVffEHpzp0Yvr44iotxWq3Y8/Ndk739DrPZzJo1a7jlllvo1asXrVu35tFHH2X48OH4+/sDrl9OLFq0iF9//RWz2UyvXr1Yu3Ztzb0K6tgjjzyCw+Hgz3/+M/n5+fTs2ZMPPviAiIiI4173+OOPc/PNN3PRRRcRFRXFjBkzyMvLa5CYRUTkzGA4Ky+6KQDk5eURFhZGbm4uoaGhjR2OVzabjbVr1zJs2LBqY+VEGpLeRTkRJSUl7Nmzh1atWrmTsPrgcDjIy8sjNDS0wZK5psbpdILdjtNqxeFeeqx8GTKb9YTGbxsVY77Lx30bvr6Yyj9jNrsr1RU+//xz+vbty86dO2nTpk19PdpppaHexYb6f0tOX/p7WpqS0+F9PNVcUBVsERGRM4hhGFAx9juw+nhzp8NRfd3v8gTcYbNCxXGbDQoLq7dvMvHWxo0Eh4WR0q4du9PTmTJzJn0uvJDWrVo1xCOKiIg0WUqwRUREziKGyYTh5wflM6FXVlH9dngk35WS8DIbToeDvKNHmbloEelZWTQLD6f/BRewcNo099hvd/W7ovJdvs9b9VtERORMogRbREREgGPVb7OPDxyn+n3TX/5C6oQJHgm4w2pzLTt23Oq3GcPXMwF3d0O3WDDO0m79IiJy5lCCLSIiIifkd6vfZWU4ypcdq1YBLyvD6bDjLLFDSYm31ssr3RbP6ndFAu6jf7KIiEjTp7+tRERE5JQZhgEWC2aLBaih+l054S4fB17RHR2ns3wSNqv36rfZjGGpqHpbqlfA1fVcRESaACXYIiIiUu8MkwnD3x+8zHTtdDpdFW4vCbi7+m2347QXQ0mxt9aPJd0WX0xVE3Czuf4fUEREBCXYIiIi0sgMw9U9HIsFgoKqHXfa7dVmPndU+ozTeex7wF61fbO5UrW7SgVc1W8REalDSrBFRESkSTPMZlcV+kSr35Wr4BXV7+JiKPZS/TYMj5nPTVUTcFW/RUSkFpRgi4iIyGnrpKrflRLwWlW/q1bBVf0WEZEqlGCLiIg0IXv37qVVq1Z8//33dO3atU7bbtmyJZMnT2by5Ml12m5T9rvVb5vNtca3t5nP7fYTrn6bqiThaNZzEZGzkv70FxERqScbN27k0ksv5ciRI4SHhzd2OFKFYRiuZNjXF6ih+l3R1bzUisPm2f28cvXb4aV9X5MJ26FD1Svgvr4YPj6qfouInIGUYIuIiIh4YZjNGAEBEBBQ7dix6ndF0l2lCm63YzgcOI5X/XaP+/ay7JjGfouInJZMjR2AiIhIU3XJJZcwceJEJk6cSFhYGFFRUcyePRun0wnAyy+/TM+ePQkJCSE2NpYbb7yRnJwcwNXV+9JLLwUgIiICwzBITU0FwOFwsGjRItq2bYufnx/Jyck89NBDHvfevXs3l156KYGBgXTp0oVNmzZ5HP/ss8/o168fAQEBJCUlMWnSJAorrR+dk5PD8OHDCQgIoFWrVrzyyiv19WM6KxmGgcnXF3NwMD6RkVhiY/BNSsKvTRv8O3bEr0MHrNHRWJKSsMTEYI6IxBQc7KqYG4ar+l1aiiM/n7LDh7FlZ2NNS6N0505Ktm+n5OefKd29G2tGBrb9+yk7cgR7YaFr9vTy909ERJoeVbBFRKTBOZ1Oisu8rWd8ahwOB8VlxfjYfDCZvP8OOcAnoFZdc1etWsW4ceP46quv+Oabb7j11ltJTk5m/Pjx2Gw25s+fT/v27cnJyWHKlCmkpqaydu1akpKS+Ne//sXVV1/Njh07CA0NJaC8EnrffffxwgsvsGTJEvr27UtWVhY///yzx31nzpzJ4sWLSUlJYebMmdxwww3s3LkTHx8fdu3axZAhQ1iwYAEvvvgiBw4ccP8iYMWKFQCkpqaSmZnJRx99hMViYdKkSe7kXxqAyYTT1xdTSEi1d7F69bu8Am61uqrgdrtr9vOyMigqqt52efXb5OuLzTBwFBRQ+OVmjMQELImJmLyMNxcRkYahBFtERBpccVkxvV/t3Sj33nzjZgItgSd8flJSEkuWLMEwDNq3b8/WrVtZsmQJ48eP5+abb3af17p1a5YtW0avXr0oKCggODiYyMhIAKKjo91jsPPz83niiSd46qmnGDt2LABt2rShb9++HvedOnUql19+OQBz587lnHPOYefOnXTo0IGFCxcyevRo92RlKSkpLFu2jIsvvphnn32WtLQ03nvvPb766it69eoFwPLly+nYseNJ/cykbnmO/a7OWVbmTsAdVZPwirHfpaXYS0uxOxzY8/LIXvAQpqwsAHyaN8eSlIRvUlL510T3Z3NUlMZ+i4jUIyXYIiIix3HBBRd4JCQXXnghjz32GHa7nS1btjBnzhx++OEHjhw5gsPhmuoqLS2NTp06eW1v+/btlJaWctlllx33vp07d3Z/HxcXB7i6fXfo0IEffviBH3/80aPbt9PpxOFwsGfPHn755Rd8fHzo0aOH+3iHDh000dppwvDxwfDxgYAAqo7EdjocHut+lxUWYuTn49u2DY78fBwFBZQdOEDZgQMUf/dd9bb9/V0Jd2ISvslJWBKTsCQlupLxxERMfn4N85AiImcoJdgiItLgAnwC2Hzj5jpv1+FwkJ+fT4iXbrmV710XSkpKGDx4MIMHD+aVV16hefPmpKWlMXjwYKzlayp7vb+XCbO8sVgs7u8rEvyKBL6goIAJEyYwadKkatclJyfzyy+/1OZR5DRimEwe1W9LYCA+BQUkPfkkfn5+OHJzsaZnYEtPc33NSHd9TUvDlp2Ns6SE0l93UvrrTq/t+8TEuBLuxCQsyUnuxNs3KQlzs2aqfouI/A4l2CIi0uAMw6hVN+0T5XA4KPMpI9ASWGOCXVubN3v+IuDLL78kJSWFn3/+mUOHDvHII4+QlJQEwDfffONxrm95EmS32937UlJSCAgIYMOGDdxyyy0nFVP37t3Ztm0bbdu29Xq8Q4cOlJWV8e2337q7iO/YsYOjR4+e1P3k9GAYBubwcALCwwk479xqx51WK7asLM8EPD3dNZFaWhqOwkLK9u+nbP9+ir/5tnr7gYH4JpZ3N09MrJSAJ2FJTMBUQ5d3EZGziRJsERGR40hLS2PKlClMmDCB7777jieffJLHHnuM5ORkfH19efLJJ7ntttv43//+x/z58z2ubdGiBYZh8M477zBs2DACAgIIDg5mxowZTJ8+HV9fX/r06cOBAwf46aefGDdu3AnFNGPGDC644AImTpzILbfcQlBQENu2bWPdunU89dRTtG/fniFDhjBhwgSeffZZfHx8mDx58glXz+XMZPj64tuiBb4tWgB9PI45nU7sR4+6Eu709EpfM7BmpFOWlY2zqIjSX36h1FsPCcPAJzb2WAKelIglKdk9/ttcPpO+iMiZTgm2iIjIcYwZM4bi4mLOP/98zGYzd911F7feeiuGYbBy5Uruv/9+li1bRvfu3Vm8eDFXXHGF+9qEhATmzp3Lvffey0033cSYMWNYuXIls2fPxsfHhwceeIDMzEzi4uK47bbbTjimzp078/HHHzNz5kz69euH0+mkTZs2jBo1yn3OihUruOWWW7j44ouJiYlhwYIFzJ49u05/NnLmMAwDn4gIfCIiCKg0/r+Cw2rFtm8ftowMV+Kdlo41ozwBT0/HWVREWVYWZVlZ8PXX1a43BQUdS7zd477LE/D4eFe3dxGRM4Dh1GKK1eTl5REWFkZubi6hoaGNHY5XNpuNtWvXMmzYMI9xeiINTe+inIiSkhL27NlDq1at8K/HJYQcDgd5eXmEhobWSRfxSy65hK5du7J06dJTD07OKnX9Ltakof7fOh6n04n98OHyqnf5uO+0dHf387L9++F4/9w0mfCJjcE3KfnY+O+kRHyTk7EkJmIOD1f1+xTo72lpSk6H9/FUc0FVsEVERETkpBmGgU+zZvg0a0ZA167VjjtKS7Hty/Q67tuakYGzuJiyzCzKMrNgc/XJD03BwZWWHUt0j/v2TU7CEheH0UT/kS4iZycl2CIiIiJSb0x+fvi1boVf61bVjjmdTuyHDnkd921LS6csJwdHQQGl27dTun27l8ZNWOLiqq/7XZ6Am8PCGuAJRUSOUYItIiJSg40bNzZ2CCJnNMMw8ImKwicqCrp1q3bcUVKCbd++auO+K5Yfc5Yft+3bR9GXX1a73hQaemzitfJ1vysmXrPExbnWGxcRqUP6U0VEREREmiSTvz9+bdrg16ZNtWNOp5OyAwewZZR3O09LP7bud3o6ZQcO4MjLo2TbNkq2baveuNmMJT7eo+J9bAK2JMxNdB4eEWnalGCLiIiIyGnHMAws0dFYoqOhe/dqxx3FxeWznlefeM2Wnu5aF7y8azpsqna9OSzMVemumHit8rrfsTGqfouIV/qTQURERETOOKaAAPxSUvBLSal2zOlwuKrf6ccq3u5x4BkZ2A8exJ6biz03l5L//a964z4+WBLij8147h7/7fpqDg5ugCcUkaZICbaIiIiInFUMkwlLTAyWmBgCe/asdtxRVHSs8l0x8Vp6Wvn47wycNhu239Kw/ZbmtX1zeDiW5ORj47+TErGUr/vtExODYTbX9yOKSCNRgi0iIiIiUokpMBD/9u3wb9+u2jGnw0FZTg7WtLRjM55XSsDthw9jP3oU+9GjlPz4Y/XGLRZ84+NdCXj5+G/3ut8JiZiDgxrgCUWkvijBFhERERE5QYbJhCU2FktsLJx/frXj9oJCbPsy3Am4e+K1tDSsmZlgs2H97Tesv/1GoZf2zZGRnuO+K3VD94mJwTCZ6v8hReSkKcEWERFpYvbu3UurVq34/vvv6dq1a5223bJlSyZPnszkyZPrtN0KqampHD16lDVr1tRL+yJNnTk4CHP79vi3b1/tmNNup2z//honXrMfOeKqgB8+TMkP1avfhsWCJTHR68RrvokJmIJU/RZpbEqwRURE6tHGjRu59NJLOXLkCOHh4Y0djog0IqN8aTBLfDz09lL9zs8vn/ncs9u5NSMd275MnDYb1j17sO7Z4736HRVVbdy3KS4Wn9xcnA5H/T+giCjBFhERkZqlpqbSsmVL5syZ0yD3czqd2O12fLQEkpyFzCEhmDt2xL9jx2rHnGVl2LL3e594LT3dNev5wYMUHzxI8ZYtHte2BnY/9jiWxESvE69ZEhMxBQQ0zEOKnOE0iENERKQGl1xyCRMnTmTixImEhYURFRXF7NmzcTqd7nNefvllevbsSUhICLGxsdx4443k5OQArq7el156KQAREREYhkFqaioADoeDRYsW0bZtW/z8/EhOTuahhx7yuP/u3bu59NJLCQwMpEuXLmza5LlW72effUa/fv0ICAggKSmJSZMmUVh4rK6Vk5PD8OHDCQgIoFWrVrzyyiv18WM6rtLSUiZNmkR0dDT+/v707duXr7/+2n1848aNGIbBe++9R48ePfDz8+Ozzz5j165dXHnllcTExBAcHEyvXr1Yv359g8cv0lQYPj74JiYQdMEFRFx7LdFT7iZxyRJavfFP2m3+knZfbablv94gYelSoqfeQ/ioUQRddCE+iYk4TSacpaVYd+2i4OOPOfL3v7N/4SNk3HEHu4dfwY5u3fmlXz/23jiazBkzOPDkUxxds4ai777DlpPj8WeeiByffj0sIiINzul04iwurvN2HQ4HjuJiHD4+UMNEQEZAAIZhnHCbq1atYty4cXz11Vd888033HrrrSQnJzN+/HgAbDYb8+fPp3379uTk5DBlyhRSU1NZu3YtSUlJ/Otf/+Lqq69mx44dhIaGElBeJbrvvvt44YUXWLJkCX379iUrK4uff/7Z494zZ85k8eLFpKSkMHPmTG644QZ27tyJj48Pu3btYsiQISxYsIAXX3yRAwcOuH8ZsGLFCsBVfc7MzOSjjz7CYrEwadIkd/LfUKZPn86//vUvVq1aRYsWLVi0aBGDBw9m586dREZGus+79957Wbx4Ma1btyYiIoL09HSGDRvGQw89hJ+fHy+99BLDhw9nx44dJCcnN+gziJwOzKGhBJxzDgHnnOOx32azsfbttxnYrRvOrKxKy4+VT7yWno4jPx/7gYMUHzhI8XffVWvb8PfHkpiAb1LysfHf7pnPEzD5+zfUY4o0eUqwRUSkwTmLi9nRvUe9tb//OMfaf/ctRmDgCbeVlJTEkiVLMAyD9u3bs3XrVpYsWeJOsG+++Wb3ua1bt2bZsmX06tWLgoICgoOD3UlkdHS0ewx2fn4+TzzxBE899RRjx44FoE2bNvTt29fj3lOnTuXyyy8HYO7cuZxzzjns3LmTDh06sHDhQkaPHu2erCwlJYVly5Zx8cUX8+yzz5KWlsZ7773HV199Ra9evQBYvnw5Hb10Pa0vhYWFPPvss6xcuZKhQ4cC8MILL7Bu3TqWL1/OtGnT3OfOmzePgQMHuj9HRkbSpUsX9+f58+fz5ptv8tZbbzFx4sQGewaRM4LZ7JocrVUrvE2DZs/NdSXc6WnlX9Nd477T0rFlZeEsKcG6cxfWnbu8Nu8THV3e7fzYjOeWxCR8k5MwN2tWq19qipzulGCLiIgcxwUXXODxj8MLL7yQxx57DLvdjtls5ttvv2XOnDn88MMPHDlyBEf5REJpaWl06tTJa5vbt2+ntLSUyy677Lj37ty5s/v7uLg4wNXtu0OHDvzwww/8+OOPHt2+nU4nDoeDPXv28Msvv+Dj40OPHsd+kdGhQ4ffnWjtlVdeYcKECe7PpaWlGIbB4sWL3fvee+89+vXrd9x2AHbt2oXNZqNPnz7ufRaLhfPPP5/t27d7nNuzZ0+PzwUFBcyZM4d3332XrKwsysrKKC4uJi0t7XfvKyK1Yw4LIyAsjIBzz6l2zGmzYcvKKh/3XWn8d3kC7igooCwnh7KcHIq//bba9UZAQKVx30mVxn8nuarffn4N8YgiDUYJtoiINDgjIID231X/h9ipcjgc5OXnExoSguk4XcTrSmFhIYMHD2bw4MG88sorNG/enLS0NAYPHozVaq3xuoATjMFisbi/r0jyKxL4goICJkyYwKRJk6pdl5yczC+//FKbR3G74oor6N27t/vzjBkzSEhI8LhPQkLCSbV9PEFVlheaOnUq69atY/HixbRt25aAgACuueaa4/5cRaTuGRYLvsnJ+HoZmuF0OrEfPYqtfJkxa1p55bu8Cm7LzsZZXEzpr79S+uuvXho38ImJOZaAJ5dXvssTcHNkpKrfctpRgi0iIg3OMIxaddM+YQ4HprIyTIGBNSbYtbV582aPz19++SUpKSmYzWZ+/vlnDh06xCOPPEJSUhIA33zzjcf5vr6+ANjtdve+lJQUAgIC2LBhA7fccstJxdW9e3e2bdtG27ZtvR7v0KEDZWVlfPvtt+4u4jt27ODo0aPHbTckJISQkBCPz5GRkTXe53jatGmDr68vn3/+OS1atABc40G//vrr312H+/PPPyc1NZWRI0cCrl8o7N27t9YxiEj9MQwDn4gIfCIiCDjvvGrHnVYrtsxMz3W/K43/dhQVUZadTVl2NlT5sxPAFBjoqnRXW/c70VX9Lv/zVaQpUYItIiJyHGlpaUyZMoUJEybw3Xff8eSTT/LYY48Brkqxr68vTz75JLfddhv/+9//mD9/vsf1LVq0wDAM3nnnHYYNG0ZAQADBwcHMmDGD6dOn4+vrS58+fThw4AA//fQT48aNO6G4ZsyYwQUXXMDEiRO55ZZbCAoKYtu2baxbt46nnnqK9u3bM2TIECZMmMCzzz6Lj48PkydPPuHqeV0ICgri9ttvZ9q0aURGRpKcnMyiRYsoKir63edMSUnh3//+N8OHD8cwDGbPnu2u3ovI6cHw9cW3ZUt8W7asdszpdGI/cqS823mlBDw9HWtGBmXZ2TiKiijdsYPSHTu8NG7gExd7bMI1d/dz11dzeLiq36epp556ipUrV7J161aGDh3KmjVr3Mdmz57NmjVr2L59OxMnTmTp0qXHbevWW2/l448/5tdff+Xxxx/3+OVuXl4eN9xwg3tFjtdff53g4GCPa19//fVax68EW0RE5DjGjBlDcXEx559/Pmazmbvuuotbb70VgObNm7Ny5Uruv/9+li1bRvfu3Vm8eDFXXHGF+/qEhATmzp3Lvffey0033cSYMWNYuXIls2fPxsfHhwceeIDMzEzi4uK47bbbTjiuzp078/HHHzNz5kz69euH0+mkTZs2jBo1yn3OihUruOWWW7j44ouJiYlhwYIFzJ49u+5+OCfgkUceweFw8Oc//5n8/Hx69uzJBx98QERExHGve/zxx7n55pu56KKLiIqKYsaMGeTl5TVQ1CJS3wzDwCcyEp/ISAIqTWhYwWG1YsvYV2Xd72MJuLOoiLLMLMoys+Crr6pdbwoOdiXc3tb9jovDUPW7yYqPj2fWrFmsX7+ejIwMj2Nt27Zl0aJFvPDCCyfUVpcuXRg1ahQzZ86sduz5558nNDSUQ4cOccMNN/D8889zzz33uHt6LVy48KTiN5xa2K6avLw8wsLCyM3NJTQ0tLHD8cpms7F27VqGDRvmMUZPpKHpXZQTUVJSwp49e2jVqhX+9bici8PhIC8vj9DQ0DrpIn7JJZfQtWvX3/0NuUhVdf0u1qSh/t+S09eZ+Pe00+nEfviwe9K1igS84nPZ/uOtJQGYTFhiY7EklyfciZ4JuCksTNXvelKb93HOnDls2bLFo4JdITU1lfDw8BP++/mSSy5hxIgRHhXs22+/na5duzJhwgSee+45fvzxR5555hlSU1NZtWrVSeeCqmCLiIiIiMhpwzAMfJo1w6dZMwK6dq123FFaim3fPvfEa+5x3xXV7+JibJmZ2DIzKfqyevumkJDybufHEnD3ut+xsRhnyC8qznbnnXceH374IampqXz00UdccsklfPbZZ+zZs+eU2lWCLSIiIiIiZwyTnx9+rVvj17p1tWNOpxP7wYNex33b0tMpy8nBkZ9P6bbtlG7bXr1xsxlLXJw7Aa+27ncT7f0q1Y0bN47//e9/9OzZk379+vGnP/2Jiy++mOXLl9O9e3eGDRtGdHQ0Tz31FPHx8SfcrhJsERGRGmzcuLGxQxARkTpkGAY+zZvj07w5dO9W7bijuBjbvn1exn27PjtLS13LkmVkULSpevnbFBZWZd3vYxOwWWJjMXyUfjUVfn5+PPPMM+7P8+fP56qrrsJmswHw5ptv8u6773LPPffw2muvnXC7+i8sIiIiIiICmAIC8GvbFj8vSxM6HQ7KDhysMvFamutrRjr2Awdx5OZSkptLyU8/VW/cxwdLfHzN635XWiJRGtYvv/zCmjVr2LRpE6+88grgSsAvvPBCFi1aVKu2lGCLiIiIiIj8DsNkwhITjSUmmsAePaoddxQVubqaZ2S4lx+rSMBtGRmudcHT0rClpXlt3xwefmzG80TPBNwnNhbDbK7vR2wSysrK3JvD4aCkpASTyYSvry82mw273e7eSkpKMJvNNU6YZrVacTgcOBwOysrKKCkpwcfHB58qPQnuuOMOli1bhq+vLy3Ll5XLzc1l3bp1tGnTplbxK8EWERERERE5RabAQPzbtcO/Xbtqx1zV7wPY0tKOjf9Oz3B9zsjAfugQ9qNHsR89SsnWrdUbt1jwjY93dTVPSnSt/52c5B7/bQ4OaoAnbBgLFixg7ty57s8BAQFcfPHFbNy4kfHjx7Nq1Sr3saeeeoqxY8eycuVKAM455xzuv/9+Ro8eDcCgQYP4+OOPAfj000+ZNm0aDz74IHPmzHG3sXLlStq2bUufPn0A6FH+y5POnTuTlJRU67WwtUyXF1qmS+TE6V2UE3G6LtMlcrK0TJc0Ffp7+vTgKCzEmrEPW3papRnP07GlpWPbtw9nJDWxAAAAIb9JREFU+bjgmpgjIlwJd6LnuG/fpCR8oqObRPV7165d7Nmzh8LCQvr160dkZGRjh+TVqeaCqmCLiIiIiIg0IlNQEP7t2+Hf3kv1226nLCfn2IRrFeO/yydesx8+jP3IEexHjlDyw4/VrjcsFiwJCZUS8ErrficmYAqq3+r3u+++yyOPPMJnn31GQEAAr732Gm3btuW6667j/vvvJzk5uV7v39CUYIuIiIiIiDRRRsXSYHFxcP751Y7bCwqwZWRUm3jNlp6ONTMTp82Gde9erHv3UuilfXOzZq6J15KPrftdMfGaT3Q0xin0wnn00UeZPn065ioV9JKSEpYvX84bb7zBxx9/zDnnnHPS92hqlGCLiIg0YS1btmTy5MlMnjwZcC0x8+abbzJixIhGjUtERJoGc3Aw5g4d8O/Qodoxp91OWXa257jvSt3Q7UePYj90iOJDhyj+4Ydq1xu+vlgSE93rflck3pZEVzd0U0BAjXGtXbuW6dOnA2C326sdLysr4+jRowwePJidO3eeMcNclGCLiIjUIDU11WMylcjISHr16sWiRYvo3Llzo8SUlZVFREREo9xbREROL4bZ7OoenpAA9K523J6f757x3L38WFq6azb0zEycVivW3bux7t7tvfrdPKrSuO/kY+O/E5NY9MgjmM1mr8m1+/52O/v27eONN97gT3/6U909eCNSgi0iInIcQ4YMYcWKFQBkZ2cza9Ys/vjHP5JWwzIr9S02NrZR7isiImcec0gI5k6d8O/UqdoxZ1kZtuz95RXvSuO+yxNwR24u9gMHKT5wkOLvv692/RKHg4ykZNJtNjJsVtKtNnIMA9/9OR7nmUwmXnjhhTMmwdYUqyIiIsfh5+dHbGwssbGxdO3alXvvvZf09HQOHDgAwIwZM2jXrh2BgYG0bt2a2bNnY6s02+sPP/zApZdeSkhICKGhofTo0YNvvvnGffyzzz6jX79+BAQEkJSUxKRJkygs9FYncDEMgzVr1gCwd+9eDMPg3//+N5deeimBgYF06dKFTZs2eVxT23uIiIgYPj74JiYQdOGFRFx3HdH3TCFxyRJa/esN2m/+knabv6Tlv94gYekSmt8zhfDrriPooguxJCbiNJnwN5lo6+fHpcHB/DkikvtjYlgaHU3iCy943MfhcLB79+5Gesq6pwq2iIg0GqvVWuMxk8mEj4/PCZ1rGIbH8jNWq9Xr0ki+vr4nGalLQUEBf//732nbti3NmjUDICQkhJUrVxIfH8/WrVsZP348ISEh7nFno0ePplu3bjz77LOYzWa2bNnijnXXrl0MGTKEBQsW8OKLL3LgwAEmTpzIxIkT3VXzEzFz5kwWL15MSkoKM2fO5IYbbmDnzp34+PjU2T1EREQqM4eFERAWRoCXCcq+/Owzru3fnySLhSSLL4kWC0m+FpL9/EiObw8//c/jfD8/v4YKu94pwRYRkUazcOHCGo+lpKRw4403uj8vXrzYozJcWYsWLUhNTXV/fvLJJykqKqp23oMPPljrGN955x2Cg4MBKCwsJC4ujnfeecedwM+aNct9bsuWLZk6dSqvv/66O8FOS0tj2rRpdCiffCYlJcV9/sKFCxk9erR7ArOUlBSWLVvGxRdfzLPPPnvCE75MnTqVyy+/HIC5c+dyzjnnsHPnTjp06FBn9xARETlR53btyhFfXzIKC9nEsb+PAwICeG3JElj3X/c+Hx8fLrnkkkaIsn6oi7iIiMhxXHrppWzZsoUtW7bw1VdfMXjwYIYOHcpvv/0GwOrVq+nTpw+xsbEEBwcza9Ysj/HZU6ZM4ZZbbmHAgAE88sgj7Nq1y33shx9+YOXKlQQHB7u3wYMH43A42LNnzwnHWHnCtbi4OABycnLq9B4iIiInKjg4mJtvvtmjJ1pNysrKmDhxYgNE1TCaRAX76aef5tFHHyU7O5suXbrw5JNPcr6XNd6qev3117nhhhu48sor3ePRAJxOJw8++CAvvPACR48epU+fPjz77LMeVQMREWl89913X43Hqnbxnjp1ao3nGobh8fkvf/mL1y7iJyMoKIi2bdu6P//tb38jLCyMF154gcsvv5zRo0czd+5cBg8eTFhYGK+//jqPPfaY+/w5c+Zw44038u677/Lee+/x4IMP8vrrrzNy5EgKCgqYMGECkyZNqnbf5OTkE46xcvf4ip+Fw+EAqLN7iIiI1Ma9997L6tWrOXToUI0ziRuGwdixY+natWvDBlePGj3BXr16NVOmTOG5556jd+/eLF26lMGDB7Njxw6io6NrvG7v3r1MnTqVfv36VTu2aNEili1bxqpVq2jVqhWzZ89m8ODBbNu2TV3hRESakNqMia7tuXWVYFdlGAYmk4ni4mK++OILWrRowcyZM93HKyrblbVr14527dpx9913c8MNN7BixQpGjhxJ9+7d2bZtm0cCX9ca4h4iIiJVxcfH88knnzB48GB+++03zGaz+1hFZTs1NZXnn3++sUKsF43eRfzxxx9n/Pjx3HTTTXTq1InnnnuOwMBAXnzxxRqvsdvt7opB69atPY45nU6WLl3KrFmzuPLKK+ncuTMvvfQSmZmZHlVuERGRE1FaWkp2djbZ2dls376dv/zlLxQUFDB8+HBSUlJIS0vj9ddfZ9euXSxbtow333zTfW1xcTETJ05k48aN/Pbbb3z++ed8/fXXdOzYEXDNQP7FF18wceJEtmzZwq+//sp//vOfOu0q1xD3EBER8aZ9+/b88ssvrF69mksuucTdc+rPf/4zW7Zs4cUXX/TohXUmaNQKttVq5dtvv/XoImgymRgwYEC1JUYqmzdvHtHR0YwbN45PP/3U49iePXvIzs5mwIAB7n1hYWH07t2bTZs2cf3111drr7S0lNLSUvfnvLw8AGw2W40T6jS2iriaanxy9tC7KCfCZrPhdDpxOBzursv1wel0ur/WxX2cTifvv/++e1xzSEgIHTp0YPXq1fzhD38AYPLkyUycOJHS0lKGDRvGrFmzmDt3Lg6HA8MwOHjwIGPGjGH//v1ERUUxcuRIHnzwQRwOB+eeey4fffQRs2bNol+/fjidTtq0acN1113nEX/V56n4OVbsq/p95X0neg+pW3X9LtbE4XDgdDqx2Wwe1SGRCvp7WhqbYRiMHDmSkSNHYrPZWLduHY8++igWi6VJvpenGpPhrPgboBFkZmaSkJDAF198wYUXXujeP336dD7++GP+v707DYrqTN8GfjX7ZtsoSovilriDiOJCQLBGAlqO0dGKxjCKmhrMCKVEYpSZRBwyChiTSVSiMR/EmonKWBXXjCQMCgRFQBRREXQICqMsRiUgqCx9vx/+ryfpAY1LQ7Ncv6qu4jzPc07fz+HW7puzZWZmNlsnPT0db7zxBnJzc+Hg4IDFixejqqpKOTp96tQpeHl54ebNm8oXIgCYN28eVCoVEhISmm1z/fr1+Mtf/tKsfc+ePbCxsTHATImIujYzMzNotVo4Ozu/8KOyiOhn9fX1KC0tRXl5ORobG40dDhFRh1dXV4c333wTP/30E9Rq9TOvb/RrsJ9FTU0NFi5ciC+//BIODg4G225ERARWrVqlLFdXV8PZ2Rn+/v7PtVPbwqO//rz66qud7rQK6liYi/Q0Hjx4gNLSUtjZ2bXqvTBEBDU1NejWrVuzG58RtaW2ysUHDx7A2toaPj4+vM8MtYif09SedIR8fHQ28/MyaoHt4OAAU1NTVFRU6LVXVFRAq9U2G19UVIRr165h5syZStuj067MzMxQWFiorFdRUaF3BLuiouKxd6eztLRs8eHm5ubm7fYX/0hHiJG6BuYiPUlTU5Nyc7DWuvkY8PNnwqP3IjKWtspFExMTqFQq/h9Mv4o5Qu1Je87HF43LqN8+LCwsMG7cOCQnJyttOp0OycnJeqeMPzJ8+HBcuHBBeR5pbm4uXnvtNeUZpc7Ozhg0aBC0Wq3eNqurq5GZmdniNomIiIiIiIgMweiniK9atQpBQUHw8PDAhAkT8Omnn6K2thZLliwBACxatAh9+/ZFdHQ0rKys4OLiore+RqMBAL32sLAw/PWvf8WQIUOUx3Q5OTlh9uzZbTUtIiIiIiIi6mKMXmDPnz8ft27dwrp161BeXo4xY8YgMTERjo6OAICSkpJnPrXqvffeQ21tLYKDg1FVVQVvb28kJiby2iQiIiIiIiJqNUYvsAEgNDT0sc/jTElJeeK68fHxzdpUKhWioqIQFRVlgOiIiMhQjPjgCqJOif+miIjaF94BhoiIWt2j5/PW19cbORKizqWurg7Ai9+Uh4iIDKNdHMEmIqLOzczMDDY2Nrh16xbMzc1b7a7KOp0O9fX1ePDgAe8iTkbV2rkoIqirq0NlZSU0Go3yRywiIjIuFthERNTqVCoV+vTpg+LiYly/fr3V3kdEcP/+fVhbW/M52GRUbZWLGo2mxUebEhGRcbDAJiKiNmFhYYEhQ4a06mniDQ0NSEtLg4+PD0+ZJaNqi1w0NzfnkWsionaGBTYREbUZExOTVn2ig6mpKRobG2FlZcUCm4yKuUhE1DXxAjUiIiIiIiIiA2CBTURERERERGQALLCJiIiIiIiIDIDXYLdARAAA1dXVRo7k8RoaGlBXV4fq6mpe20VGxVyk9oT5SO0Fc5HaC+YitScdIR8f1YCPasJnxQK7BTU1NQAAZ2dnI0dCREREREREba2mpgbdu3d/5vVU8ryleSem0+lw8+ZNdOvWrd0+R7W6uhrOzs4oLS2FWq02djjUhTEXqT1hPlJ7wVyk9oK5SO1JR8hHEUFNTQ2cnJxgYvLsV1TzCHYLTExM0K9fP2OH8VTUanW7TU7qWpiL1J4wH6m9YC5Se8FcpPakvefj8xy5foQ3OSMiIiIiIiIyABbYRERERERERAbAAruDsrS0RGRkJCwtLY0dCnVxzEVqT5iP1F4wF6m9YC5Se9IV8pE3OSMiIiIiIiIyAB7BJiIiIiIiIjIAFthEREREREREBsACm4iIiIiIiMgAWGB3QHFxcRg4cCCsrKwwceJEZGVlGTsk6mSio6Mxfvx4dOvWDb1798bs2bNRWFioN+bBgwcICQlBz549YWdnh7lz56KiokJvTElJCWbMmAEbGxv07t0bq1evRmNjY1tOhTqZmJgYqFQqhIWFKW3MRWpLN27cwO9//3v07NkT1tbWcHV1xZkzZ5R+EcG6devQp08fWFtbw8/PD1evXtXbxp07dxAYGAi1Wg2NRoO33noL9+7da+upUAfW1NSEDz74AIMGDYK1tTVeeuklfPjhh/jlrZWYi9Ra0tLSMHPmTDg5OUGlUuHgwYN6/YbKvby8PEyePBlWVlZwdnbGpk2bWntqBsECu4NJSEjAqlWrEBkZibNnz8LNzQ0BAQGorKw0dmjUiaSmpiIkJASnT59GUlISGhoa4O/vj9raWmXMO++8gyNHjmD//v1ITU3FzZs3MWfOHKW/qakJM2bMQH19PU6dOoXdu3cjPj4e69atM8aUqBPIzs7GF198gdGjR+u1Mxeprdy9exdeXl4wNzfHsWPHkJ+fj48//hj29vbKmE2bNmHLli3YsWMHMjMzYWtri4CAADx48EAZExgYiEuXLiEpKQlHjx5FWloagoODjTEl6qBiY2Oxfft2bNu2DZcvX0ZsbCw2bdqErVu3KmOYi9Raamtr4ebmhri4uBb7DZF71dXV8Pf3x4ABA5CTk4OPPvoI69evx86dO1t9fi9MqEOZMGGChISEKMtNTU3i5OQk0dHRRoyKOrvKykoBIKmpqSIiUlVVJebm5rJ//35lzOXLlwWAZGRkiIjIv/71LzExMZHy8nJlzPbt20WtVsvDhw/bdgLU4dXU1MiQIUMkKSlJfH19ZeXKlSLCXKS2tWbNGvH29n5sv06nE61WKx999JHSVlVVJZaWlrJ3714REcnPzxcAkp2drYw5duyYqFQquXHjRusFT53KjBkzZOnSpXptc+bMkcDAQBFhLlLbASAHDhxQlg2Ve59//rnY29vrfU6vWbNGhg0b1sozenE8gt2B1NfXIycnB35+fkqbiYkJ/Pz8kJGRYcTIqLP76aefAAA9evQAAOTk5KChoUEvF4cPH47+/fsruZiRkQFXV1c4OjoqYwICAlBdXY1Lly61YfTUGYSEhGDGjBl6OQcwF6ltHT58GB4eHnj99dfRu3dvuLu748svv1T6i4uLUV5erpeP3bt3x8SJE/XyUaPRwMPDQxnj5+cHExMTZGZmtt1kqEN75ZVXkJycjCtXrgAAzp8/j/T0dEyfPh0Ac5GMx1C5l5GRAR8fH1hYWChjAgICUFhYiLt377bRbJ6PmbEDoKf3448/oqmpSe9LIgA4OjqioKDASFFRZ6fT6RAWFgYvLy+4uLgAAMrLy2FhYQGNRqM31tHREeXl5cqYlnL1UR/R09q3bx/Onj2L7OzsZn3MRWpLP/zwA7Zv345Vq1bhT3/6E7Kzs7FixQpYWFggKChIyaeW8u2X+di7d2+9fjMzM/To0YP5SE9t7dq1qK6uxvDhw2FqaoqmpiZs2LABgYGBAMBcJKMxVO6Vl5dj0KBBzbbxqO+Xl+a0NyywieiJQkJCcPHiRaSnpxs7FOqCSktLsXLlSiQlJcHKysrY4VAXp9Pp4OHhgY0bNwIA3N3dcfHiRezYsQNBQUFGjo66kn/+85/46quvsGfPHowaNQq5ubkICwuDk5MTc5HIyHiKeAfi4OAAU1PTZnfHraiogFarNVJU1JmFhobi6NGjOHHiBPr166e0a7Va1NfXo6qqSm/8L3NRq9W2mKuP+oieRk5ODiorKzF27FiYmZnBzMwMqamp2LJlC8zMzODo6MhcpDbTp08fjBw5Uq9txIgRKCkpAfBzPj3pc1qr1Ta7MWljYyPu3LnDfKSntnr1aqxduxZvvPEGXF1dsXDhQrzzzjuIjo4GwFwk4zFU7nXkz24W2B2IhYUFxo0bh+TkZKVNp9MhOTkZnp6eRoyMOhsRQWhoKA4cOIDjx483O0Vn3LhxMDc318vFwsJClJSUKLno6emJCxcu6P0HmpSUBLVa3ewLKtHjTJ06FRcuXEBubq7y8vDwQGBgoPIzc5HaipeXV7NHFl65cgUDBgwAAAwaNAharVYvH6urq5GZmamXj1VVVcjJyVHGHD9+HDqdDhMnTmyDWVBnUFdXBxMT/a/xpqam0Ol0AJiLZDyGyj1PT0+kpaWhoaFBGZOUlIRhw4a169PDAfAu4h3Nvn37xNLSUuLj4yU/P1+Cg4NFo9Ho3R2X6EX98Y9/lO7du0tKSoqUlZUpr7q6OmXM22+/Lf3795fjx4/LmTNnxNPTUzw9PZX+xsZGcXFxEX9/f8nNzZXExETp1auXREREGGNK1In88i7iIsxFajtZWVliZmYmGzZskKtXr8pXX30lNjY28o9//EMZExMTIxqNRg4dOiR5eXkya9YsGTRokNy/f18ZM23aNHF3d5fMzExJT0+XIUOGyIIFC4wxJeqggoKCpG/fvnL06FEpLi6Wr7/+WhwcHOS9995TxjAXqbXU1NTIuXPn5Ny5cwJAPvnkEzl37pxcv35dRAyTe1VVVeLo6CgLFy6Uixcvyr59+8TGxka++OKLNp/vs2KB3QFt3bpV+vfvLxYWFjJhwgQ5ffq0sUOiTgZAi69du3YpY+7fvy/Lly8Xe3t7sbGxkd/97ndSVlamt51r167J9OnTxdraWhwcHCQ8PFwaGhraeDbU2fxvgc1cpLZ05MgRcXFxEUtLSxk+fLjs3LlTr1+n08kHH3wgjo6OYmlpKVOnTpXCwkK9Mbdv35YFCxaInZ2dqNVqWbJkidTU1LTlNKiDq66ulpUrV0r//v3FyspKBg8eLH/+85/1HmnEXKTWcuLEiRa/JwYFBYmI4XLv/Pnz4u3tLZaWltK3b1+JiYlpqym+EJWIiHGOnRMRERERERF1HrwGm4iIiIiIiMgAWGATERERERERGQALbCIiIiIiIiIDYIFNREREREREZAAssImIiIiIiIgMgAU2ERERERERkQGwwCYiIiIiIiIyABbYRERERERERAbAApuIiKiVXLt2DSqVCrm5ucYORVFQUIBJkybBysoKY8aMMXY4rWb9+vWden5ERNQ+scAmIqJOa/HixVCpVIiJidFrP3jwIFQqlZGiMq7IyEjY2tqisLAQycnJLY5pD/stPj4eKpXqia9r1661SSxERERPiwU2ERF1alZWVoiNjcXdu3eNHYrB1NfXP/e6RUVF8Pb2xoABA9CzZ8/HjjP2fps/fz7KysqUl6enJ/7whz/otTk7OxslNiIiosdhgU1ERJ2an58ftFotoqOjHzumpdOJP/30UwwcOFBZXrx4MWbPno2NGzfC0dERGo0GUVFRaGxsxOrVq9GjRw/069cPu3btarb9goICvPLKK7CysoKLiwtSU1P1+i9evIjp06fDzs4Ojo6OWLhwIX788Uelf8qUKQgNDUVYWBgcHBwQEBDQ4jx0Oh2ioqLQr18/WFpaYsyYMUhMTFT6VSoVcnJyEBUVBZVKhfXr17/QfgOA9PR0TJ48GdbW1nB2dsaKFStQW1sLANi2bRtcXFyUsY+OgO/YsUPvfd5///1m27W2toZWq1VeFhYWsLGxUZbr6+sxZ84c2NnZQa1WY968eaioqHhsnEVFRRg8eDBCQ0MhInj48CHeffdd9O3bF7a2tpg4cSJSUlKU8fHx8dBoNPj2228xYsQI2NnZYdq0aSgrK1PGpKSkYMKECbC1tYVGo4GXlxeuX7/+xP1FRESdGwtsIiLq1ExNTbFx40Zs3boV//3vf19oW8ePH8fNmzeRlpaGTz75BJGRkfjtb38Le3t7ZGZm4u2338ayZcuavc/q1asRHh6Oc+fOwdPTEzNnzsTt27cBAFVVVfjNb34Dd3d3nDlzBomJiaioqMC8efP0trF7925YWFjg5MmTegXqL3322Wf4+OOPsXnzZuTl5SEgIACvvfYarl69CgAoKyvDqFGjEB4ejrKyMrz77ruPnevT7LeioiJMmzYNc+fORV5eHhISEpCeno7Q0FAAgK+vL/Lz83Hr1i0AQGpqKhwcHJRCtqGhARkZGZgyZcqTd/z/0Ol0mDVrFu7cuYPU1FQkJSXhhx9+wPz581scn5eXB29vb7z55pvYtm0bVCoVQkNDkZGRgX379iEvLw+vv/46pk2bpuwrAKirq8PmzZvx97//HWlpaSgpKVH2WWNjI2bPng1fX1/k5eUhIyMDwcHBXfbSAyIi+v+EiIiokwoKCpJZs2aJiMikSZNk6dKlIiJy4MAB+eVHYGRkpLi5uemt+7e//U0GDBigt60BAwZIU1OT0jZs2DCZPHmystzY2Ci2trayd+9eEREpLi4WABITE6OMaWhokH79+klsbKyIiHz44Yfi7++v996lpaUCQAoLC0VExNfXV9zd3X91vk5OTrJhwwa9tvHjx8vy5cuVZTc3N4mMjHzidp52v7311lsSHByst+73338vJiYmcv/+fdHpdNKzZ0/Zv3+/iIiMGTNGoqOjRavViohIenq6mJubS21t7a/OzdfXV1auXCkiIt99952YmppKSUmJ0n/p0iUBIFlZWSLy8+/05MmTYm9vL5s3b1bGXr9+XUxNTeXGjRt67zF16lSJiIgQEZFdu3YJAPnPf/6j9MfFxYmjo6OIiNy+fVsASEpKyq/GTkREXQePYBMRUZcQGxuL3bt34/Lly8+9jVGjRsHE5OePTkdHR7i6uirLpqam6NmzJyorK/XW8/T0VH42MzODh4eHEsf58+dx4sQJ2NnZKa/hw4cD+L8jxI+MGzfuibFVV1fj5s2b8PLy0mv38vJ6oTk/ab+dP38e8fHxerEHBARAp9OhuLgYKpUKPj4+SElJQVVVFfLz87F8+XI8fPgQBQUFSE1Nxfjx42FjY/NMMV2+fBnOzs5612CPHDkSGo1GL86SkhK8+uqrWLduHcLDw5X2CxcuoKmpCUOHDtWLPTU1VW+f29jY4KWXXlKW+/Tpo/xue/TogcWLFyMgIAAzZ87EZ599pnf6OBERdU1mxg6AiIioLfj4+CAgIAARERFYvHixXp+JiQlERK+toaGh2TbMzc31llUqVYttOp3uqeO6d+8eZs6cidjY2GZ9ffr0UX62tbV96m0a0pP2271797Bs2TKsWLGi2Xr9+/cH8H/Xj+/cuRPff/893N3doVarlaI7NTUVvr6+rRZ7r1694OTkhL1792Lp0qVQq9VK3KampsjJyYGpqaneOnZ2dsrPLf1uf5knu3btwooVK5CYmIiEhAS8//77SEpKwqRJk1ptTkRE1L7xCDYREXUZMTExOHLkCDIyMvTae/XqhfLycr3iyZDPrj59+rTyc2NjI3JycjBixAgAwNixY3Hp0iUMHDgQL7/8st7rWYpqtVoNJycnnDx5Uq/95MmTGDly5AvF/7j9NnbsWOTn5zeL++WXX4aFhQWAn6/D3r9/v3Kt9ZQpU/Dvf/8bJ0+efObrrwFgxIgRKC0tRWlpqdKWn5+PqqoqvblaW1vj6NGjsLKyQkBAAGpqagAA7u7uaGpqQmVlZbO4tVrtM8Xi7u6OiIgInDp1Ci4uLtizZ88zz4eIiDoPFthERNRluLq6IjAwEFu2bNFrnzJlCm7duoVNmzahqKgIcXFxOHbsmMHeNy4uDgcOHEBBQQFCQkJw9+5dLF26FAAQEhKCO3fuYMGCBcjOzkZRURG+/fZbLFmyBE1NTc/0PqtXr0ZsbCwSEhJQWFiItWvXIjc3FytXrnyh+B+339asWYNTp04hNDQUubm5uHr1Kg4dOqTc5AwARo8eDXt7e+zZs0evwD548CAePnzY7JT2p+Hn56fEdPbsWWRlZWHRokXw9fWFh4eH3lhbW1t88803MDMzw/Tp03Hv3j0MHToUgYGBWLRoEb7++msUFxcjKysL0dHR+Oabb54qhuLiYkRERCAjIwPXr1/Hd999h6tXryp/OCEioq6JBTYREXUpUVFRzU7hHjFiBD7//HPExcXBzc0NWVlZT7zD9rOKiYlBTEwM3NzckJ6ejsOHD8PBwQEAlKPOTU1N8Pf3h6urK8LCwqDRaPSu934aK1aswKpVqxAeHg5XV1ckJibi8OHDGDJkyAvPoaX9Nnr0aKSmpuLKlSuYPHky3N3dsW7dOjg5OSljVCoVJk+eDJVKBW9vb2U9tVoNDw+P5zr1XaVS4dChQ7C3t4ePjw/8/PwwePBgJCQktDjezs4Ox44dg4hgxowZqK2txa5du7Bo0SKEh4dj2LBhmD17NrKzs5VT23+NjY0NCgoKMHfuXAwdOhTBwcEICQnBsmXLnnk+RETUeajkfy86IyIiIiIiIqJnxiPYRERERERERAbAApuIiIiIiIjIAFhgExERERERERkAC2wiIiIiIiIiA2CBTURERERERGQALLCJiIiIiIiIDIAFNhEREREREZEBsMAmIiIiIiIiMgAW2EREREREREQGwAKbiIiIiIiIyABYYBMREREREREZAAtsIiIiIiIiIgP4fxScgHGMxzhXAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: Scatter plot of Accuracy vs num_new_tokens\n",
    "subset = df[df[\"model_name\"] == \"Llama-3.2-3B-Instruct\"]  # filter one model for clarity\n",
    "print(subset)\n",
    "base_line = subset[subset[\"num_new_tokens\"] == \"Baseline\"]\n",
    "subset = subset[subset[\"num_new_tokens\"] != \"Baseline\"]\n",
    "\n",
    "group_col = [\"finetuning\"]\n",
    "metric = \"prompt_level_strict_acc\"\n",
    "\n",
    "my_compression_plotter(subset, group_col, metric, base_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL PATCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cmlscratch/astein0/tmp/ipykernel_3167261/405337152.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'NAN' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  main_results.fillna(\"NAN\", inplace=True)\n",
      "/cmlscratch/astein0/tmp/ipykernel_3167261/405337152.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  main_results[\"task_name\"].replace(\"ifeval\", \"baseline\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: model_name_sanitized                                       meta-llama__Llama-3.2-3B\n",
      "trained_model_name                                         meta-llama__Llama-3.2-3B\n",
      "model_name                                                             Llama-3.2-3B\n",
      "embeddings_init                                                                 NAN\n",
      "task_name                                                                  baseline\n",
      "exact_match                                                                     NAN\n",
      "exact_match_stderr                                                              NAN\n",
      "prompt_level_strict_acc                                                    0.195933\n",
      "prompt_level_strict_acc_stderr                                             0.017081\n",
      "inst_level_strict_acc                                                       0.32494\n",
      "inst_level_strict_acc_stderr                                                    NAN\n",
      "prompt_level_loose_acc                                                     0.218115\n",
      "prompt_level_loose_acc_stderr                                              0.017771\n",
      "inst_level_loose_acc                                                        0.35012\n",
      "inst_level_loose_acc_stderr                                                     NAN\n",
      "compression_ratio                                                          0.001207\n",
      "learning_ratio                                                              0.00482\n",
      "theoretical_compression_ratio                                                   0.0\n",
      "alias                                                                        ifeval\n",
      "limit                                                                           NAN\n",
      "json_path                         eval_results/full_patching/meta-llama__Llama-3...\n",
      "dataset_str                                                                 unknown\n",
      "finetuning_params                                                               NAN\n",
      "total_batch_size                                                                NAN\n",
      "learning_rate                                                                   NAN\n",
      "main_loss_type                                                                  NAN\n",
      "embedding_init_strategy                                                         NAN\n",
      "num_new_tokens                                                                  NAN\n",
      "unfreeze_params_steps                                                           NAN\n",
      "finetune_params_prefreeze                                                       NAN\n",
      "dataset                                                                         NAN\n",
      "tokenizer_path                                                                  NAN\n",
      "seed                                                                            NAN\n",
      "reset_optimizer                                                                 NAN\n",
      "warmup_steps                                                                    NAN\n",
      "lr_schedule                                                                     NAN\n",
      "warmup_steps_prefreeze                                                          NAN\n",
      "lr_schedule_prefreeze                                                           NAN\n",
      "Name: 12, dtype: object\n",
      "Error: model_name_sanitized                              meta-llama__Llama-3.2-3B-Instruct\n",
      "trained_model_name                                meta-llama__Llama-3.2-3B-Instruct\n",
      "model_name                                                    Llama-3.2-3B-Instruct\n",
      "embeddings_init                                                                 NAN\n",
      "task_name                                                                  baseline\n",
      "exact_match                                                                     NAN\n",
      "exact_match_stderr                                                              NAN\n",
      "prompt_level_strict_acc                                                      0.7061\n",
      "prompt_level_strict_acc_stderr                                             0.019604\n",
      "inst_level_strict_acc                                                      0.790168\n",
      "inst_level_strict_acc_stderr                                                    NAN\n",
      "prompt_level_loose_acc                                                     0.750462\n",
      "prompt_level_loose_acc_stderr                                              0.018622\n",
      "inst_level_loose_acc                                                       0.822542\n",
      "inst_level_loose_acc_stderr                                                     NAN\n",
      "compression_ratio                                                         -0.010776\n",
      "learning_ratio                                                             0.012392\n",
      "theoretical_compression_ratio                                                   0.0\n",
      "alias                                                                        ifeval\n",
      "limit                                                                           NAN\n",
      "json_path                         eval_results/full_patching/meta-llama__Llama-3...\n",
      "dataset_str                                                                 unknown\n",
      "finetuning_params                                                               NAN\n",
      "total_batch_size                                                                NAN\n",
      "learning_rate                                                                   NAN\n",
      "main_loss_type                                                                  NAN\n",
      "embedding_init_strategy                                                         NAN\n",
      "num_new_tokens                                                                  NAN\n",
      "unfreeze_params_steps                                                           NAN\n",
      "finetune_params_prefreeze                                                       NAN\n",
      "dataset                                                                         NAN\n",
      "tokenizer_path                                                                  NAN\n",
      "seed                                                                            NAN\n",
      "reset_optimizer                                                                 NAN\n",
      "warmup_steps                                                                    NAN\n",
      "lr_schedule                                                                     NAN\n",
      "warmup_steps_prefreeze                                                          NAN\n",
      "lr_schedule_prefreeze                                                           NAN\n",
      "Name: 37, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>num_new_tokens</th>\n",
       "      <th>finetuning_params</th>\n",
       "      <th>model_name</th>\n",
       "      <th>trained_model_name</th>\n",
       "      <th>model_name_sanitized</th>\n",
       "      <th>dataset_str</th>\n",
       "      <th>embeddings_init</th>\n",
       "      <th colspan=\"4\" halign=\"left\">prompt_level_strict_acc</th>\n",
       "      <th colspan=\"4\" halign=\"left\">inst_level_strict_acc</th>\n",
       "      <th colspan=\"4\" halign=\"left\">prompt_level_loose_acc</th>\n",
       "      <th colspan=\"4\" halign=\"left\">inst_level_loose_acc</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>learning_ratio</th>\n",
       "      <th>theoretical_compression_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>74ed1b01-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.153420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153420</td>\n",
       "      <td>0.285372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.285372</td>\n",
       "      <td>0.166359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166359</td>\n",
       "      <td>0.303357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.303357</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>74ed1b01-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.415896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.415896</td>\n",
       "      <td>0.558753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.558753</td>\n",
       "      <td>0.517560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.517560</td>\n",
       "      <td>0.640288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.640288</td>\n",
       "      <td>0.012280</td>\n",
       "      <td>-0.002480</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>74ed1b01-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.700555</td>\n",
       "      <td>0.005228</td>\n",
       "      <td>2</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>0.782974</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>2</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>0.748614</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>0.819544</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>2</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>0.012387</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>d1f6703c-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.696858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.696858</td>\n",
       "      <td>0.781775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.781775</td>\n",
       "      <td>0.746765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.746765</td>\n",
       "      <td>0.818945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818945</td>\n",
       "      <td>-0.010761</td>\n",
       "      <td>0.012375</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__d1f6703c-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__d1f6703c-Llama-3.2-3B-m...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.149723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.149723</td>\n",
       "      <td>0.267386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.267386</td>\n",
       "      <td>0.162662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.162662</td>\n",
       "      <td>0.284173</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.284173</td>\n",
       "      <td>-0.001223</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>368f5252-Llama-3.2-3B-Instruct-mixed-0</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.556377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.556377</td>\n",
       "      <td>0.664269</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.664269</td>\n",
       "      <td>0.621072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.621072</td>\n",
       "      <td>0.724221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.724221</td>\n",
       "      <td>-0.011691</td>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__8d72bbbe-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__8d72bbbe-Llama-3.2-3B-I...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.554529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.554529</td>\n",
       "      <td>0.661871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.661871</td>\n",
       "      <td>0.609982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.609982</td>\n",
       "      <td>0.714628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.714628</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>8760b763-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.700555</td>\n",
       "      <td>0.005228</td>\n",
       "      <td>2</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>0.782974</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>2</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>0.748614</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>0.819544</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>2</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>0.012387</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>a84197a7-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.700555</td>\n",
       "      <td>0.005228</td>\n",
       "      <td>2</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>0.782974</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>2</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>0.748614</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>0.819544</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>2</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>0.012387</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__8760b763-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__8760b763-Llama-3.2-3B-m...</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.205176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.205176</td>\n",
       "      <td>0.327338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327338</td>\n",
       "      <td>0.223660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.223660</td>\n",
       "      <td>0.351319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.351319</td>\n",
       "      <td>-0.000266</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__a84197a7-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__a84197a7-Llama-3.2-3B-m...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>0.324940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.324940</td>\n",
       "      <td>0.218115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.218115</td>\n",
       "      <td>0.350120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.350120</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__c5764fce-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__c5764fce-Llama-3.2-3B-I...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>-0.010785</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__cb47c58d-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__cb47c58d-Llama-3.2-3B-I...</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>-0.010785</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>461ac385-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.171904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.171904</td>\n",
       "      <td>0.299760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.299760</td>\n",
       "      <td>0.179298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.179298</td>\n",
       "      <td>0.312950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.312950</td>\n",
       "      <td>0.017405</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.018738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>461ac385-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.414048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414048</td>\n",
       "      <td>0.541966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.541966</td>\n",
       "      <td>0.504621</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.504621</td>\n",
       "      <td>0.624700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.624700</td>\n",
       "      <td>-0.001514</td>\n",
       "      <td>0.021735</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>461ac385-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.696858</td>\n",
       "      <td>0.010456</td>\n",
       "      <td>2</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>2</td>\n",
       "      <td>0.788969</td>\n",
       "      <td>0.741220</td>\n",
       "      <td>0.010456</td>\n",
       "      <td>2</td>\n",
       "      <td>0.748614</td>\n",
       "      <td>0.820144</td>\n",
       "      <td>0.008478</td>\n",
       "      <td>2</td>\n",
       "      <td>0.826139</td>\n",
       "      <td>-0.010158</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.015584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>bd539cf5-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.685767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.685767</td>\n",
       "      <td>0.778177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.778177</td>\n",
       "      <td>0.741220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.741220</td>\n",
       "      <td>0.822542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.822542</td>\n",
       "      <td>-0.010551</td>\n",
       "      <td>0.028006</td>\n",
       "      <td>0.015644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__bd539cf5-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__bd539cf5-Llama-3.2-3B-m...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.177449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.177449</td>\n",
       "      <td>0.288969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.288969</td>\n",
       "      <td>0.199630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199630</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>0.014895</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.017731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>9e5180ab-Llama-3.2-3B-Instruct-mixed-10</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.534196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.534196</td>\n",
       "      <td>0.637890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.637890</td>\n",
       "      <td>0.582255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.582255</td>\n",
       "      <td>0.681055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.681055</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.039284</td>\n",
       "      <td>0.013212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__e93b6877-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__e93b6877-Llama-3.2-3B-I...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.528651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.528651</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>0.602588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.602588</td>\n",
       "      <td>0.706235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706235</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.017905</td>\n",
       "      <td>0.014476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>d2d49ae1-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.695933</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>2</td>\n",
       "      <td>0.698706</td>\n",
       "      <td>0.778777</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>2</td>\n",
       "      <td>0.779376</td>\n",
       "      <td>0.739372</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>2</td>\n",
       "      <td>0.741220</td>\n",
       "      <td>0.815348</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>2</td>\n",
       "      <td>0.816547</td>\n",
       "      <td>-0.008216</td>\n",
       "      <td>0.025708</td>\n",
       "      <td>0.015791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>daac169e-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.685767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.685767</td>\n",
       "      <td>0.770983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.770983</td>\n",
       "      <td>0.730129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.730129</td>\n",
       "      <td>0.809353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.809353</td>\n",
       "      <td>-0.008610</td>\n",
       "      <td>0.026663</td>\n",
       "      <td>0.016339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__d2d49ae1-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__d2d49ae1-Llama-3.2-3B-m...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.186691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186691</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>0.203327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.203327</td>\n",
       "      <td>0.341727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.341727</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>0.018304</td>\n",
       "      <td>0.016143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__daac169e-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__daac169e-Llama-3.2-3B-m...</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.188540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.188540</td>\n",
       "      <td>0.321343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.321343</td>\n",
       "      <td>0.207024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.207024</td>\n",
       "      <td>0.345324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.345324</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.015031</td>\n",
       "      <td>0.015013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__12f3cea0-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__12f3cea0-Llama-3.2-3B-I...</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>0.751799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751799</td>\n",
       "      <td>0.715342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.715342</td>\n",
       "      <td>0.798561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.798561</td>\n",
       "      <td>-0.001833</td>\n",
       "      <td>0.021714</td>\n",
       "      <td>0.018278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__f276bbe3-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__f276bbe3-Llama-3.2-3B-I...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.674677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.674677</td>\n",
       "      <td>0.763789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.763789</td>\n",
       "      <td>0.720887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.720887</td>\n",
       "      <td>0.800959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800959</td>\n",
       "      <td>-0.001153</td>\n",
       "      <td>0.021340</td>\n",
       "      <td>0.018647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>100.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>0a90e719-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>0.264988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.264988</td>\n",
       "      <td>0.164510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.164510</td>\n",
       "      <td>0.292566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.292566</td>\n",
       "      <td>0.035638</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.042507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>100.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>0a90e719-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.390018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.390018</td>\n",
       "      <td>0.534772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.534772</td>\n",
       "      <td>0.482440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.482440</td>\n",
       "      <td>0.609113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.609113</td>\n",
       "      <td>0.017502</td>\n",
       "      <td>-0.001952</td>\n",
       "      <td>0.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>100.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>0a90e719-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.662662</td>\n",
       "      <td>0.016991</td>\n",
       "      <td>2</td>\n",
       "      <td>0.674677</td>\n",
       "      <td>0.760192</td>\n",
       "      <td>0.015261</td>\n",
       "      <td>2</td>\n",
       "      <td>0.770983</td>\n",
       "      <td>0.718115</td>\n",
       "      <td>0.006535</td>\n",
       "      <td>2</td>\n",
       "      <td>0.722736</td>\n",
       "      <td>0.801559</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>2</td>\n",
       "      <td>0.806954</td>\n",
       "      <td>-0.007221</td>\n",
       "      <td>0.068312</td>\n",
       "      <td>0.059331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>2ecfe6ea-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.659889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.659889</td>\n",
       "      <td>0.760192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.760192</td>\n",
       "      <td>0.706100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706100</td>\n",
       "      <td>0.799760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.799760</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>0.068191</td>\n",
       "      <td>0.059037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>100.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__2ecfe6ea-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__2ecfe6ea-Llama-3.2-3B-m...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.142329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142329</td>\n",
       "      <td>0.260192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.260192</td>\n",
       "      <td>0.155268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.155268</td>\n",
       "      <td>0.282974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.282974</td>\n",
       "      <td>0.040349</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>0.046784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>100.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>958352ea-Llama-3.2-3B-Instruct-mixed-100</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.486137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.486137</td>\n",
       "      <td>0.606715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.606715</td>\n",
       "      <td>0.537893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.537893</td>\n",
       "      <td>0.653477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.653477</td>\n",
       "      <td>0.028351</td>\n",
       "      <td>0.025881</td>\n",
       "      <td>0.048398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>100.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__e5175fd2-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__e5175fd2-Llama-3.2-3B-I...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.519409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.519409</td>\n",
       "      <td>0.628297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.628297</td>\n",
       "      <td>0.574861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.574861</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.033028</td>\n",
       "      <td>0.020466</td>\n",
       "      <td>0.052770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>100.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>3c315a04-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.678373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.678373</td>\n",
       "      <td>0.768585</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>2</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>2</td>\n",
       "      <td>0.730129</td>\n",
       "      <td>0.807554</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>2</td>\n",
       "      <td>0.808153</td>\n",
       "      <td>-0.002833</td>\n",
       "      <td>0.063897</td>\n",
       "      <td>0.059611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>100.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>508aaf68-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.660813</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>2</td>\n",
       "      <td>0.663586</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>2</td>\n",
       "      <td>0.757794</td>\n",
       "      <td>0.713494</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>2</td>\n",
       "      <td>0.719039</td>\n",
       "      <td>0.796763</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>2</td>\n",
       "      <td>0.799760</td>\n",
       "      <td>-0.004923</td>\n",
       "      <td>0.066068</td>\n",
       "      <td>0.059580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>100.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__3c315a04-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__3c315a04-Llama-3.2-3B-m...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.194085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.194085</td>\n",
       "      <td>0.320144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.320144</td>\n",
       "      <td>0.210721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.210721</td>\n",
       "      <td>0.342926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.342926</td>\n",
       "      <td>0.039745</td>\n",
       "      <td>0.028531</td>\n",
       "      <td>0.064240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>100.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__508aaf68-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__508aaf68-Llama-3.2-3B-m...</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.207024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.207024</td>\n",
       "      <td>0.322542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.322542</td>\n",
       "      <td>0.227357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.227357</td>\n",
       "      <td>0.347722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.347722</td>\n",
       "      <td>0.031562</td>\n",
       "      <td>0.040201</td>\n",
       "      <td>0.068354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>100.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>2916d55c-Llama-3.2-3B-Instruct-mixed-100</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.685767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.685767</td>\n",
       "      <td>0.767386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.767386</td>\n",
       "      <td>0.739372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.739372</td>\n",
       "      <td>0.810552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.810552</td>\n",
       "      <td>0.020501</td>\n",
       "      <td>0.046151</td>\n",
       "      <td>0.066113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>100.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__5c0eb32b-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__5c0eb32b-Llama-3.2-3B-I...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.659889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.659889</td>\n",
       "      <td>0.758993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.758993</td>\n",
       "      <td>0.707948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.707948</td>\n",
       "      <td>0.798561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.798561</td>\n",
       "      <td>0.022516</td>\n",
       "      <td>0.045404</td>\n",
       "      <td>0.067439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>500.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>e45070ca-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>0.270983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.270983</td>\n",
       "      <td>0.149723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.149723</td>\n",
       "      <td>0.280576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.280576</td>\n",
       "      <td>0.087240</td>\n",
       "      <td>0.012155</td>\n",
       "      <td>0.100254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>500.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>e45070ca-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.343808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.343808</td>\n",
       "      <td>0.482014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.482014</td>\n",
       "      <td>0.428835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.428835</td>\n",
       "      <td>0.557554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.557554</td>\n",
       "      <td>0.009105</td>\n",
       "      <td>0.011711</td>\n",
       "      <td>0.013769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>500.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>e45070ca-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.651571</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>2</td>\n",
       "      <td>0.652495</td>\n",
       "      <td>0.746403</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>2</td>\n",
       "      <td>0.749400</td>\n",
       "      <td>0.691312</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693161</td>\n",
       "      <td>0.784173</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>2</td>\n",
       "      <td>0.786571</td>\n",
       "      <td>-0.001879</td>\n",
       "      <td>0.121165</td>\n",
       "      <td>0.118054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>500.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>e8e7ce8d-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.661738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.661738</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>0.687616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.687616</td>\n",
       "      <td>0.781775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.781775</td>\n",
       "      <td>-0.000953</td>\n",
       "      <td>0.121166</td>\n",
       "      <td>0.119027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>500.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__e8e7ce8d-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__e8e7ce8d-Llama-3.2-3B-m...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.166359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166359</td>\n",
       "      <td>0.276978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.276978</td>\n",
       "      <td>0.186691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186691</td>\n",
       "      <td>0.296163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.296163</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>0.100258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>500.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>0a4e765d-Llama-3.2-3B-Instruct-mixed-500</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.523105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.523105</td>\n",
       "      <td>0.622302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.622302</td>\n",
       "      <td>0.563771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.563771</td>\n",
       "      <td>0.660671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.660671</td>\n",
       "      <td>0.068671</td>\n",
       "      <td>0.034358</td>\n",
       "      <td>0.099375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>500.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__4d6edc1e-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__4d6edc1e-Llama-3.2-3B-I...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.510166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.510166</td>\n",
       "      <td>0.616307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.616307</td>\n",
       "      <td>0.548983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548983</td>\n",
       "      <td>0.658273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.658273</td>\n",
       "      <td>0.060745</td>\n",
       "      <td>0.040485</td>\n",
       "      <td>0.098851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>500.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>234bdfaa-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.643253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.643253</td>\n",
       "      <td>0.748201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.748201</td>\n",
       "      <td>0.667283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.667283</td>\n",
       "      <td>0.773381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.773381</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.116043</td>\n",
       "      <td>0.118194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>500.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>dfe1b80e-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.177449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.177449</td>\n",
       "      <td>0.288969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.288969</td>\n",
       "      <td>0.184843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.184843</td>\n",
       "      <td>0.296163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.296163</td>\n",
       "      <td>0.614980</td>\n",
       "      <td>0.019093</td>\n",
       "      <td>0.632944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>500.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>dfe1b80e-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.650647</td>\n",
       "      <td>0.005228</td>\n",
       "      <td>2</td>\n",
       "      <td>0.654344</td>\n",
       "      <td>0.745803</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750600</td>\n",
       "      <td>0.688540</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>2</td>\n",
       "      <td>0.695009</td>\n",
       "      <td>0.778177</td>\n",
       "      <td>0.011870</td>\n",
       "      <td>2</td>\n",
       "      <td>0.786571</td>\n",
       "      <td>-0.000597</td>\n",
       "      <td>0.118972</td>\n",
       "      <td>0.117214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>500.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__234bdfaa-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__234bdfaa-Llama-3.2-3B-m...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.197782</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.197782</td>\n",
       "      <td>0.309353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.309353</td>\n",
       "      <td>0.212569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.212569</td>\n",
       "      <td>0.338129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.338129</td>\n",
       "      <td>0.147659</td>\n",
       "      <td>0.043606</td>\n",
       "      <td>0.192847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>500.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>755abfca-Llama-3.2-3B-Instruct-mixed-500</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.628466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.628466</td>\n",
       "      <td>0.730216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.730216</td>\n",
       "      <td>0.674677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.674677</td>\n",
       "      <td>0.768585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.768585</td>\n",
       "      <td>0.036607</td>\n",
       "      <td>0.088294</td>\n",
       "      <td>0.126927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>500.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__4b9504d3-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__4b9504d3-Llama-3.2-3B-I...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>0.700555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700555</td>\n",
       "      <td>0.790168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.790168</td>\n",
       "      <td>0.037358</td>\n",
       "      <td>0.090833</td>\n",
       "      <td>0.130479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>461b9dfe-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.632163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.632163</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>0.669131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.669131</td>\n",
       "      <td>0.767386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.767386</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.145470</td>\n",
       "      <td>0.145546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>4eb8fba9-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.134935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.134935</td>\n",
       "      <td>0.262590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.262590</td>\n",
       "      <td>0.146026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.146026</td>\n",
       "      <td>0.273381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.273381</td>\n",
       "      <td>0.101597</td>\n",
       "      <td>0.022770</td>\n",
       "      <td>0.122494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>4eb8fba9-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.327172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327172</td>\n",
       "      <td>0.468825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.468825</td>\n",
       "      <td>0.414048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414048</td>\n",
       "      <td>0.552758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.552758</td>\n",
       "      <td>0.016155</td>\n",
       "      <td>-0.000890</td>\n",
       "      <td>0.014179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>4eb8fba9-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.640481</td>\n",
       "      <td>0.016991</td>\n",
       "      <td>2</td>\n",
       "      <td>0.652495</td>\n",
       "      <td>0.742206</td>\n",
       "      <td>0.010174</td>\n",
       "      <td>2</td>\n",
       "      <td>0.749400</td>\n",
       "      <td>0.680222</td>\n",
       "      <td>0.013070</td>\n",
       "      <td>2</td>\n",
       "      <td>0.689464</td>\n",
       "      <td>0.775180</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>2</td>\n",
       "      <td>0.780576</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.143656</td>\n",
       "      <td>0.145665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__461b9dfe-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__461b9dfe-Llama-3.2-3B-m...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.171904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.171904</td>\n",
       "      <td>0.282974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.282974</td>\n",
       "      <td>0.190388</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.190388</td>\n",
       "      <td>0.320144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.320144</td>\n",
       "      <td>0.121145</td>\n",
       "      <td>0.048861</td>\n",
       "      <td>0.147647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>9df250b6-Llama-3.2-3B-Instruct-mixed-1000</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.484288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.484288</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>0.534196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.534196</td>\n",
       "      <td>0.643885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.643885</td>\n",
       "      <td>0.081260</td>\n",
       "      <td>0.058773</td>\n",
       "      <td>0.118345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__8ce7f0d6-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__8ce7f0d6-Llama-3.2-3B-I...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.449168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.449168</td>\n",
       "      <td>0.577938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.577938</td>\n",
       "      <td>0.493530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493530</td>\n",
       "      <td>0.623501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623501</td>\n",
       "      <td>0.075506</td>\n",
       "      <td>0.038008</td>\n",
       "      <td>0.114864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>63d35748-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.658041</td>\n",
       "      <td>0.026141</td>\n",
       "      <td>2</td>\n",
       "      <td>0.676525</td>\n",
       "      <td>0.757194</td>\n",
       "      <td>0.019501</td>\n",
       "      <td>2</td>\n",
       "      <td>0.770983</td>\n",
       "      <td>0.693161</td>\n",
       "      <td>0.020913</td>\n",
       "      <td>2</td>\n",
       "      <td>0.707948</td>\n",
       "      <td>0.787170</td>\n",
       "      <td>0.014413</td>\n",
       "      <td>2</td>\n",
       "      <td>0.797362</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.140660</td>\n",
       "      <td>0.142585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>ba7c8b60-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>new_only</td>\n",
       "      <td>0.646026</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>2</td>\n",
       "      <td>0.648799</td>\n",
       "      <td>0.740408</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>2</td>\n",
       "      <td>0.742206</td>\n",
       "      <td>0.678373</td>\n",
       "      <td>0.010456</td>\n",
       "      <td>2</td>\n",
       "      <td>0.685767</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.005087</td>\n",
       "      <td>2</td>\n",
       "      <td>0.773381</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>0.137756</td>\n",
       "      <td>0.145391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__63d35748-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__63d35748-Llama-3.2-3B-m...</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.184843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.184843</td>\n",
       "      <td>0.293765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.293765</td>\n",
       "      <td>0.192237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.192237</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>0.158073</td>\n",
       "      <td>0.043921</td>\n",
       "      <td>0.204460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>output__full_patching__ba7c8b60-Llama-3.2-3B-m...</td>\n",
       "      <td>output__full_patching__ba7c8b60-Llama-3.2-3B-m...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.192237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.192237</td>\n",
       "      <td>0.305755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.305755</td>\n",
       "      <td>0.207024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.207024</td>\n",
       "      <td>0.329736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.329736</td>\n",
       "      <td>0.133380</td>\n",
       "      <td>0.034137</td>\n",
       "      <td>0.170377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>6e1e953a-Llama-3.2-3B-Instruct-mixed-1000</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.606285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.606285</td>\n",
       "      <td>0.703837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.703837</td>\n",
       "      <td>0.652495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652495</td>\n",
       "      <td>0.743405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.743405</td>\n",
       "      <td>0.042820</td>\n",
       "      <td>0.105766</td>\n",
       "      <td>0.150301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>output__full_patching__80c6810f-Llama-3.2-3B-I...</td>\n",
       "      <td>output__full_patching__80c6810f-Llama-3.2-3B-I...</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.621072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.621072</td>\n",
       "      <td>0.723022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.723022</td>\n",
       "      <td>0.661738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.661738</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.755396</td>\n",
       "      <td>0.044952</td>\n",
       "      <td>0.106657</td>\n",
       "      <td>0.155445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>0.324940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.324940</td>\n",
       "      <td>0.218115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.218115</td>\n",
       "      <td>0.350120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.350120</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>NAN</td>\n",
       "      <td>NAN</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.706100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706100</td>\n",
       "      <td>0.790168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.790168</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750462</td>\n",
       "      <td>0.822542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.822542</td>\n",
       "      <td>-0.010776</td>\n",
       "      <td>0.012392</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_new_tokens finetuning_params             model_name  \\\n",
       "                                                             \n",
       "0             0.0        embeddings           Llama-3.2-3B   \n",
       "1             0.0        embeddings           Llama-3.2-3B   \n",
       "2             0.0        embeddings           Llama-3.2-3B   \n",
       "3             0.0        embeddings           Llama-3.2-3B   \n",
       "4             0.0        embeddings           Llama-3.2-3B   \n",
       "5             0.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "6             0.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "7             0.0   new_tokens_only           Llama-3.2-3B   \n",
       "8             0.0   new_tokens_only           Llama-3.2-3B   \n",
       "9             0.0   new_tokens_only           Llama-3.2-3B   \n",
       "10            0.0   new_tokens_only           Llama-3.2-3B   \n",
       "11            0.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "12            0.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "13           10.0        embeddings           Llama-3.2-3B   \n",
       "14           10.0        embeddings           Llama-3.2-3B   \n",
       "15           10.0        embeddings           Llama-3.2-3B   \n",
       "16           10.0        embeddings           Llama-3.2-3B   \n",
       "17           10.0        embeddings           Llama-3.2-3B   \n",
       "18           10.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "19           10.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "20           10.0   new_tokens_only           Llama-3.2-3B   \n",
       "21           10.0   new_tokens_only           Llama-3.2-3B   \n",
       "22           10.0   new_tokens_only           Llama-3.2-3B   \n",
       "23           10.0   new_tokens_only           Llama-3.2-3B   \n",
       "24           10.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "25           10.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "26          100.0        embeddings           Llama-3.2-3B   \n",
       "27          100.0        embeddings           Llama-3.2-3B   \n",
       "28          100.0        embeddings           Llama-3.2-3B   \n",
       "29          100.0        embeddings           Llama-3.2-3B   \n",
       "30          100.0        embeddings           Llama-3.2-3B   \n",
       "31          100.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "32          100.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "33          100.0   new_tokens_only           Llama-3.2-3B   \n",
       "34          100.0   new_tokens_only           Llama-3.2-3B   \n",
       "35          100.0   new_tokens_only           Llama-3.2-3B   \n",
       "36          100.0   new_tokens_only           Llama-3.2-3B   \n",
       "37          100.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "38          100.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "39          500.0        embeddings           Llama-3.2-3B   \n",
       "40          500.0        embeddings           Llama-3.2-3B   \n",
       "41          500.0        embeddings           Llama-3.2-3B   \n",
       "42          500.0        embeddings           Llama-3.2-3B   \n",
       "43          500.0        embeddings           Llama-3.2-3B   \n",
       "44          500.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "45          500.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "46          500.0   new_tokens_only           Llama-3.2-3B   \n",
       "47          500.0   new_tokens_only           Llama-3.2-3B   \n",
       "48          500.0   new_tokens_only           Llama-3.2-3B   \n",
       "49          500.0   new_tokens_only           Llama-3.2-3B   \n",
       "50          500.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "51          500.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "52         1000.0        embeddings           Llama-3.2-3B   \n",
       "53         1000.0        embeddings           Llama-3.2-3B   \n",
       "54         1000.0        embeddings           Llama-3.2-3B   \n",
       "55         1000.0        embeddings           Llama-3.2-3B   \n",
       "56         1000.0        embeddings           Llama-3.2-3B   \n",
       "57         1000.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "58         1000.0        embeddings  Llama-3.2-3B-Instruct   \n",
       "59         1000.0   new_tokens_only           Llama-3.2-3B   \n",
       "60         1000.0   new_tokens_only           Llama-3.2-3B   \n",
       "61         1000.0   new_tokens_only           Llama-3.2-3B   \n",
       "62         1000.0   new_tokens_only           Llama-3.2-3B   \n",
       "63         1000.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "64         1000.0   new_tokens_only  Llama-3.2-3B-Instruct   \n",
       "65            NAN               NAN           Llama-3.2-3B   \n",
       "66            NAN               NAN  Llama-3.2-3B-Instruct   \n",
       "\n",
       "                                   trained_model_name  \\\n",
       "                                                        \n",
       "0                       74ed1b01-Llama-3.2-3B-mixed-0   \n",
       "1                       74ed1b01-Llama-3.2-3B-mixed-0   \n",
       "2                       74ed1b01-Llama-3.2-3B-mixed-0   \n",
       "3                       d1f6703c-Llama-3.2-3B-mixed-0   \n",
       "4   output__full_patching__d1f6703c-Llama-3.2-3B-m...   \n",
       "5              368f5252-Llama-3.2-3B-Instruct-mixed-0   \n",
       "6   output__full_patching__8d72bbbe-Llama-3.2-3B-I...   \n",
       "7                       8760b763-Llama-3.2-3B-mixed-0   \n",
       "8                       a84197a7-Llama-3.2-3B-mixed-0   \n",
       "9   output__full_patching__8760b763-Llama-3.2-3B-m...   \n",
       "10  output__full_patching__a84197a7-Llama-3.2-3B-m...   \n",
       "11  output__full_patching__c5764fce-Llama-3.2-3B-I...   \n",
       "12  output__full_patching__cb47c58d-Llama-3.2-3B-I...   \n",
       "13                     461ac385-Llama-3.2-3B-mixed-10   \n",
       "14                     461ac385-Llama-3.2-3B-mixed-10   \n",
       "15                     461ac385-Llama-3.2-3B-mixed-10   \n",
       "16                     bd539cf5-Llama-3.2-3B-mixed-10   \n",
       "17  output__full_patching__bd539cf5-Llama-3.2-3B-m...   \n",
       "18            9e5180ab-Llama-3.2-3B-Instruct-mixed-10   \n",
       "19  output__full_patching__e93b6877-Llama-3.2-3B-I...   \n",
       "20                     d2d49ae1-Llama-3.2-3B-mixed-10   \n",
       "21                     daac169e-Llama-3.2-3B-mixed-10   \n",
       "22  output__full_patching__d2d49ae1-Llama-3.2-3B-m...   \n",
       "23  output__full_patching__daac169e-Llama-3.2-3B-m...   \n",
       "24  output__full_patching__12f3cea0-Llama-3.2-3B-I...   \n",
       "25  output__full_patching__f276bbe3-Llama-3.2-3B-I...   \n",
       "26                    0a90e719-Llama-3.2-3B-mixed-100   \n",
       "27                    0a90e719-Llama-3.2-3B-mixed-100   \n",
       "28                    0a90e719-Llama-3.2-3B-mixed-100   \n",
       "29                    2ecfe6ea-Llama-3.2-3B-mixed-100   \n",
       "30  output__full_patching__2ecfe6ea-Llama-3.2-3B-m...   \n",
       "31           958352ea-Llama-3.2-3B-Instruct-mixed-100   \n",
       "32  output__full_patching__e5175fd2-Llama-3.2-3B-I...   \n",
       "33                    3c315a04-Llama-3.2-3B-mixed-100   \n",
       "34                    508aaf68-Llama-3.2-3B-mixed-100   \n",
       "35  output__full_patching__3c315a04-Llama-3.2-3B-m...   \n",
       "36  output__full_patching__508aaf68-Llama-3.2-3B-m...   \n",
       "37           2916d55c-Llama-3.2-3B-Instruct-mixed-100   \n",
       "38  output__full_patching__5c0eb32b-Llama-3.2-3B-I...   \n",
       "39                    e45070ca-Llama-3.2-3B-mixed-500   \n",
       "40                    e45070ca-Llama-3.2-3B-mixed-500   \n",
       "41                    e45070ca-Llama-3.2-3B-mixed-500   \n",
       "42                    e8e7ce8d-Llama-3.2-3B-mixed-500   \n",
       "43  output__full_patching__e8e7ce8d-Llama-3.2-3B-m...   \n",
       "44           0a4e765d-Llama-3.2-3B-Instruct-mixed-500   \n",
       "45  output__full_patching__4d6edc1e-Llama-3.2-3B-I...   \n",
       "46                    234bdfaa-Llama-3.2-3B-mixed-500   \n",
       "47                    dfe1b80e-Llama-3.2-3B-mixed-500   \n",
       "48                    dfe1b80e-Llama-3.2-3B-mixed-500   \n",
       "49  output__full_patching__234bdfaa-Llama-3.2-3B-m...   \n",
       "50           755abfca-Llama-3.2-3B-Instruct-mixed-500   \n",
       "51  output__full_patching__4b9504d3-Llama-3.2-3B-I...   \n",
       "52                   461b9dfe-Llama-3.2-3B-mixed-1000   \n",
       "53                   4eb8fba9-Llama-3.2-3B-mixed-1000   \n",
       "54                   4eb8fba9-Llama-3.2-3B-mixed-1000   \n",
       "55                   4eb8fba9-Llama-3.2-3B-mixed-1000   \n",
       "56  output__full_patching__461b9dfe-Llama-3.2-3B-m...   \n",
       "57          9df250b6-Llama-3.2-3B-Instruct-mixed-1000   \n",
       "58  output__full_patching__8ce7f0d6-Llama-3.2-3B-I...   \n",
       "59                   63d35748-Llama-3.2-3B-mixed-1000   \n",
       "60                   ba7c8b60-Llama-3.2-3B-mixed-1000   \n",
       "61  output__full_patching__63d35748-Llama-3.2-3B-m...   \n",
       "62  output__full_patching__ba7c8b60-Llama-3.2-3B-m...   \n",
       "63          6e1e953a-Llama-3.2-3B-Instruct-mixed-1000   \n",
       "64  output__full_patching__80c6810f-Llama-3.2-3B-I...   \n",
       "65                           meta-llama__Llama-3.2-3B   \n",
       "66                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "\n",
       "                                 model_name_sanitized           dataset_str  \\\n",
       "                                                                              \n",
       "0                            meta-llama__Llama-3.2-3B               default   \n",
       "1                   meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "2                   meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "3                   meta-llama__Llama-3.2-3B-Instruct  translation, default   \n",
       "4   output__full_patching__d1f6703c-Llama-3.2-3B-m...  translation, default   \n",
       "5                   meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "6   output__full_patching__8d72bbbe-Llama-3.2-3B-I...  translation, default   \n",
       "7                   meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "8                   meta-llama__Llama-3.2-3B-Instruct  translation, default   \n",
       "9   output__full_patching__8760b763-Llama-3.2-3B-m...               default   \n",
       "10  output__full_patching__a84197a7-Llama-3.2-3B-m...  translation, default   \n",
       "11  output__full_patching__c5764fce-Llama-3.2-3B-I...  translation, default   \n",
       "12  output__full_patching__cb47c58d-Llama-3.2-3B-I...               default   \n",
       "13                           meta-llama__Llama-3.2-3B               default   \n",
       "14                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "15                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "16                  meta-llama__Llama-3.2-3B-Instruct  translation, default   \n",
       "17  output__full_patching__bd539cf5-Llama-3.2-3B-m...  translation, default   \n",
       "18                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "19  output__full_patching__e93b6877-Llama-3.2-3B-I...  translation, default   \n",
       "20                  meta-llama__Llama-3.2-3B-Instruct  translation, default   \n",
       "21                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "22  output__full_patching__d2d49ae1-Llama-3.2-3B-m...  translation, default   \n",
       "23  output__full_patching__daac169e-Llama-3.2-3B-m...               default   \n",
       "24  output__full_patching__12f3cea0-Llama-3.2-3B-I...               default   \n",
       "25  output__full_patching__f276bbe3-Llama-3.2-3B-I...  translation, default   \n",
       "26                           meta-llama__Llama-3.2-3B               default   \n",
       "27                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "28                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "29                  meta-llama__Llama-3.2-3B-Instruct  translation, default   \n",
       "30  output__full_patching__2ecfe6ea-Llama-3.2-3B-m...  translation, default   \n",
       "31                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "32  output__full_patching__e5175fd2-Llama-3.2-3B-I...  translation, default   \n",
       "33                  meta-llama__Llama-3.2-3B-Instruct  translation, default   \n",
       "34                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "35  output__full_patching__3c315a04-Llama-3.2-3B-m...  translation, default   \n",
       "36  output__full_patching__508aaf68-Llama-3.2-3B-m...               default   \n",
       "37                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "38  output__full_patching__5c0eb32b-Llama-3.2-3B-I...  translation, default   \n",
       "39                           meta-llama__Llama-3.2-3B               default   \n",
       "40                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "41                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "42                  meta-llama__Llama-3.2-3B-Instruct  translation, default   \n",
       "43  output__full_patching__e8e7ce8d-Llama-3.2-3B-m...  translation, default   \n",
       "44                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "45  output__full_patching__4d6edc1e-Llama-3.2-3B-I...  translation, default   \n",
       "46                  meta-llama__Llama-3.2-3B-Instruct  translation, default   \n",
       "47                           meta-llama__Llama-3.2-3B               default   \n",
       "48                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "49  output__full_patching__234bdfaa-Llama-3.2-3B-m...  translation, default   \n",
       "50                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "51  output__full_patching__4b9504d3-Llama-3.2-3B-I...  translation, default   \n",
       "52                  meta-llama__Llama-3.2-3B-Instruct  translation, default   \n",
       "53                           meta-llama__Llama-3.2-3B               default   \n",
       "54                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "55                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "56  output__full_patching__461b9dfe-Llama-3.2-3B-m...  translation, default   \n",
       "57                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "58  output__full_patching__8ce7f0d6-Llama-3.2-3B-I...  translation, default   \n",
       "59                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "60                  meta-llama__Llama-3.2-3B-Instruct  translation, default   \n",
       "61  output__full_patching__63d35748-Llama-3.2-3B-m...               default   \n",
       "62  output__full_patching__ba7c8b60-Llama-3.2-3B-m...  translation, default   \n",
       "63                  meta-llama__Llama-3.2-3B-Instruct               default   \n",
       "64  output__full_patching__80c6810f-Llama-3.2-3B-I...  translation, default   \n",
       "65                           meta-llama__Llama-3.2-3B               unknown   \n",
       "66                  meta-llama__Llama-3.2-3B-Instruct               unknown   \n",
       "\n",
       "   embeddings_init prompt_level_strict_acc                            \\\n",
       "                                      mean       std count       max   \n",
       "0              NAN                0.153420       NaN     1  0.153420   \n",
       "1              NAN                0.415896       NaN     1  0.415896   \n",
       "2         new_only                0.700555  0.005228     2  0.704251   \n",
       "3         new_only                0.696858       NaN     1  0.696858   \n",
       "4              NAN                0.149723       NaN     1  0.149723   \n",
       "5              NAN                0.556377       NaN     1  0.556377   \n",
       "6              NAN                0.554529       NaN     1  0.554529   \n",
       "7         new_only                0.700555  0.005228     2  0.704251   \n",
       "8         new_only                0.700555  0.005228     2  0.704251   \n",
       "9              NAN                0.205176       NaN     1  0.205176   \n",
       "10             NAN                0.195933       NaN     1  0.195933   \n",
       "11             NAN                0.704251       NaN     1  0.704251   \n",
       "12             NAN                0.704251       NaN     1  0.704251   \n",
       "13             NAN                0.171904       NaN     1  0.171904   \n",
       "14             NAN                0.414048       NaN     1  0.414048   \n",
       "15        new_only                0.696858  0.010456     2  0.704251   \n",
       "16        new_only                0.685767       NaN     1  0.685767   \n",
       "17             NAN                0.177449       NaN     1  0.177449   \n",
       "18             NAN                0.534196       NaN     1  0.534196   \n",
       "19             NAN                0.528651       NaN     1  0.528651   \n",
       "20        new_only                0.695933  0.003921     2  0.698706   \n",
       "21        new_only                0.685767       NaN     1  0.685767   \n",
       "22             NAN                0.186691       NaN     1  0.186691   \n",
       "23             NAN                0.188540       NaN     1  0.188540   \n",
       "24             NAN                0.656192       NaN     1  0.656192   \n",
       "25             NAN                0.674677       NaN     1  0.674677   \n",
       "26             NAN                0.138632       NaN     1  0.138632   \n",
       "27             NAN                0.390018       NaN     1  0.390018   \n",
       "28        new_only                0.662662  0.016991     2  0.674677   \n",
       "29        new_only                0.659889       NaN     1  0.659889   \n",
       "30             NAN                0.142329       NaN     1  0.142329   \n",
       "31             NAN                0.486137       NaN     1  0.486137   \n",
       "32             NAN                0.519409       NaN     1  0.519409   \n",
       "33        new_only                0.678373  0.000000     2  0.678373   \n",
       "34        new_only                0.660813  0.003921     2  0.663586   \n",
       "35             NAN                0.194085       NaN     1  0.194085   \n",
       "36             NAN                0.207024       NaN     1  0.207024   \n",
       "37             NAN                0.685767       NaN     1  0.685767   \n",
       "38             NAN                0.659889       NaN     1  0.659889   \n",
       "39             NAN                0.138632       NaN     1  0.138632   \n",
       "40             NAN                0.343808       NaN     1  0.343808   \n",
       "41        new_only                0.651571  0.001307     2  0.652495   \n",
       "42        new_only                0.661738       NaN     1  0.661738   \n",
       "43             NAN                0.166359       NaN     1  0.166359   \n",
       "44             NAN                0.523105       NaN     1  0.523105   \n",
       "45             NAN                0.510166       NaN     1  0.510166   \n",
       "46        new_only                0.643253       NaN     1  0.643253   \n",
       "47             NAN                0.177449       NaN     1  0.177449   \n",
       "48        new_only                0.650647  0.005228     2  0.654344   \n",
       "49             NAN                0.197782       NaN     1  0.197782   \n",
       "50             NAN                0.628466       NaN     1  0.628466   \n",
       "51             NAN                0.656192       NaN     1  0.656192   \n",
       "52        new_only                0.632163       NaN     1  0.632163   \n",
       "53             NAN                0.134935       NaN     1  0.134935   \n",
       "54             NAN                0.327172       NaN     1  0.327172   \n",
       "55        new_only                0.640481  0.016991     2  0.652495   \n",
       "56             NAN                0.171904       NaN     1  0.171904   \n",
       "57             NAN                0.484288       NaN     1  0.484288   \n",
       "58             NAN                0.449168       NaN     1  0.449168   \n",
       "59        new_only                0.658041  0.026141     2  0.676525   \n",
       "60        new_only                0.646026  0.003921     2  0.648799   \n",
       "61             NAN                0.184843       NaN     1  0.184843   \n",
       "62             NAN                0.192237       NaN     1  0.192237   \n",
       "63             NAN                0.606285       NaN     1  0.606285   \n",
       "64             NAN                0.621072       NaN     1  0.621072   \n",
       "65             NAN                0.195933       NaN     1  0.195933   \n",
       "66             NAN                0.706100       NaN     1  0.706100   \n",
       "\n",
       "   inst_level_strict_acc                           prompt_level_loose_acc  \\\n",
       "                    mean       std count       max                   mean   \n",
       "0               0.285372       NaN     1  0.285372               0.166359   \n",
       "1               0.558753       NaN     1  0.558753               0.517560   \n",
       "2               0.782974  0.001696     2  0.784173               0.748614   \n",
       "3               0.781775       NaN     1  0.781775               0.746765   \n",
       "4               0.267386       NaN     1  0.267386               0.162662   \n",
       "5               0.664269       NaN     1  0.664269               0.621072   \n",
       "6               0.661871       NaN     1  0.661871               0.609982   \n",
       "7               0.782974  0.001696     2  0.784173               0.748614   \n",
       "8               0.782974  0.001696     2  0.784173               0.748614   \n",
       "9               0.327338       NaN     1  0.327338               0.223660   \n",
       "10              0.324940       NaN     1  0.324940               0.218115   \n",
       "11              0.784173       NaN     1  0.784173               0.750462   \n",
       "12              0.784173       NaN     1  0.784173               0.750462   \n",
       "13              0.299760       NaN     1  0.299760               0.179298   \n",
       "14              0.541966       NaN     1  0.541966               0.504621   \n",
       "15              0.784173  0.006783     2  0.788969               0.741220   \n",
       "16              0.778177       NaN     1  0.778177               0.741220   \n",
       "17              0.288969       NaN     1  0.288969               0.199630   \n",
       "18              0.637890       NaN     1  0.637890               0.582255   \n",
       "19              0.647482       NaN     1  0.647482               0.602588   \n",
       "20              0.778777  0.000848     2  0.779376               0.739372   \n",
       "21              0.770983       NaN     1  0.770983               0.730129   \n",
       "22              0.316547       NaN     1  0.316547               0.203327   \n",
       "23              0.321343       NaN     1  0.321343               0.207024   \n",
       "24              0.751799       NaN     1  0.751799               0.715342   \n",
       "25              0.763789       NaN     1  0.763789               0.720887   \n",
       "26              0.264988       NaN     1  0.264988               0.164510   \n",
       "27              0.534772       NaN     1  0.534772               0.482440   \n",
       "28              0.760192  0.015261     2  0.770983               0.718115   \n",
       "29              0.760192       NaN     1  0.760192               0.706100   \n",
       "30              0.260192       NaN     1  0.260192               0.155268   \n",
       "31              0.606715       NaN     1  0.606715               0.537893   \n",
       "32              0.628297       NaN     1  0.628297               0.574861   \n",
       "33              0.768585  0.001696     2  0.769784               0.728281   \n",
       "34              0.755396  0.003391     2  0.757794               0.713494   \n",
       "35              0.320144       NaN     1  0.320144               0.210721   \n",
       "36              0.322542       NaN     1  0.322542               0.227357   \n",
       "37              0.767386       NaN     1  0.767386               0.739372   \n",
       "38              0.758993       NaN     1  0.758993               0.707948   \n",
       "39              0.270983       NaN     1  0.270983               0.149723   \n",
       "40              0.482014       NaN     1  0.482014               0.428835   \n",
       "41              0.746403  0.004239     2  0.749400               0.691312   \n",
       "42              0.755396       NaN     1  0.755396               0.687616   \n",
       "43              0.276978       NaN     1  0.276978               0.186691   \n",
       "44              0.622302       NaN     1  0.622302               0.563771   \n",
       "45              0.616307       NaN     1  0.616307               0.548983   \n",
       "46              0.748201       NaN     1  0.748201               0.667283   \n",
       "47              0.288969       NaN     1  0.288969               0.184843   \n",
       "48              0.745803  0.006783     2  0.750600               0.688540   \n",
       "49              0.309353       NaN     1  0.309353               0.212569   \n",
       "50              0.730216       NaN     1  0.730216               0.674677   \n",
       "51              0.755396       NaN     1  0.755396               0.700555   \n",
       "52              0.733813       NaN     1  0.733813               0.669131   \n",
       "53              0.262590       NaN     1  0.262590               0.146026   \n",
       "54              0.468825       NaN     1  0.468825               0.414048   \n",
       "55              0.742206  0.010174     2  0.749400               0.680222   \n",
       "56              0.282974       NaN     1  0.282974               0.190388   \n",
       "57              0.593525       NaN     1  0.593525               0.534196   \n",
       "58              0.577938       NaN     1  0.577938               0.493530   \n",
       "59              0.757194  0.019501     2  0.770983               0.693161   \n",
       "60              0.740408  0.002544     2  0.742206               0.678373   \n",
       "61              0.293765       NaN     1  0.293765               0.192237   \n",
       "62              0.305755       NaN     1  0.305755               0.207024   \n",
       "63              0.703837       NaN     1  0.703837               0.652495   \n",
       "64              0.723022       NaN     1  0.723022               0.661738   \n",
       "65              0.324940       NaN     1  0.324940               0.218115   \n",
       "66              0.790168       NaN     1  0.790168               0.750462   \n",
       "\n",
       "                             inst_level_loose_acc                            \\\n",
       "         std count       max                 mean       std count       max   \n",
       "0        NaN     1  0.166359             0.303357       NaN     1  0.303357   \n",
       "1        NaN     1  0.517560             0.640288       NaN     1  0.640288   \n",
       "2   0.002614     2  0.750462             0.819544  0.000848     2  0.820144   \n",
       "3        NaN     1  0.746765             0.818945       NaN     1  0.818945   \n",
       "4        NaN     1  0.162662             0.284173       NaN     1  0.284173   \n",
       "5        NaN     1  0.621072             0.724221       NaN     1  0.724221   \n",
       "6        NaN     1  0.609982             0.714628       NaN     1  0.714628   \n",
       "7   0.002614     2  0.750462             0.819544  0.000848     2  0.820144   \n",
       "8   0.002614     2  0.750462             0.819544  0.000848     2  0.820144   \n",
       "9        NaN     1  0.223660             0.351319       NaN     1  0.351319   \n",
       "10       NaN     1  0.218115             0.350120       NaN     1  0.350120   \n",
       "11       NaN     1  0.750462             0.820144       NaN     1  0.820144   \n",
       "12       NaN     1  0.750462             0.820144       NaN     1  0.820144   \n",
       "13       NaN     1  0.179298             0.312950       NaN     1  0.312950   \n",
       "14       NaN     1  0.504621             0.624700       NaN     1  0.624700   \n",
       "15  0.010456     2  0.748614             0.820144  0.008478     2  0.826139   \n",
       "16       NaN     1  0.741220             0.822542       NaN     1  0.822542   \n",
       "17       NaN     1  0.199630             0.316547       NaN     1  0.316547   \n",
       "18       NaN     1  0.582255             0.681055       NaN     1  0.681055   \n",
       "19       NaN     1  0.602588             0.706235       NaN     1  0.706235   \n",
       "20  0.002614     2  0.741220             0.815348  0.001696     2  0.816547   \n",
       "21       NaN     1  0.730129             0.809353       NaN     1  0.809353   \n",
       "22       NaN     1  0.203327             0.341727       NaN     1  0.341727   \n",
       "23       NaN     1  0.207024             0.345324       NaN     1  0.345324   \n",
       "24       NaN     1  0.715342             0.798561       NaN     1  0.798561   \n",
       "25       NaN     1  0.720887             0.800959       NaN     1  0.800959   \n",
       "26       NaN     1  0.164510             0.292566       NaN     1  0.292566   \n",
       "27       NaN     1  0.482440             0.609113       NaN     1  0.609113   \n",
       "28  0.006535     2  0.722736             0.801559  0.007631     2  0.806954   \n",
       "29       NaN     1  0.706100             0.799760       NaN     1  0.799760   \n",
       "30       NaN     1  0.155268             0.282974       NaN     1  0.282974   \n",
       "31       NaN     1  0.537893             0.653477       NaN     1  0.653477   \n",
       "32       NaN     1  0.574861             0.679856       NaN     1  0.679856   \n",
       "33  0.002614     2  0.730129             0.807554  0.000848     2  0.808153   \n",
       "34  0.007842     2  0.719039             0.796763  0.004239     2  0.799760   \n",
       "35       NaN     1  0.210721             0.342926       NaN     1  0.342926   \n",
       "36       NaN     1  0.227357             0.347722       NaN     1  0.347722   \n",
       "37       NaN     1  0.739372             0.810552       NaN     1  0.810552   \n",
       "38       NaN     1  0.707948             0.798561       NaN     1  0.798561   \n",
       "39       NaN     1  0.149723             0.280576       NaN     1  0.280576   \n",
       "40       NaN     1  0.428835             0.557554       NaN     1  0.557554   \n",
       "41  0.002614     2  0.693161             0.784173  0.003391     2  0.786571   \n",
       "42       NaN     1  0.687616             0.781775       NaN     1  0.781775   \n",
       "43       NaN     1  0.186691             0.296163       NaN     1  0.296163   \n",
       "44       NaN     1  0.563771             0.660671       NaN     1  0.660671   \n",
       "45       NaN     1  0.548983             0.658273       NaN     1  0.658273   \n",
       "46       NaN     1  0.667283             0.773381       NaN     1  0.773381   \n",
       "47       NaN     1  0.184843             0.296163       NaN     1  0.296163   \n",
       "48  0.009149     2  0.695009             0.778177  0.011870     2  0.786571   \n",
       "49       NaN     1  0.212569             0.338129       NaN     1  0.338129   \n",
       "50       NaN     1  0.674677             0.768585       NaN     1  0.768585   \n",
       "51       NaN     1  0.700555             0.790168       NaN     1  0.790168   \n",
       "52       NaN     1  0.669131             0.767386       NaN     1  0.767386   \n",
       "53       NaN     1  0.146026             0.273381       NaN     1  0.273381   \n",
       "54       NaN     1  0.414048             0.552758       NaN     1  0.552758   \n",
       "55  0.013070     2  0.689464             0.775180  0.007631     2  0.780576   \n",
       "56       NaN     1  0.190388             0.320144       NaN     1  0.320144   \n",
       "57       NaN     1  0.534196             0.643885       NaN     1  0.643885   \n",
       "58       NaN     1  0.493530             0.623501       NaN     1  0.623501   \n",
       "59  0.020913     2  0.707948             0.787170  0.014413     2  0.797362   \n",
       "60  0.010456     2  0.685767             0.769784  0.005087     2  0.773381   \n",
       "61       NaN     1  0.192237             0.316547       NaN     1  0.316547   \n",
       "62       NaN     1  0.207024             0.329736       NaN     1  0.329736   \n",
       "63       NaN     1  0.652495             0.743405       NaN     1  0.743405   \n",
       "64       NaN     1  0.661738             0.755396       NaN     1  0.755396   \n",
       "65       NaN     1  0.218115             0.350120       NaN     1  0.350120   \n",
       "66       NaN     1  0.750462             0.822542       NaN     1  0.822542   \n",
       "\n",
       "   compression_ratio learning_ratio theoretical_compression_ratio  \n",
       "                mean           mean                          mean  \n",
       "0          -0.000226       0.000229                      0.000000  \n",
       "1           0.012280      -0.002480                      0.000000  \n",
       "2          -0.010773       0.012387                      0.000000  \n",
       "3          -0.010761       0.012375                      0.000000  \n",
       "4          -0.001223       0.001373                      0.000000  \n",
       "5          -0.011691       0.017971                      0.000000  \n",
       "6          -0.010000       0.011621                      0.000000  \n",
       "7          -0.010773       0.012387                      0.000000  \n",
       "8          -0.010773       0.012387                      0.000000  \n",
       "9          -0.000266       0.006193                      0.000000  \n",
       "10          0.001207       0.004820                      0.000000  \n",
       "11         -0.010785       0.012398                      0.000000  \n",
       "12         -0.010785       0.012398                      0.000000  \n",
       "13          0.017405       0.001320                      0.018738  \n",
       "14         -0.001514       0.021735                      0.003100  \n",
       "15         -0.010158       0.027514                      0.015584  \n",
       "16         -0.010551       0.028006                      0.015644  \n",
       "17          0.014895       0.002959                      0.017731  \n",
       "18          0.000824       0.039284                      0.013212  \n",
       "19          0.001851       0.017905                      0.014476  \n",
       "20         -0.008216       0.025708                      0.015791  \n",
       "21         -0.008610       0.026663                      0.016339  \n",
       "22          0.006792       0.018304                      0.016143  \n",
       "23          0.008682       0.015031                      0.015013  \n",
       "24         -0.001833       0.021714                      0.018278  \n",
       "25         -0.001153       0.021340                      0.018647  \n",
       "26          0.035638       0.006736                      0.042507  \n",
       "27          0.017502      -0.001952                      0.010700  \n",
       "28         -0.007221       0.068312                      0.059331  \n",
       "29         -0.007376       0.068191                      0.059037  \n",
       "30          0.040349       0.006330                      0.046784  \n",
       "31          0.028351       0.025881                      0.048398  \n",
       "32          0.033028       0.020466                      0.052770  \n",
       "33         -0.002833       0.063897                      0.059611  \n",
       "34         -0.004923       0.066068                      0.059580  \n",
       "35          0.039745       0.028531                      0.064240  \n",
       "36          0.031562       0.040201                      0.068354  \n",
       "37          0.020501       0.046151                      0.066113  \n",
       "38          0.022516       0.045404                      0.067439  \n",
       "39          0.087240       0.012155                      0.100254  \n",
       "40          0.009105       0.011711                      0.013769  \n",
       "41         -0.001879       0.121165                      0.118054  \n",
       "42         -0.000953       0.121166                      0.119027  \n",
       "43          0.089000       0.010453                      0.100258  \n",
       "44          0.068671       0.034358                      0.099375  \n",
       "45          0.060745       0.040485                      0.098851  \n",
       "46          0.002964       0.116043                      0.118194  \n",
       "47          0.614980       0.019093                      0.632944  \n",
       "48         -0.000597       0.118972                      0.117214  \n",
       "49          0.147659       0.043606                      0.192847  \n",
       "50          0.036607       0.088294                      0.126927  \n",
       "51          0.037358       0.090833                      0.130479  \n",
       "52          0.000799       0.145470                      0.145546  \n",
       "53          0.101597       0.022770                      0.122494  \n",
       "54          0.016155      -0.000890                      0.014179  \n",
       "55          0.002476       0.143656                      0.145665  \n",
       "56          0.121145       0.048861                      0.147647  \n",
       "57          0.081260       0.058773                      0.118345  \n",
       "58          0.075506       0.038008                      0.114864  \n",
       "59          0.002421       0.140660                      0.142585  \n",
       "60          0.007525       0.137756                      0.145391  \n",
       "61          0.158073       0.043921                      0.204460  \n",
       "62          0.133380       0.034137                      0.170377  \n",
       "63          0.042820       0.105766                      0.150301  \n",
       "64          0.044952       0.106657                      0.155445  \n",
       "65          0.001207       0.004820                      0.000000  \n",
       "66         -0.010776       0.012392                      0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>new_embeddings_init</th>\n",
       "      <th>finetuning</th>\n",
       "      <th>dataset</th>\n",
       "      <th>trained_model_name</th>\n",
       "      <th>num_new_tokens</th>\n",
       "      <th>scores</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>theoretical_compression_ratio</th>\n",
       "      <th>learning_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>74ed1b01-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.227127</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_all_embeddings</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>74ed1b01-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.533124</td>\n",
       "      <td>0.012280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>74ed1b01-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.762922</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>d1f6703c-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.761086</td>\n",
       "      <td>-0.010761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__d1f6703c-Llama-3.2-3B-m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.215986</td>\n",
       "      <td>-0.001223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>368f5252-Llama-3.2-3B-Instruct-mixed-0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.641485</td>\n",
       "      <td>-0.011691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__8d72bbbe-Llama-3.2-3B-I...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.635252</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>8760b763-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.762922</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>a84197a7-Llama-3.2-3B-mixed-0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.762922</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>output__full_patching__8760b763-Llama-3.2-3B-m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.276873</td>\n",
       "      <td>-0.000266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__a84197a7-Llama-3.2-3B-m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.272277</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__c5764fce-Llama-3.2-3B-I...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.764758</td>\n",
       "      <td>-0.010785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>output__full_patching__cb47c58d-Llama-3.2-3B-I...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.764758</td>\n",
       "      <td>-0.010785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>461ac385-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.240978</td>\n",
       "      <td>0.017405</td>\n",
       "      <td>0.018738</td>\n",
       "      <td>0.001320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_all_embeddings</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>461ac385-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.521334</td>\n",
       "      <td>-0.001514</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.021735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>461ac385-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.760599</td>\n",
       "      <td>-0.010158</td>\n",
       "      <td>0.015584</td>\n",
       "      <td>0.027514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>bd539cf5-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.756927</td>\n",
       "      <td>-0.010551</td>\n",
       "      <td>0.015644</td>\n",
       "      <td>0.028006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__bd539cf5-Llama-3.2-3B-m...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.245649</td>\n",
       "      <td>0.014895</td>\n",
       "      <td>0.017731</td>\n",
       "      <td>0.002959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>9e5180ab-Llama-3.2-3B-Instruct-mixed-10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.608849</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.013212</td>\n",
       "      <td>0.039284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__e93b6877-Llama-3.2-3B-I...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.621239</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.014476</td>\n",
       "      <td>0.017905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>d2d49ae1-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.757357</td>\n",
       "      <td>-0.008216</td>\n",
       "      <td>0.015791</td>\n",
       "      <td>0.025708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>daac169e-Llama-3.2-3B-mixed-10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.749058</td>\n",
       "      <td>-0.008610</td>\n",
       "      <td>0.016339</td>\n",
       "      <td>0.026663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__d2d49ae1-Llama-3.2-3B-m...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.262073</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>0.016143</td>\n",
       "      <td>0.018304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>output__full_patching__daac169e-Llama-3.2-3B-m...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.265558</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.015013</td>\n",
       "      <td>0.015031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>output__full_patching__12f3cea0-Llama-3.2-3B-I...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.730473</td>\n",
       "      <td>-0.001833</td>\n",
       "      <td>0.018278</td>\n",
       "      <td>0.021714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__f276bbe3-Llama-3.2-3B-I...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.740078</td>\n",
       "      <td>-0.001153</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>0.021340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>0a90e719-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.215174</td>\n",
       "      <td>0.035638</td>\n",
       "      <td>0.042507</td>\n",
       "      <td>0.006736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_all_embeddings</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>0a90e719-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.504086</td>\n",
       "      <td>0.017502</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>-0.001952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>0a90e719-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>-0.007221</td>\n",
       "      <td>0.059331</td>\n",
       "      <td>0.068312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>2ecfe6ea-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.731485</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>0.059037</td>\n",
       "      <td>0.068191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__2ecfe6ea-Llama-3.2-3B-m...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.210191</td>\n",
       "      <td>0.040349</td>\n",
       "      <td>0.046784</td>\n",
       "      <td>0.006330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>958352ea-Llama-3.2-3B-Instruct-mixed-100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.571055</td>\n",
       "      <td>0.028351</td>\n",
       "      <td>0.048398</td>\n",
       "      <td>0.025881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__e5175fd2-Llama-3.2-3B-I...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.600606</td>\n",
       "      <td>0.033028</td>\n",
       "      <td>0.052770</td>\n",
       "      <td>0.020466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>3c315a04-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.745698</td>\n",
       "      <td>-0.002833</td>\n",
       "      <td>0.059611</td>\n",
       "      <td>0.063897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>508aaf68-Llama-3.2-3B-mixed-100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.731616</td>\n",
       "      <td>-0.004923</td>\n",
       "      <td>0.059580</td>\n",
       "      <td>0.066068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__3c315a04-Llama-3.2-3B-m...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.266969</td>\n",
       "      <td>0.039745</td>\n",
       "      <td>0.064240</td>\n",
       "      <td>0.028531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>output__full_patching__508aaf68-Llama-3.2-3B-m...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.276161</td>\n",
       "      <td>0.031562</td>\n",
       "      <td>0.068354</td>\n",
       "      <td>0.040201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>2916d55c-Llama-3.2-3B-Instruct-mixed-100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.750769</td>\n",
       "      <td>0.020501</td>\n",
       "      <td>0.066113</td>\n",
       "      <td>0.046151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__5c0eb32b-Llama-3.2-3B-I...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.731348</td>\n",
       "      <td>0.022516</td>\n",
       "      <td>0.067439</td>\n",
       "      <td>0.045404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>e45070ca-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>500</td>\n",
       "      <td>0.209978</td>\n",
       "      <td>0.087240</td>\n",
       "      <td>0.100254</td>\n",
       "      <td>0.012155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_all_embeddings</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>e45070ca-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>500</td>\n",
       "      <td>0.453053</td>\n",
       "      <td>0.009105</td>\n",
       "      <td>0.013769</td>\n",
       "      <td>0.011711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>e45070ca-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>500</td>\n",
       "      <td>0.718365</td>\n",
       "      <td>-0.001879</td>\n",
       "      <td>0.118054</td>\n",
       "      <td>0.121165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>e8e7ce8d-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>500</td>\n",
       "      <td>0.721631</td>\n",
       "      <td>-0.000953</td>\n",
       "      <td>0.119027</td>\n",
       "      <td>0.121166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__e8e7ce8d-Llama-3.2-3B-m...</td>\n",
       "      <td>500</td>\n",
       "      <td>0.231548</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.100258</td>\n",
       "      <td>0.010453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>0a4e765d-Llama-3.2-3B-Instruct-mixed-500</td>\n",
       "      <td>500</td>\n",
       "      <td>0.592462</td>\n",
       "      <td>0.068671</td>\n",
       "      <td>0.099375</td>\n",
       "      <td>0.034358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__4d6edc1e-Llama-3.2-3B-I...</td>\n",
       "      <td>500</td>\n",
       "      <td>0.583433</td>\n",
       "      <td>0.060745</td>\n",
       "      <td>0.098851</td>\n",
       "      <td>0.040485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>234bdfaa-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>500</td>\n",
       "      <td>0.708030</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.118194</td>\n",
       "      <td>0.116043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>dfe1b80e-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>500</td>\n",
       "      <td>0.236856</td>\n",
       "      <td>0.614980</td>\n",
       "      <td>0.632944</td>\n",
       "      <td>0.019093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>dfe1b80e-Llama-3.2-3B-mixed-500</td>\n",
       "      <td>500</td>\n",
       "      <td>0.715792</td>\n",
       "      <td>-0.000597</td>\n",
       "      <td>0.117214</td>\n",
       "      <td>0.118972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__234bdfaa-Llama-3.2-3B-m...</td>\n",
       "      <td>500</td>\n",
       "      <td>0.264458</td>\n",
       "      <td>0.147659</td>\n",
       "      <td>0.192847</td>\n",
       "      <td>0.043606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>755abfca-Llama-3.2-3B-Instruct-mixed-500</td>\n",
       "      <td>500</td>\n",
       "      <td>0.700486</td>\n",
       "      <td>0.036607</td>\n",
       "      <td>0.126927</td>\n",
       "      <td>0.088294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__4b9504d3-Llama-3.2-3B-I...</td>\n",
       "      <td>500</td>\n",
       "      <td>0.725578</td>\n",
       "      <td>0.037358</td>\n",
       "      <td>0.130479</td>\n",
       "      <td>0.090833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>461b9dfe-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.700623</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.145546</td>\n",
       "      <td>0.145470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>4eb8fba9-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.204233</td>\n",
       "      <td>0.101597</td>\n",
       "      <td>0.122494</td>\n",
       "      <td>0.022770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_all_embeddings</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>4eb8fba9-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.440701</td>\n",
       "      <td>0.016155</td>\n",
       "      <td>0.014179</td>\n",
       "      <td>-0.000890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>4eb8fba9-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.709522</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.145665</td>\n",
       "      <td>0.143656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__461b9dfe-Llama-3.2-3B-m...</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.241352</td>\n",
       "      <td>0.121145</td>\n",
       "      <td>0.147647</td>\n",
       "      <td>0.048861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>default</td>\n",
       "      <td>9df250b6-Llama-3.2-3B-Instruct-mixed-1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.563974</td>\n",
       "      <td>0.081260</td>\n",
       "      <td>0.118345</td>\n",
       "      <td>0.058773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__8ce7f0d6-Llama-3.2-3B-I...</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.536034</td>\n",
       "      <td>0.075506</td>\n",
       "      <td>0.114864</td>\n",
       "      <td>0.038008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>63d35748-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.723891</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.142585</td>\n",
       "      <td>0.140660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>patched_new_tokens</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>ba7c8b60-Llama-3.2-3B-mixed-1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.708648</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>0.145391</td>\n",
       "      <td>0.137756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>output__full_patching__63d35748-Llama-3.2-3B-m...</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.246848</td>\n",
       "      <td>0.158073</td>\n",
       "      <td>0.204460</td>\n",
       "      <td>0.043921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__ba7c8b60-Llama-3.2-3B-m...</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.258688</td>\n",
       "      <td>0.133380</td>\n",
       "      <td>0.170377</td>\n",
       "      <td>0.034137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>default</td>\n",
       "      <td>6e1e953a-Llama-3.2-3B-Instruct-mixed-1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.676506</td>\n",
       "      <td>0.042820</td>\n",
       "      <td>0.150301</td>\n",
       "      <td>0.105766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__80c6810f-Llama-3.2-3B-I...</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.690307</td>\n",
       "      <td>0.044952</td>\n",
       "      <td>0.155445</td>\n",
       "      <td>0.106657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>None</td>\n",
       "      <td>unknown</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.272277</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>NAN</td>\n",
       "      <td>None</td>\n",
       "      <td>unknown</td>\n",
       "      <td>meta-llama__Llama-3.2-3B-Instruct</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.767318</td>\n",
       "      <td>-0.010776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name     new_embeddings_init       finetuning  \\\n",
       "0            Llama-3.2-3B                     NAN       embeddings   \n",
       "1   Llama-3.2-3B-Instruct  patched_all_embeddings       embeddings   \n",
       "2   Llama-3.2-3B-Instruct      patched_new_tokens       embeddings   \n",
       "3   Llama-3.2-3B-Instruct      patched_new_tokens       embeddings   \n",
       "4            Llama-3.2-3B                     NAN       embeddings   \n",
       "5   Llama-3.2-3B-Instruct                     NAN       embeddings   \n",
       "6   Llama-3.2-3B-Instruct                     NAN       embeddings   \n",
       "7   Llama-3.2-3B-Instruct      patched_new_tokens  new_tokens_only   \n",
       "8   Llama-3.2-3B-Instruct      patched_new_tokens  new_tokens_only   \n",
       "9            Llama-3.2-3B                     NAN  new_tokens_only   \n",
       "10           Llama-3.2-3B                     NAN  new_tokens_only   \n",
       "11  Llama-3.2-3B-Instruct                     NAN  new_tokens_only   \n",
       "12  Llama-3.2-3B-Instruct                     NAN  new_tokens_only   \n",
       "13           Llama-3.2-3B                     NAN       embeddings   \n",
       "14  Llama-3.2-3B-Instruct  patched_all_embeddings       embeddings   \n",
       "15  Llama-3.2-3B-Instruct      patched_new_tokens       embeddings   \n",
       "16  Llama-3.2-3B-Instruct      patched_new_tokens       embeddings   \n",
       "17           Llama-3.2-3B                     NAN       embeddings   \n",
       "18  Llama-3.2-3B-Instruct                     NAN       embeddings   \n",
       "19  Llama-3.2-3B-Instruct                     NAN       embeddings   \n",
       "20  Llama-3.2-3B-Instruct      patched_new_tokens  new_tokens_only   \n",
       "21  Llama-3.2-3B-Instruct      patched_new_tokens  new_tokens_only   \n",
       "22           Llama-3.2-3B                     NAN  new_tokens_only   \n",
       "23           Llama-3.2-3B                     NAN  new_tokens_only   \n",
       "24  Llama-3.2-3B-Instruct                     NAN  new_tokens_only   \n",
       "25  Llama-3.2-3B-Instruct                     NAN  new_tokens_only   \n",
       "26           Llama-3.2-3B                     NAN       embeddings   \n",
       "27  Llama-3.2-3B-Instruct  patched_all_embeddings       embeddings   \n",
       "28  Llama-3.2-3B-Instruct      patched_new_tokens       embeddings   \n",
       "29  Llama-3.2-3B-Instruct      patched_new_tokens       embeddings   \n",
       "30           Llama-3.2-3B                     NAN       embeddings   \n",
       "31  Llama-3.2-3B-Instruct                     NAN       embeddings   \n",
       "32  Llama-3.2-3B-Instruct                     NAN       embeddings   \n",
       "33  Llama-3.2-3B-Instruct      patched_new_tokens  new_tokens_only   \n",
       "34  Llama-3.2-3B-Instruct      patched_new_tokens  new_tokens_only   \n",
       "35           Llama-3.2-3B                     NAN  new_tokens_only   \n",
       "36           Llama-3.2-3B                     NAN  new_tokens_only   \n",
       "37  Llama-3.2-3B-Instruct                     NAN  new_tokens_only   \n",
       "38  Llama-3.2-3B-Instruct                     NAN  new_tokens_only   \n",
       "39           Llama-3.2-3B                     NAN       embeddings   \n",
       "40  Llama-3.2-3B-Instruct  patched_all_embeddings       embeddings   \n",
       "41  Llama-3.2-3B-Instruct      patched_new_tokens       embeddings   \n",
       "42  Llama-3.2-3B-Instruct      patched_new_tokens       embeddings   \n",
       "43           Llama-3.2-3B                     NAN       embeddings   \n",
       "44  Llama-3.2-3B-Instruct                     NAN       embeddings   \n",
       "45  Llama-3.2-3B-Instruct                     NAN       embeddings   \n",
       "46  Llama-3.2-3B-Instruct      patched_new_tokens  new_tokens_only   \n",
       "47           Llama-3.2-3B                     NAN  new_tokens_only   \n",
       "48  Llama-3.2-3B-Instruct      patched_new_tokens  new_tokens_only   \n",
       "49           Llama-3.2-3B                     NAN  new_tokens_only   \n",
       "50  Llama-3.2-3B-Instruct                     NAN  new_tokens_only   \n",
       "51  Llama-3.2-3B-Instruct                     NAN  new_tokens_only   \n",
       "52  Llama-3.2-3B-Instruct      patched_new_tokens       embeddings   \n",
       "53           Llama-3.2-3B                     NAN       embeddings   \n",
       "54  Llama-3.2-3B-Instruct  patched_all_embeddings       embeddings   \n",
       "55  Llama-3.2-3B-Instruct      patched_new_tokens       embeddings   \n",
       "56           Llama-3.2-3B                     NAN       embeddings   \n",
       "57  Llama-3.2-3B-Instruct                     NAN       embeddings   \n",
       "58  Llama-3.2-3B-Instruct                     NAN       embeddings   \n",
       "59  Llama-3.2-3B-Instruct      patched_new_tokens  new_tokens_only   \n",
       "60  Llama-3.2-3B-Instruct      patched_new_tokens  new_tokens_only   \n",
       "61           Llama-3.2-3B                     NAN  new_tokens_only   \n",
       "62           Llama-3.2-3B                     NAN  new_tokens_only   \n",
       "63  Llama-3.2-3B-Instruct                     NAN  new_tokens_only   \n",
       "64  Llama-3.2-3B-Instruct                     NAN  new_tokens_only   \n",
       "65           Llama-3.2-3B                     NAN             None   \n",
       "66  Llama-3.2-3B-Instruct                     NAN             None   \n",
       "\n",
       "                 dataset                                 trained_model_name  \\\n",
       "0                default                      74ed1b01-Llama-3.2-3B-mixed-0   \n",
       "1                default                      74ed1b01-Llama-3.2-3B-mixed-0   \n",
       "2                default                      74ed1b01-Llama-3.2-3B-mixed-0   \n",
       "3   translation, default                      d1f6703c-Llama-3.2-3B-mixed-0   \n",
       "4   translation, default  output__full_patching__d1f6703c-Llama-3.2-3B-m...   \n",
       "5                default             368f5252-Llama-3.2-3B-Instruct-mixed-0   \n",
       "6   translation, default  output__full_patching__8d72bbbe-Llama-3.2-3B-I...   \n",
       "7                default                      8760b763-Llama-3.2-3B-mixed-0   \n",
       "8   translation, default                      a84197a7-Llama-3.2-3B-mixed-0   \n",
       "9                default  output__full_patching__8760b763-Llama-3.2-3B-m...   \n",
       "10  translation, default  output__full_patching__a84197a7-Llama-3.2-3B-m...   \n",
       "11  translation, default  output__full_patching__c5764fce-Llama-3.2-3B-I...   \n",
       "12               default  output__full_patching__cb47c58d-Llama-3.2-3B-I...   \n",
       "13               default                     461ac385-Llama-3.2-3B-mixed-10   \n",
       "14               default                     461ac385-Llama-3.2-3B-mixed-10   \n",
       "15               default                     461ac385-Llama-3.2-3B-mixed-10   \n",
       "16  translation, default                     bd539cf5-Llama-3.2-3B-mixed-10   \n",
       "17  translation, default  output__full_patching__bd539cf5-Llama-3.2-3B-m...   \n",
       "18               default            9e5180ab-Llama-3.2-3B-Instruct-mixed-10   \n",
       "19  translation, default  output__full_patching__e93b6877-Llama-3.2-3B-I...   \n",
       "20  translation, default                     d2d49ae1-Llama-3.2-3B-mixed-10   \n",
       "21               default                     daac169e-Llama-3.2-3B-mixed-10   \n",
       "22  translation, default  output__full_patching__d2d49ae1-Llama-3.2-3B-m...   \n",
       "23               default  output__full_patching__daac169e-Llama-3.2-3B-m...   \n",
       "24               default  output__full_patching__12f3cea0-Llama-3.2-3B-I...   \n",
       "25  translation, default  output__full_patching__f276bbe3-Llama-3.2-3B-I...   \n",
       "26               default                    0a90e719-Llama-3.2-3B-mixed-100   \n",
       "27               default                    0a90e719-Llama-3.2-3B-mixed-100   \n",
       "28               default                    0a90e719-Llama-3.2-3B-mixed-100   \n",
       "29  translation, default                    2ecfe6ea-Llama-3.2-3B-mixed-100   \n",
       "30  translation, default  output__full_patching__2ecfe6ea-Llama-3.2-3B-m...   \n",
       "31               default           958352ea-Llama-3.2-3B-Instruct-mixed-100   \n",
       "32  translation, default  output__full_patching__e5175fd2-Llama-3.2-3B-I...   \n",
       "33  translation, default                    3c315a04-Llama-3.2-3B-mixed-100   \n",
       "34               default                    508aaf68-Llama-3.2-3B-mixed-100   \n",
       "35  translation, default  output__full_patching__3c315a04-Llama-3.2-3B-m...   \n",
       "36               default  output__full_patching__508aaf68-Llama-3.2-3B-m...   \n",
       "37               default           2916d55c-Llama-3.2-3B-Instruct-mixed-100   \n",
       "38  translation, default  output__full_patching__5c0eb32b-Llama-3.2-3B-I...   \n",
       "39               default                    e45070ca-Llama-3.2-3B-mixed-500   \n",
       "40               default                    e45070ca-Llama-3.2-3B-mixed-500   \n",
       "41               default                    e45070ca-Llama-3.2-3B-mixed-500   \n",
       "42  translation, default                    e8e7ce8d-Llama-3.2-3B-mixed-500   \n",
       "43  translation, default  output__full_patching__e8e7ce8d-Llama-3.2-3B-m...   \n",
       "44               default           0a4e765d-Llama-3.2-3B-Instruct-mixed-500   \n",
       "45  translation, default  output__full_patching__4d6edc1e-Llama-3.2-3B-I...   \n",
       "46  translation, default                    234bdfaa-Llama-3.2-3B-mixed-500   \n",
       "47               default                    dfe1b80e-Llama-3.2-3B-mixed-500   \n",
       "48               default                    dfe1b80e-Llama-3.2-3B-mixed-500   \n",
       "49  translation, default  output__full_patching__234bdfaa-Llama-3.2-3B-m...   \n",
       "50               default           755abfca-Llama-3.2-3B-Instruct-mixed-500   \n",
       "51  translation, default  output__full_patching__4b9504d3-Llama-3.2-3B-I...   \n",
       "52  translation, default                   461b9dfe-Llama-3.2-3B-mixed-1000   \n",
       "53               default                   4eb8fba9-Llama-3.2-3B-mixed-1000   \n",
       "54               default                   4eb8fba9-Llama-3.2-3B-mixed-1000   \n",
       "55               default                   4eb8fba9-Llama-3.2-3B-mixed-1000   \n",
       "56  translation, default  output__full_patching__461b9dfe-Llama-3.2-3B-m...   \n",
       "57               default          9df250b6-Llama-3.2-3B-Instruct-mixed-1000   \n",
       "58  translation, default  output__full_patching__8ce7f0d6-Llama-3.2-3B-I...   \n",
       "59               default                   63d35748-Llama-3.2-3B-mixed-1000   \n",
       "60  translation, default                   ba7c8b60-Llama-3.2-3B-mixed-1000   \n",
       "61               default  output__full_patching__63d35748-Llama-3.2-3B-m...   \n",
       "62  translation, default  output__full_patching__ba7c8b60-Llama-3.2-3B-m...   \n",
       "63               default          6e1e953a-Llama-3.2-3B-Instruct-mixed-1000   \n",
       "64  translation, default  output__full_patching__80c6810f-Llama-3.2-3B-I...   \n",
       "65               unknown                           meta-llama__Llama-3.2-3B   \n",
       "66               unknown                  meta-llama__Llama-3.2-3B-Instruct   \n",
       "\n",
       "   num_new_tokens    scores  compression_ratio  theoretical_compression_ratio  \\\n",
       "0               0  0.227127          -0.000226                       0.000000   \n",
       "1               0  0.533124           0.012280                       0.000000   \n",
       "2               0  0.762922          -0.010773                       0.000000   \n",
       "3               0  0.761086          -0.010761                       0.000000   \n",
       "4               0  0.215986          -0.001223                       0.000000   \n",
       "5               0  0.641485          -0.011691                       0.000000   \n",
       "6               0  0.635252          -0.010000                       0.000000   \n",
       "7               0  0.762922          -0.010773                       0.000000   \n",
       "8               0  0.762922          -0.010773                       0.000000   \n",
       "9               0  0.276873          -0.000266                       0.000000   \n",
       "10              0  0.272277           0.001207                       0.000000   \n",
       "11              0  0.764758          -0.010785                       0.000000   \n",
       "12              0  0.764758          -0.010785                       0.000000   \n",
       "13             10  0.240978           0.017405                       0.018738   \n",
       "14             10  0.521334          -0.001514                       0.003100   \n",
       "15             10  0.760599          -0.010158                       0.015584   \n",
       "16             10  0.756927          -0.010551                       0.015644   \n",
       "17             10  0.245649           0.014895                       0.017731   \n",
       "18             10  0.608849           0.000824                       0.013212   \n",
       "19             10  0.621239           0.001851                       0.014476   \n",
       "20             10  0.757357          -0.008216                       0.015791   \n",
       "21             10  0.749058          -0.008610                       0.016339   \n",
       "22             10  0.262073           0.006792                       0.016143   \n",
       "23             10  0.265558           0.008682                       0.015013   \n",
       "24             10  0.730473          -0.001833                       0.018278   \n",
       "25             10  0.740078          -0.001153                       0.018647   \n",
       "26            100  0.215174           0.035638                       0.042507   \n",
       "27            100  0.504086           0.017502                       0.010700   \n",
       "28            100  0.735632          -0.007221                       0.059331   \n",
       "29            100  0.731485          -0.007376                       0.059037   \n",
       "30            100  0.210191           0.040349                       0.046784   \n",
       "31            100  0.571055           0.028351                       0.048398   \n",
       "32            100  0.600606           0.033028                       0.052770   \n",
       "33            100  0.745698          -0.002833                       0.059611   \n",
       "34            100  0.731616          -0.004923                       0.059580   \n",
       "35            100  0.266969           0.039745                       0.064240   \n",
       "36            100  0.276161           0.031562                       0.068354   \n",
       "37            100  0.750769           0.020501                       0.066113   \n",
       "38            100  0.731348           0.022516                       0.067439   \n",
       "39            500  0.209978           0.087240                       0.100254   \n",
       "40            500  0.453053           0.009105                       0.013769   \n",
       "41            500  0.718365          -0.001879                       0.118054   \n",
       "42            500  0.721631          -0.000953                       0.119027   \n",
       "43            500  0.231548           0.089000                       0.100258   \n",
       "44            500  0.592462           0.068671                       0.099375   \n",
       "45            500  0.583433           0.060745                       0.098851   \n",
       "46            500  0.708030           0.002964                       0.118194   \n",
       "47            500  0.236856           0.614980                       0.632944   \n",
       "48            500  0.715792          -0.000597                       0.117214   \n",
       "49            500  0.264458           0.147659                       0.192847   \n",
       "50            500  0.700486           0.036607                       0.126927   \n",
       "51            500  0.725578           0.037358                       0.130479   \n",
       "52           1000  0.700623           0.000799                       0.145546   \n",
       "53           1000  0.204233           0.101597                       0.122494   \n",
       "54           1000  0.440701           0.016155                       0.014179   \n",
       "55           1000  0.709522           0.002476                       0.145665   \n",
       "56           1000  0.241352           0.121145                       0.147647   \n",
       "57           1000  0.563974           0.081260                       0.118345   \n",
       "58           1000  0.536034           0.075506                       0.114864   \n",
       "59           1000  0.723891           0.002421                       0.142585   \n",
       "60           1000  0.708648           0.007525                       0.145391   \n",
       "61           1000  0.246848           0.158073                       0.204460   \n",
       "62           1000  0.258688           0.133380                       0.170377   \n",
       "63           1000  0.676506           0.042820                       0.150301   \n",
       "64           1000  0.690307           0.044952                       0.155445   \n",
       "65       Baseline  0.272277           0.001207                       0.000000   \n",
       "66       Baseline  0.767318          -0.010776                       0.000000   \n",
       "\n",
       "    learning_ratio  \n",
       "0         0.000229  \n",
       "1        -0.002480  \n",
       "2         0.012387  \n",
       "3         0.012375  \n",
       "4         0.001373  \n",
       "5         0.017971  \n",
       "6         0.011621  \n",
       "7         0.012387  \n",
       "8         0.012387  \n",
       "9         0.006193  \n",
       "10        0.004820  \n",
       "11        0.012398  \n",
       "12        0.012398  \n",
       "13        0.001320  \n",
       "14        0.021735  \n",
       "15        0.027514  \n",
       "16        0.028006  \n",
       "17        0.002959  \n",
       "18        0.039284  \n",
       "19        0.017905  \n",
       "20        0.025708  \n",
       "21        0.026663  \n",
       "22        0.018304  \n",
       "23        0.015031  \n",
       "24        0.021714  \n",
       "25        0.021340  \n",
       "26        0.006736  \n",
       "27       -0.001952  \n",
       "28        0.068312  \n",
       "29        0.068191  \n",
       "30        0.006330  \n",
       "31        0.025881  \n",
       "32        0.020466  \n",
       "33        0.063897  \n",
       "34        0.066068  \n",
       "35        0.028531  \n",
       "36        0.040201  \n",
       "37        0.046151  \n",
       "38        0.045404  \n",
       "39        0.012155  \n",
       "40        0.011711  \n",
       "41        0.121165  \n",
       "42        0.121166  \n",
       "43        0.010453  \n",
       "44        0.034358  \n",
       "45        0.040485  \n",
       "46        0.116043  \n",
       "47        0.019093  \n",
       "48        0.118972  \n",
       "49        0.043606  \n",
       "50        0.088294  \n",
       "51        0.090833  \n",
       "52        0.145470  \n",
       "53        0.022770  \n",
       "54       -0.000890  \n",
       "55        0.143656  \n",
       "56        0.048861  \n",
       "57        0.058773  \n",
       "58        0.038008  \n",
       "59        0.140660  \n",
       "60        0.137756  \n",
       "61        0.043921  \n",
       "62        0.034137  \n",
       "63        0.105766  \n",
       "64        0.106657  \n",
       "65        0.004820  \n",
       "66        0.012392  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment = \"full_patching\"\n",
    "results_file_name = \"eval_results.csv\"\n",
    "main_results = get_results(results_file_name, experiment)\n",
    "# main_results.sort_values(by=\"exact_match\", ascending=True)\n",
    "# print(main_results.sort_values(by=\"prompt_level_strict_acc\", ascending=True))\n",
    "\n",
    "if experiment == \"longer_embeddings\":\n",
    "    main_results.loc[main_results[\"finetuning_params\"] == \"embeddings\", \"finetuning_params\"] = \"lora\"\n",
    "\n",
    "# templating_results = get_results(results_file_name, \"templating\")\n",
    "# patching_results = get_results(results_file_name, \"patching\")\n",
    "# main_results = pd.concat([templating_results, patching_results])\n",
    "main_results.sort_values(by=\"prompt_level_strict_acc\", ascending=True)\n",
    "main_results.fillna(\"NAN\", inplace=True)\n",
    "# display(main_results)\n",
    "\n",
    "agg_col = [\"num_new_tokens\", \"finetuning_params\", \"model_name\", \"trained_model_name\", \"model_name_sanitized\", \"dataset_str\", \"embeddings_init\"]\n",
    "agged = get_aggregated_results(main_results, agg_col)\n",
    "agged.reset_index(inplace=True)\n",
    "display(agged)\n",
    "\n",
    "records = []\n",
    "agged.columns = ['_'.join(col).rstrip('_') if isinstance(col, tuple) else col for col in agged.columns]\n",
    "for _, row in agged.iterrows():\n",
    "    data_dict = {}\n",
    "    model_name = row[\"model_name\"]\n",
    "    finetuning = row[\"finetuning_params\"]\n",
    "    new_embeddings_init = row.get(\"embeddings_init\", \"None\")\n",
    "    dataset = row[\"dataset_str\"]\n",
    "    trained_model_name = row[\"trained_model_name\"]\n",
    "    if row[\"trained_model_name\"].startswith(\"meta-llama\"):\n",
    "        num_new_tokens = \"Baseline\"\n",
    "        finetuning = \"None\"\n",
    "        model_name = row[\"model_name\"]\n",
    "    else:\n",
    "        num_new_tokens = row[\"num_new_tokens\"]\n",
    "\n",
    "    # if new_embeddings_init != \"None\":\n",
    "    #     model_name = f\"{model_name}-Instruct\" # TODO CLEAN\n",
    "\n",
    "    if row[\"model_name\"] == \"Llama-3.2-3B\" and \"Instruct\" in row[\"model_name_sanitized\"]:\n",
    "        # legacy\n",
    "        if new_embeddings_init == \"new_only\":\n",
    "            new_embeddings_init = \"patched_new_tokens\"\n",
    "        else:\n",
    "            new_embeddings_init = \"patched_all_embeddings\"\n",
    "        model_name = f\"{model_name}-Instruct\"  # TODO CLEAN\n",
    "  \n",
    "    data_dict[\"model_name\"] = model_name\n",
    "    data_dict[\"new_embeddings_init\"] = new_embeddings_init\n",
    "    data_dict[\"finetuning\"] = finetuning\n",
    "    data_dict[\"dataset\"] = dataset\n",
    "    data_dict[\"trained_model_name\"] = trained_model_name\n",
    "    try:\n",
    "        data_dict[\"num_new_tokens\"] = int(num_new_tokens)\n",
    "    except:\n",
    "        data_dict[\"num_new_tokens\"] = num_new_tokens\n",
    "    \n",
    "    score_list = [row[\"prompt_level_strict_acc_mean\"], row[\"inst_level_strict_acc_mean\"], row[\"prompt_level_loose_acc_mean\"], row[\"inst_level_loose_acc_mean\"]]\n",
    "    # data_dict[\"prompt_level_strict_acc\"] = row[\"prompt_level_strict_acc_mean\"]\n",
    "    data_dict[\"scores\"] = sum(score_list) / len(score_list)\n",
    "    data_dict[\"compression_ratio\"] = row[\"compression_ratio_mean\"]\n",
    "    data_dict[\"theoretical_compression_ratio\"] = row[\"theoretical_compression_ratio_mean\"]\n",
    "    data_dict[\"learning_ratio\"] = row[\"learning_ratio_mean\"]\n",
    "    records.append(data_dict)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>new_embeddings_init</th>\n",
       "      <th>finetuning</th>\n",
       "      <th>dataset</th>\n",
       "      <th>trained_model_name</th>\n",
       "      <th>num_new_tokens</th>\n",
       "      <th>scores</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>theoretical_compression_ratio</th>\n",
       "      <th>learning_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__d1f6703c-Llama-3.2-3B-m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.215986</td>\n",
       "      <td>-0.001223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__a84197a7-Llama-3.2-3B-m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.272277</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__bd539cf5-Llama-3.2-3B-m...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.245649</td>\n",
       "      <td>0.014895</td>\n",
       "      <td>0.017731</td>\n",
       "      <td>0.002959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__d2d49ae1-Llama-3.2-3B-m...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.262073</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>0.016143</td>\n",
       "      <td>0.018304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__2ecfe6ea-Llama-3.2-3B-m...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.210191</td>\n",
       "      <td>0.040349</td>\n",
       "      <td>0.046784</td>\n",
       "      <td>0.006330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__3c315a04-Llama-3.2-3B-m...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.266969</td>\n",
       "      <td>0.039745</td>\n",
       "      <td>0.064240</td>\n",
       "      <td>0.028531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__e8e7ce8d-Llama-3.2-3B-m...</td>\n",
       "      <td>500</td>\n",
       "      <td>0.231548</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.100258</td>\n",
       "      <td>0.010453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__234bdfaa-Llama-3.2-3B-m...</td>\n",
       "      <td>500</td>\n",
       "      <td>0.264458</td>\n",
       "      <td>0.147659</td>\n",
       "      <td>0.192847</td>\n",
       "      <td>0.043606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__461b9dfe-Llama-3.2-3B-m...</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.241352</td>\n",
       "      <td>0.121145</td>\n",
       "      <td>0.147647</td>\n",
       "      <td>0.048861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>new_tokens_only</td>\n",
       "      <td>translation, default</td>\n",
       "      <td>output__full_patching__ba7c8b60-Llama-3.2-3B-m...</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.258688</td>\n",
       "      <td>0.133380</td>\n",
       "      <td>0.170377</td>\n",
       "      <td>0.034137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Llama-3.2-3B</td>\n",
       "      <td>NAN</td>\n",
       "      <td>None</td>\n",
       "      <td>unknown</td>\n",
       "      <td>meta-llama__Llama-3.2-3B</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.272277</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model_name new_embeddings_init       finetuning               dataset  \\\n",
       "4   Llama-3.2-3B                 NAN       embeddings  translation, default   \n",
       "10  Llama-3.2-3B                 NAN  new_tokens_only  translation, default   \n",
       "17  Llama-3.2-3B                 NAN       embeddings  translation, default   \n",
       "22  Llama-3.2-3B                 NAN  new_tokens_only  translation, default   \n",
       "30  Llama-3.2-3B                 NAN       embeddings  translation, default   \n",
       "35  Llama-3.2-3B                 NAN  new_tokens_only  translation, default   \n",
       "43  Llama-3.2-3B                 NAN       embeddings  translation, default   \n",
       "49  Llama-3.2-3B                 NAN  new_tokens_only  translation, default   \n",
       "56  Llama-3.2-3B                 NAN       embeddings  translation, default   \n",
       "62  Llama-3.2-3B                 NAN  new_tokens_only  translation, default   \n",
       "65  Llama-3.2-3B                 NAN             None               unknown   \n",
       "\n",
       "                                   trained_model_name num_new_tokens  \\\n",
       "4   output__full_patching__d1f6703c-Llama-3.2-3B-m...              0   \n",
       "10  output__full_patching__a84197a7-Llama-3.2-3B-m...              0   \n",
       "17  output__full_patching__bd539cf5-Llama-3.2-3B-m...             10   \n",
       "22  output__full_patching__d2d49ae1-Llama-3.2-3B-m...             10   \n",
       "30  output__full_patching__2ecfe6ea-Llama-3.2-3B-m...            100   \n",
       "35  output__full_patching__3c315a04-Llama-3.2-3B-m...            100   \n",
       "43  output__full_patching__e8e7ce8d-Llama-3.2-3B-m...            500   \n",
       "49  output__full_patching__234bdfaa-Llama-3.2-3B-m...            500   \n",
       "56  output__full_patching__461b9dfe-Llama-3.2-3B-m...           1000   \n",
       "62  output__full_patching__ba7c8b60-Llama-3.2-3B-m...           1000   \n",
       "65                           meta-llama__Llama-3.2-3B       Baseline   \n",
       "\n",
       "      scores  compression_ratio  theoretical_compression_ratio  learning_ratio  \n",
       "4   0.215986          -0.001223                       0.000000        0.001373  \n",
       "10  0.272277           0.001207                       0.000000        0.004820  \n",
       "17  0.245649           0.014895                       0.017731        0.002959  \n",
       "22  0.262073           0.006792                       0.016143        0.018304  \n",
       "30  0.210191           0.040349                       0.046784        0.006330  \n",
       "35  0.266969           0.039745                       0.064240        0.028531  \n",
       "43  0.231548           0.089000                       0.100258        0.010453  \n",
       "49  0.264458           0.147659                       0.192847        0.043606  \n",
       "56  0.241352           0.121145                       0.147647        0.048861  \n",
       "62  0.258688           0.133380                       0.170377        0.034137  \n",
       "65  0.272277           0.001207                       0.000000        0.004820  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subset = df.copy()\n",
    "model = \"Llama-3.2-3B\"\n",
    "# model = \"Llama-3.2-3B-Instruct\"\n",
    "dataset = \"translation, default\"\n",
    "# dataset = \"default\"\n",
    "subset = subset[subset[\"model_name\"] == model]  # filter one model for clarity\n",
    "subset = subset[subset[\"dataset\"].isin([dataset, \"unknown\"])]\n",
    "# subset = subset[subset[\"dataset\"].isin([\"default\", \"unknown\"])]\n",
    "\n",
    "print(len(subset))\n",
    "display(subset)\n",
    "base_line = subset[subset[\"num_new_tokens\"] == \"Baseline\"]\n",
    "subset = subset[subset[\"num_new_tokens\"] != \"Baseline\"]\n",
    "\n",
    "group_col = [\"finetuning\", \"new_embeddings_init\"]\n",
    "# metric = \"prompt_level_strict_acc\"\n",
    "metric = \"scores\"\n",
    "\n",
    "my_compression_plotter(subset, group_col, metric, base_line, title = f\"{model} on {dataset.replace(', ', ' and ')} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eff-tok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
