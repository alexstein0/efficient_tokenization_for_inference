{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert new .model and .json files to huggingface tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshomes/astein0/.pyenv/versions/3.10.4/envs/eff-tok/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from train_tokenizer import read_training_info\n",
    "from efficient_tokenization import tokenize_simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_tokenizer_to_huggingface(new_tokenizer_info_path: str, original_tokenizer: str):\n",
    "    # must get original tokenizer from huggingface\n",
    "    tokenizer = AutoTokenizer.from_pretrained(original_tokenizer)\n",
    "    # load new_tokenizer_info_path\n",
    "    new_tokenizer_info = read_training_info(new_tokenizer_info_path)\n",
    "    # get merges and new_tokens\n",
    "    merges = new_tokenizer_info[\"merges\"]\n",
    "    new_tokens = new_tokenizer_info[\"new_tokens\"]\n",
    "    # add new tokens to tokenizer\n",
    "    tokenizer.add_tokens(new_tokens)\n",
    "    return tokenizer\n",
    "\n",
    "info_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start/new_mergeable_ranks_128266.json\"\n",
    "original_tokenizer = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# tok = convert_tokenizer_to_huggingface(info_path, original_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokenizer_to_huggingface_correct(new_tokenizer_info_path: str, original_tokenizer: str):\n",
    "    # must get original tokenizer from huggingface\n",
    "    tokenizer = AutoTokenizer.from_pretrained(original_tokenizer)\n",
    "    tokenizer_json = json.loads(tokenizer._tokenizer.to_str())\n",
    "    \n",
    "    # old_vocab = tokenizer_json[\"model\"][\"vocab\"]\n",
    "    # starting_index = len(tokenizer.get_vocab())\n",
    "    old_merges = tokenizer_json[\"model\"][\"merges\"]\n",
    "\n",
    "    # Extract vocab (token: index)\n",
    "    old_vocab = tokenizer.get_vocab()\n",
    "    \n",
    "    # load new_tokenizer_info_path\n",
    "    new_tokenizer_info = read_training_info(new_tokenizer_info_path)\n",
    "    # get merges and new_tokens\n",
    "    new_merges = new_tokenizer_info[\"merges\"]\n",
    "    new_tokens = new_tokenizer_info[\"new_tokens\"]\n",
    "\n",
    "    # Update vocab (append at the next available ID)\n",
    "    new_vocab = {**old_vocab}  # Copy the old vocab\n",
    "    starting_index = max(old_vocab.values()) + 1\n",
    "\n",
    "    for i, token in enumerate(new_tokens):\n",
    "        new_vocab[token] = starting_index + i\n",
    "\n",
    "    new_vocab_sorted = dict(sorted(new_vocab.items(), key=lambda item: item[1]))\n",
    "    # new_vocab_sorted = new_vocab\n",
    "\n",
    "    joined_merges = [x for x in old_merges]\n",
    "    joined_merges.extend(new_merges)\n",
    "\n",
    "    added_tokens = tokenizer.get_added_vocab()\n",
    "    add_tok_ids = [tok_id for tok, tok_id in added_tokens.items()]\n",
    "\n",
    "    new_vocab_sorted_no_added = {tok: tok_id for  tok, tok_id in new_vocab_sorted.items() if tok_id not in add_tok_ids}\n",
    "\n",
    "    new_tokenizer_info = {**tokenizer_json}\n",
    "    new_tokenizer_info[\"model\"][\"vocab\"] = new_vocab_sorted_no_added\n",
    "    new_tokenizer_info[\"model\"][\"merges\"] = joined_merges\n",
    "\n",
    "    # add new tokens to tokenizer\n",
    "    return new_tokenizer_info\n",
    "\n",
    "info_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start/new_mergeable_ranks_128266.json\"\n",
    "original_tokenizer_str = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "new_tokenizer_json = convert_tokenizer_to_huggingface_correct(info_path, original_tokenizer_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 2️⃣ Define new save path\n",
    "new_tokenizer_path = \"tokenizers/new_custom_tokenizer\"\n",
    "os.makedirs(new_tokenizer_path, exist_ok=True)\n",
    "\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(original_tokenizer_str)\n",
    "\n",
    "# 3️⃣ Save the tokenizer to a directory\n",
    "original_tokenizer.save_pretrained(new_tokenizer_path)\n",
    "\n",
    "tokenizer_json_path = os.path.join(new_tokenizer_path, \"tokenizer.json\")\n",
    "with open(tokenizer_json_path, \"w\") as f:\n",
    "    json.dump(new_tokenizer_json, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5️⃣ Reload tokenizer to verify it works\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(new_tokenizer_path)\n",
    "\n",
    "# 6️⃣ Check vocab size to confirm new tokens were added\n",
    "print(f\"✅ New tokenizer loaded! Vocab size: {len(new_tokenizer.get_vocab())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokenizer = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(original_tokenizer)\n",
    "tokenizer_json = json.loads(tokenizer._tokenizer.to_str())\n",
    "    \n",
    "old_vocab = tokenizer_json[\"model\"][\"vocab\"]\n",
    "starting_index = len(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tok = load_pretokenizer(\"empty\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenizer_to_config(pre_tok):\n",
    "    # Convert the pretokenizer string representation to a config dict\n",
    "    str_rep = str(pre_tok)\n",
    "    # Remove the outer Sequence() wrapper\n",
    "    inner = str_rep[len(\"Sequence(pretokenizers=[\"):-2]\n",
    "    \n",
    "    # Parse ByteLevel config\n",
    "    # Format is: ByteLevel(add_prefix_space=False, trim_offsets=True, use_regex=False)\n",
    "    byte_level_config = {}\n",
    "    byte_level_params = inner[len(\"ByteLevel(\"):-1].split(\", \")\n",
    "    for param in byte_level_params:\n",
    "        key, value = param.split(\"=\")\n",
    "        byte_level_config[key] = value.lower() == \"true\"  # Convert string to boolean\n",
    "    \n",
    "    config = {\n",
    "        \"type\": \"Sequence\",\n",
    "        \"pretokenizers\": [{\n",
    "            \"type\": \"ByteLevel\",\n",
    "            **byte_level_config\n",
    "        }]\n",
    "    }\n",
    "    return config\n",
    "\n",
    "# Use it like this:\n",
    "config = pretokenizer_to_config(pre_tok)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"pre_tokenizer\": {\n",
    "    \"type\": \"Sequence\",\n",
    "    \"pretokenizers\": [\n",
    "      {\n",
    "        \"type\": \"Split\",\n",
    "        \"pattern\": {\n",
    "          \"Regex\": \"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\n",
    "        },\n",
    "        \"behavior\": \"Isolated\",\n",
    "        \"invert\": false\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"ByteLevel\",\n",
    "        \"add_prefix_space\": false,\n",
    "        \"trim_offsets\": true,\n",
    "        \"use_regex\": false\n",
    "      }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenizer_to_config(pre_tok):\n",
    "    \"\"\"Convert any pretokenizer to a config dictionary by parsing its string representation.\"\"\"\n",
    "    str_rep = str(pre_tok)\n",
    "    \n",
    "    def parse_params(param_str):\n",
    "        \"\"\"Parse parameters from string representation into a dictionary.\"\"\"\n",
    "        params = {}\n",
    "        # Handle empty parameters\n",
    "        if not param_str:\n",
    "            return params\n",
    "            \n",
    "        for param in param_str.split(\", \"):\n",
    "            key, value = param.split(\"=\")\n",
    "            # Try to convert value to appropriate type\n",
    "            if value.lower() in ['true', 'false']:\n",
    "                params[key] = value.lower() == 'true'\n",
    "            elif value.isdigit():\n",
    "                params[key] = int(value)\n",
    "            elif value.replace('.', '').isdigit():\n",
    "                params[key] = float(value)\n",
    "            else:\n",
    "                # Remove quotes if present\n",
    "                params[key] = value.strip(\"'\\\"\")\n",
    "        return params\n",
    "    \n",
    "    def parse_pretokenizer(tok_str):\n",
    "        \"\"\"Recursively parse a pretokenizer string into a config dictionary.\"\"\"\n",
    "        # Find the type and parameters\n",
    "        tok_type = tok_str[:tok_str.find(\"(\")]\n",
    "        param_str = tok_str[tok_str.find(\"(\")+1:tok_str.rfind(\")\")]\n",
    "        \n",
    "        # Handle nested pretokenizers (like in Sequence)\n",
    "        if tok_type == \"Sequence\":\n",
    "            # Extract the list of pretokenizers\n",
    "            pretok_list = param_str[len(\"pretokenizers=[\"):-1]\n",
    "            # Split on \"), \" but keep the closing parenthesis\n",
    "            nested_toks = []\n",
    "            current = \"\"\n",
    "            paren_count = 0\n",
    "            for char in pretok_list:\n",
    "                if char == \"(\":\n",
    "                    paren_count += 1\n",
    "                elif char == \")\":\n",
    "                    paren_count -= 1\n",
    "                current += char\n",
    "                if paren_count == 0 and char == \")\":\n",
    "                    nested_toks.append(current)\n",
    "                    current = \"\"\n",
    "                elif paren_count == 0 and char == \",\":\n",
    "                    current = \"\"\n",
    "                \n",
    "            return {\n",
    "                \"type\": tok_type,\n",
    "                \"pretokenizers\": [parse_pretokenizer(tok.strip()) for tok in nested_toks if tok.strip()]\n",
    "            }\n",
    "        else:\n",
    "            # Regular pretokenizer\n",
    "            return {\n",
    "                \"type\": tok_type,\n",
    "                **parse_params(param_str)\n",
    "            }\n",
    "    \n",
    "    return parse_pretokenizer(str_rep)\n",
    "\n",
    "# Example usage:\n",
    "from tokenizers import pre_tokenizers\n",
    "\n",
    "# Test with different pretokenizer configurations\n",
    "pretok_configs = [\n",
    "    pre_tokenizers.ByteLevel(add_prefix_space=False, trim_offsets=True),\n",
    "    pre_tokenizers.Whitespace(),\n",
    "    pre_tokenizers.Sequence([\n",
    "        pre_tokenizers.ByteLevel(add_prefix_space=False),\n",
    "        pre_tokenizers.Whitespace()\n",
    "    ]),\n",
    "    pre_tokenizers.Metaspace(replacement=\"▁\"),\n",
    "    pre_tokenizers.Digits(individual_digits=True),\n",
    "    load_pretokenizer(\"empty\"),\n",
    "    load_pretokenizer(\"llama3\"),\n",
    "]\n",
    "\n",
    "# Test each configuration\n",
    "for pre_tok in pretok_configs:\n",
    "    config = pretokenizer_to_config(pre_tok)\n",
    "    print(f\"\\nPretokenizer: {pre_tok}\")\n",
    "    print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DUPLICATING the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 01:57:58 - INFO - Loading model and tokenizer...\n",
      "2025-03-20 01:57:58 - INFO - Loading base tokenizer...\n",
      "2025-03-20 01:57:59 - INFO - Loading extended tokenizer...\n",
      "2025-03-20 01:58:00 - INFO - Loading dataset...\n",
      "2025-03-20 01:58:00 - INFO - Getting genqa data...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import psutil\n",
    "import datasets\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "try:\n",
    "    threads = min(psutil.cpu_count(logical=False), len(psutil.Process().cpu_affinity()))\n",
    "except:\n",
    "    threads = os.cpu_count()\n",
    "\n",
    "import logging\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the minimum log level\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Include time, level, and message\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"  # Specify the date and time format\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "log.info(\"Loading model and tokenizer...\")\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# new_tokenizer_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000\"\n",
    "new_tokenizer_path =\"/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-10\"\n",
    "\n",
    "log.info(\"Loading base tokenizer...\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "log.info(\"Loading extended tokenizer...\")\n",
    "extended_tokenizer = AutoTokenizer.from_pretrained(new_tokenizer_path)\n",
    "\n",
    "log.info(\"Loading dataset...\")\n",
    "ds_path = \"/fs/cml-projects/llm-pretraining/datasets/raw/genqa/math\"\n",
    "ds = datasets.load_from_disk(ds_path)\n",
    "\n",
    "log.info(\"Getting genqa data...\")\n",
    "ds = tokenize_simple.get_genqa_data(ds, track_role=True, batch_size=batch_size, threads=threads)\n",
    "# log.info(\"Creating translation dataset...\")\n",
    "# ds = create_translation_dataset(ds, base_tokenizer, extended_tokenizer, batch_size, threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 21:39:51 - INFO - Creating translation dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 515509\n",
      "})\n",
      "The OrderedVocab you are attempting to save contains holes for indices [128000, 128001, 128002, 128003, 128004, 128005, 128006, 128007, 128008, 128009, 128010, 128011, 128012, 128013, 128014, 128015, 128016, 128017, 128018, 128019, 128020, 128021, 128022, 128023, 128024, 128025, 128026, 128027, 128028, 128029, 128030, 128031, 128032, 128033, 128034, 128035, 128036, 128037, 128038, 128039, 128040, 128041, 128042, 128043, 128044, 128045, 128046, 128047, 128048, 128049, 128050, 128051, 128052, 128053, 128054, 128055, 128056, 128057, 128058, 128059, 128060, 128061, 128062, 128063, 128064, 128065, 128066, 128067, 128068, 128069, 128070, 128071, 128072, 128073, 128074, 128075, 128076, 128077, 128078, 128079, 128080, 128081, 128082, 128083, 128084, 128085, 128086, 128087, 128088, 128089, 128090, 128091, 128092, 128093, 128094, 128095, 128096, 128097, 128098, 128099, 128100, 128101, 128102, 128103, 128104, 128105, 128106, 128107, 128108, 128109, 128110, 128111, 128112, 128113, 128114, 128115, 128116, 128117, 128118, 128119, 128120, 128121, 128122, 128123, 128124, 128125, 128126, 128127, 128128, 128129, 128130, 128131, 128132, 128133, 128134, 128135, 128136, 128137, 128138, 128139, 128140, 128141, 128142, 128143, 128144, 128145, 128146, 128147, 128148, 128149, 128150, 128151, 128152, 128153, 128154, 128155, 128156, 128157, 128158, 128159, 128160, 128161, 128162, 128163, 128164, 128165, 128166, 128167, 128168, 128169, 128170, 128171, 128172, 128173, 128174, 128175, 128176, 128177, 128178, 128179, 128180, 128181, 128182, 128183, 128184, 128185, 128186, 128187, 128188, 128189, 128190, 128191, 128192, 128193, 128194, 128195, 128196, 128197, 128198, 128199, 128200, 128201, 128202, 128203, 128204, 128205, 128206, 128207, 128208, 128209, 128210, 128211, 128212, 128213, 128214, 128215, 128216, 128217, 128218, 128219, 128220, 128221, 128222, 128223, 128224, 128225, 128226, 128227, 128228, 128229, 128230, 128231, 128232, 128233, 128234, 128235, 128236, 128237, 128238, 128239, 128240, 128241, 128242, 128243, 128244, 128245, 128246, 128247, 128248, 128249, 128250, 128251, 128252, 128253, 128254, 128255], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/nfshomes/astein0/.pyenv/versions/3.10.4/envs/eff-tok/lib/python3.10/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/nfshomes/astein0/.pyenv/versions/3.10.4/envs/eff-tok/lib/python3.10/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/nfshomes/astein0/.pyenv/versions/3.10.4/envs/eff-tok/lib/python3.10/site-packages/multiprocess/managers.py\", line 599, in _run_server\n",
      "    server.serve_forever()\n",
      "  File \"/nfshomes/astein0/.pyenv/versions/3.10.4/envs/eff-tok/lib/python3.10/site-packages/multiprocess/managers.py\", line 184, in serve_forever\n",
      "    sys.exit(0)\n",
      "SystemExit: 0\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/nfshomes/astein0/.pyenv/versions/3.10.4/envs/eff-tok/lib/python3.10/site-packages/multiprocess/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/nfshomes/astein0/.pyenv/versions/3.10.4/envs/eff-tok/lib/python3.10/site-packages/multiprocess/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/nfshomes/astein0/.pyenv/versions/3.10.4/envs/eff-tok/lib/python3.10/site-packages/multiprocess/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/nfshomes/astein0/.pyenv/versions/3.10.4/lib/python3.10/shutil.py\", line 722, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/nfshomes/astein0/.pyenv/versions/3.10.4/lib/python3.10/shutil.py\", line 678, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/nfshomes/astein0/.pyenv/versions/3.10.4/lib/python3.10/shutil.py\", line 676, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000009382e6a6000000d4'\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(tokenize_simple)\n",
    "print(ds)\n",
    "\n",
    "log.info(\"Creating translation dataset...\")\n",
    "# ds = tokenize_simple.create_translation_dataset(ds, base_tokenizer, extended_tokenizer, 2, threads)\n",
    "ds = tokenize_simple.create_translation_dataset_with_template(ds, base_tokenizer, new_tokenizer_path, 2, threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "ds_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized\"\n",
    "ds = datasets.load_from_disk(ds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting tokenizer to fewer tokens added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HF tokenizer from vocab file: /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000\n",
      "129256\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "print(f\"Loaded HF tokenizer from vocab file: {tokenizer_path}\")\n",
    "print(len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m      6\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(efficient_tokenization\u001b[38;5;241m.\u001b[39mtokenization_utils)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mtokenizer_path\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_info.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     tokenizer_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer_json\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer_path' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import efficient_tokenization.tokenization_utils\n",
    "\n",
    "import importlib\n",
    "importlib.reload(efficient_tokenization.tokenization_utils)\n",
    "\n",
    "with open(os.path.join(tokenizer_path, \"training_info.json\"), \"r\") as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "\n",
    "print(tokenizer_json.keys())\n",
    "print(len(tokenizer_json[\"merges\"]))\n",
    "print(len(tokenizer_json[\"sizes\"]))\n",
    "print(len(tokenizer_json[\"new_tokens\"]))\n",
    "print(tokenizer_json[\"static_info\"])\n",
    "print(tokenizer_json[\"state\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'efficient_tokenization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(\u001b[43mefficient_tokenization\u001b[49m\u001b[38;5;241m.\u001b[39mtokenization_utils)\n\u001b[1;32m      3\u001b[0m new_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/testing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m sm \u001b[38;5;241m=\u001b[39m efficient_tokenization\u001b[38;5;241m.\u001b[39mtokenization_utils\u001b[38;5;241m.\u001b[39mSaveModule\u001b[38;5;241m.\u001b[39mfrom_path(old_path\u001b[38;5;241m=\u001b[39mtokenizer_path, new_path\u001b[38;5;241m=\u001b[39mnew_save_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'efficient_tokenization' is not defined"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(efficient_tokenization.tokenization_utils)\n",
    "new_save_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/testing\"\n",
    "sm = efficient_tokenization.tokenization_utils.SaveModule.from_path(old_path=tokenizer_path, new_path=new_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/testing\n"
     ]
    }
   ],
   "source": [
    "merges = tokenizer_json[\"merges\"]\n",
    "sizes = tokenizer_json[\"sizes\"]\n",
    "# additional_info = tokenizer_json[\"additional_info\"]\n",
    "num_added_tokens = 100\n",
    "\n",
    "sm.shrink_tokenizer(merges, sizes, num_added_tokens=num_added_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import efficient_tokenization\n",
    "\n",
    "tokenizer_path1 = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer_path2 = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tok1 = AutoTokenizer.from_pretrained(tokenizer_path1)\n",
    "tok2 = AutoTokenizer.from_pretrained(tokenizer_path2)\n",
    "\n",
    "vocab1 = tok1.get_vocab()\n",
    "vocab2 = tok2.get_vocab()\n",
    "\n",
    "efficient_tokenization.tokenization_utils.compare_dicts(vocab1, vocab2)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now run the truncation for a bunch of sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting save_loc to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1\n",
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1\n",
      "Setting save_loc to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-5\n",
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-5\n",
      "Setting save_loc to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-20\n",
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-20\n",
      "Setting save_loc to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-50\n",
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-50\n",
      "Setting save_loc to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-100\n",
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-100\n",
      "Setting save_loc to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-200\n",
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-200\n",
      "Setting save_loc to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-300\n",
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-300\n",
      "Setting save_loc to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-400\n",
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-400\n",
      "Setting save_loc to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-500\n",
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-500\n",
      "Setting save_loc to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-600\n",
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-600\n",
      "Setting save_loc to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-700\n",
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-700\n",
      "Setting save_loc to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-800\n",
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-800\n",
      "Setting save_loc to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-900\n",
      "Saving tokenizer to /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-900\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(efficient_tokenization.tokenization_utils)\n",
    "# sm = efficient_tokenization.tokenization_utils.SaveModule.from_path(old_path=tokenizer_path)\n",
    "merges = tokenizer_json[\"merges\"]\n",
    "sizes = tokenizer_json[\"sizes\"]\n",
    "# additional_info = tokenizer_json[\"additional_info\"]\n",
    "num_new_tokens_list = [1, 5, 20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900]\n",
    "for num_new_tokens in num_new_tokens_list:\n",
    "    path = efficient_tokenization.tokenization_utils.get_new_path(num_new_tokens, tokenizer_path)\n",
    "    sm.shrink_tokenizer(merges, sizes, num_new_tokens=num_new_tokens, new_path=path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eff-tok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
