{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, TextStreamer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import datasets\n",
    "# from unsloth.chat_templates import get_chat_template\n",
    "# from unsloth import FastLanguageModel\n",
    "# from datasets import Dataset\n",
    "# from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_tokenization.tokenize_simple import get_genqa_data, get_tokenized_data, flatten_genqa_conversations, my_tokenize\n",
    "\n",
    "import logging\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the minimum log level\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Include time, level, and message\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"  # Specify the date and time format\n",
    ")\n",
    "\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "try:\n",
    "    threads = min(psutil.cpu_count(logical=False), len(psutil.Process().cpu_affinity()))\n",
    "except:\n",
    "    threads = os.cpu_count()\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "log.info(\"Loading model and tokenizer...\")\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "raw_data_name = \"genqa\"\n",
    "ext = \"math\"\n",
    "ds_path = f\"/fs/cml-projects/llm-pretraining/datasets/raw/{raw_data_name}/{ext}\"\n",
    "\n",
    "pre_tok_name = \"empty\"\n",
    "tokenizer_path_old = f\"/cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer-genqa-{ext}-{pre_tok_name}-start\"\n",
    "tokenizer_file_old = \"new_mergeable_ranks_2000.model\"\n",
    "vocab_file_path = f\"{tokenizer_path_old}/{tokenizer_file_old}\"\n",
    "\n",
    "# DATASET\n",
    "# dataset_path = \"/fs/cml-projects/llm-pretraining/datasets/processed/ultrachat/train\"\n",
    "dataset_path = f\"/fs/cml-projects/llm-pretraining/datasets/raw/{raw_data_name}/{ext}\"\n",
    "\n",
    "log.info(\"Downloading and processing raw dataset\")\n",
    "# tokenizer, data, tokenized_dataset = get_tokenized_data(vocab_file_path, ds_path, pre_tok_name=pre_tok_name)\n",
    "\n",
    "# get original_tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# data = get_genqa_data(dataset_path)\n",
    "# tokenized_dataset = my_tokenize(data.select_columns(\"text\"), tokenizer)\n",
    "# tokenized_dataset = tokenized_dataset.map(lambda batch: {\"num_tokens\": [len(ids) for ids in batch[\"input_ids\"]]}, batched=True, batch_size=batch_size, num_proc=threads)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficient_tokenization.tokenize_simple as tokenize_simple\n",
    "from efficient_tokenization.tokenize_simple import get_tokenized_data, flatten_genqa_conversations, my_tokenize\n",
    "\n",
    "import importlib\n",
    "importlib.reload(tokenize_simple)\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # data = tokenize_simple.get_genqa_data(dataset_path, tokenizer=tokenizer, track_role=True)\n",
    "# data = tokenize_simple.get_genqa_data(dataset_path, track_role=True)\n",
    "# tokenized_dataset = my_tokenize(data.select_columns(\"text\"), tokenizer)\n",
    "# tokenized_dataset = tokenized_dataset.map(lambda batch: {\"num_tokens\": [len(ids) for ids in batch[\"input_ids\"]]}, batched=True, batch_size=batch_size, num_proc=threads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "batch_size = 4\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/datasets/test\"\n",
    "tokenized_dataset = datasets.load_from_disk(dataset_path)\n",
    "\n",
    "# Split the dataset into train (90%) and validation (10%)\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "# tokenized_dataset[0]\n",
    "\n",
    "tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].remove_columns([\"text\", \"num_tokens\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer, max_length=2048, padding=True)\n",
    "ds = tokenized_dataset[\"train\"].select(range(16))\n",
    "print(ds[0])\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = [ds[i] for i in range(16)]\n",
    "\n",
    "# Convert lists to PyTorch tensors before collation\n",
    "for example in sample_batch:\n",
    "    example[\"input_ids\"] = torch.tensor(example[\"input_ids\"], dtype=torch.long)\n",
    "    example[\"attention_mask\"] = torch.tensor(example[\"attention_mask\"], dtype=torch.long)\n",
    "    example[\"labels\"] = torch.tensor(example[\"labels\"], dtype=torch.long)\n",
    "\n",
    "collated_batch = data_collator(sample_batch)\n",
    "print(collated_batch)\n",
    "\n",
    "# sample_batch = [tokenized_dataset[\"train\"][i] for i in range(4)]  # Pick a few examples\n",
    "# collated_batch = data_collator(sample_batch)\n",
    "# print(collated_batch)\n",
    "# print(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Context_length'] = tokenized_dataset.select_columns('num_tokens').apply(len)\n",
    "# dataset_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/datasets/test\"\n",
    "dataset_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized\"\n",
    "# args.dataset\n",
    "\n",
    "ds = datasets.load_from_disk(dataset_path)\n",
    "\n",
    "# Split the dataset into train (90%) and validation (10%)\n",
    "ds = ds.train_test_split(test_size=0.1)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(ds[\"train\"]['num_tokens'], bins=50, kde=True)\n",
    "plt.title('Distribution of Context Lengths')\n",
    "plt.xlabel('Length of Context')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHECKING NEW TOKENS\n",
    "import datasets\n",
    "batch_size = 1000\n",
    "threads = 16\n",
    "dataset_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized\"\n",
    "# args.dataset\n",
    "\n",
    "ds = datasets.load_from_disk(dataset_path)\n",
    "\n",
    "def count_large_tokens(batch):\n",
    "    # Count tokens > 1000 in each example's input_ids\n",
    "    counts = [sum(1 for token_id in ids if token_id > 128000) for ids in batch['input_ids']]\n",
    "    totals = [len(ids) for ids in batch['input_ids']]\n",
    "    percents = [count/total for count, total in zip(counts, totals)]\n",
    "    return {'large_token_count': counts, 'total_tokens': totals, 'percent_large_tokens': percents}\n",
    "\n",
    "if \"large_token_count\" not in ds.column_names:\n",
    "    # Apply the counting function to the dataset with batching\n",
    "    dataset_with_counts = ds.map(\n",
    "        count_large_tokens, \n",
    "        batched=True, \n",
    "        batch_size=batch_size, \n",
    "        num_proc=threads\n",
    "    )\n",
    "\n",
    "# You can then analyze the distribution\n",
    "plt.figure(figsize=(10, 3))\n",
    "# sns.histplot(dataset_with_counts['large_token_count'], bins=50, kde=True)\n",
    "sns.histplot(dataset_with_counts['percent_large_tokens'], bins=50, kde=True)\n",
    "plt.title('Distribution of Tokens with ID > 128000')\n",
    "plt.xlabel('Count of Tokens > 128000')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(\"Average tokens > 128000 per example:\", np.mean(dataset_with_counts['large_token_count']))\n",
    "print(\"Max tokens > 128000 in any example:\", np.max(dataset_with_counts['large_token_count']))\n",
    "print(\"Total tokens > 128000:\", sum(dataset_with_counts['large_token_count']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "gradient_accumulation_steps = 8\n",
    "processes = 8\n",
    "print(len(ds[\"train\"]) / batch_size / gradient_accumulation_steps / processes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_data = tokenized_dataset[tokenized_dataset['num_tokens'] <= 500]\n",
    "filtered_data = tokenized_dataset.filter(\n",
    "    lambda batch: [num_tokens < 2000 for num_tokens in batch[\"num_tokens\"]],\n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    num_proc=threads\n",
    ")\n",
    "\n",
    "# ln_Context = filtered_data['num_tokens'].apply(len)\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(filtered_data['num_tokens'], bins=50, kde=True)\n",
    "plt.title('Distribution of Context Lengths')\n",
    "plt.xlabel('Length of Context')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state = 32,\n",
    "    loftq_config = None,\n",
    ")\n",
    "print(model.print_trainable_parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTEND VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshomes/astein0/.pyenv/versions/3.10.4/envs/eff-tok/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from liger_kernel.transformers import AutoLigerKernelForCausalLM\n",
    "from efficient_tokenization.tokenize_simple import get_tokenizer, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# tokenizer_path = \"/cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start/new_mergeable_ranks_2000.model\"\n",
    "tokenizer_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "model = AutoLigerKernelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    use_cache=False,  # Disable KV cache during training\n",
    "    # device_map=\"auto\"  # Let accelerate handle device mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000\"\n",
    "print(\"Loading tokenizer...\")\n",
    "extended_tokenizer = AutoTokenizer.from_pretrained(new_tokenizer_path)\n",
    "texts = ['Translate the following text into the same text but with fewer tokens: text1: <|user|> Find the area of a trapezoid with bases of 8 cm and 12 cm and an altitude of 6 cm. Both bases are extended by 3 cm to form a new trapezoid. Find the area of the new trapezoid. Express your answer in simplified form.<|assistant|> The area of the original trapezoid is:\\n\\n```\\nArea = (8 cm + 12 cm) * 6 cm / 2 = 60 cm²\\n```\\n\\nThe ratio of the bases of the new trapezoid to the old trapezoid is:\\n\\n```\\nRatio = (8 cm + 3 cm + 12 cm + 3 cm) / (8 cm + 12 cm) = 1\\n```\\n\\nSince the bases are in the same ratio, the areas of the trapezoids will also be in the same ratio, so the area of the new trapezoid is:\\n\\n```\\nNew Area = 60 cm² * 1 = 60 cm²\\n```<|user|> If the original trapezoid is partitioned into two congruent right triangles by the altitude, what is the area of each triangle?<|assistant|> The area of each triangle is:\\n\\n```\\nTriangle Area = (8 cm + 12 cm) * 6 cm / 2 / 2 = 30 cm²\\n```text2: <|user|> Find the area of a trapezoid with bases of 8 cm and 12 cm and an altitude of 6 cm. Both bases are extended by 3 cm to form a new trapezoid. Find the area of the new trapezoid. Express your answer in simplified form.<|assistant|> The area of the original trapezoid is:\\n\\n```\\nArea = (8 cm + 12 cm) * 6 cm / 2 = 60 cm²\\n```\\n\\nThe ratio of the bases of the new trapezoid to the old trapezoid is:\\n\\n```\\nRatio = (8 cm + 3 cm + 12 cm + 3 cm) / (8 cm + 12 cm) = 1\\n```\\n\\nSince the bases are in the same ratio, the areas of the trapezoids will also be in the same ratio, so the area of the new trapezoid is:\\n\\n```\\nNew Area = 60 cm² * 1 = 60 cm²\\n```<|user|> If the original trapezoid is partitioned into two congruent right triangles by the altitude, what is the area of each triangle?<|assistant|> The area of each triangle is:\\n\\n```\\nTriangle Area = (8 cm + 12 cm) * 6 cm / 2 / 2 = 30 cm²\\n```', \"Translate the following text into the same text but with fewer tokens: text1: <|user|> Consider the differential equation:\\n\\n```y'' + y' - 2y = e^-x```\\n\\nSolve this equation using the method of undetermined coefficients.<|assistant|> Step 1: Find the Solution to the Homogeneous Equation \\n\\nThe homogeneous equation is:\\n\\n```y'' + y' - 2y = 0```\\n\\nIts characteristic equation is:\\n\\n```r^2 + r - 2 = 0```\\n\\nSolving for the roots, we get:\\n\\n```r = 1 ± √3i```\\n\\nTherefore, the solution to the homogeneous equation is:\\n\\n```y_h(x) = e^x (c_1 cos √3 x + c_2 sin √3 x)```\\n\\n Step 2: Find a Particular Solution to the Non-Homogeneous Equation \\n\\nSince the non-homogeneous term is e^-x, we guess a particular solution of the form:\\n\\n```y_p(x) = Ae^-x```\\n\\nDifferentiating twice, we get:\\n\\n```y_p'(x) = -Ae^-x```\\n\\n```y_p''(x) = Ae^-x```\\n\\nSubstituting these into the non-homogeneous equation, we get:\\n\\n```Ae^-x - Ae^-x - 2Ae^-x = e^-x```\\n\\nSolving for A, we get:\\n\\n```A = 1/2```\\n\\nTherefore, the particular solution is:\\n\\n```y_p(x) = (1/2)e^-x```\\n\\n Step 3: Combine the Homogeneous and Particular Solutions \\n\\nThe general solution to the non-homogeneous equation is:\\n\\n```y(x) = y_h(x) + y_p(x)```\\n\\n```y(x) = e^x (c_1 cos √3 x + c_2 sin √3 x) + (1/2)e^-x```<|user|> Find the general solution to the following differential equation:\\n\\n```y''' - 3y'' + 2y' - y = 0```<|assistant|> Step 1: Find the Solution to the Homogeneous Equation \\n\\nThe homogeneous equation is:\\n\\n```y''' - 3y'' + 2y' - y = 0```\\n\\nIts characteristic equation is:\\n\\n```r^3 - 3r^2 + 2r - 1 = 0```\\n\\nFactoring, we get:\\n\\n```(r - 1)^2 (r - 1) = 0```\\n\\nTherefore, the roots are:\\n\\n```r = 1, 1, 1```\\n\\nTherefore, the solution to the homogeneous equation is:\\n\\n```y_h(x) = c_1 e^x + c_2 x e^x + c_3 x^2 e^x```\\n\\n Step 2: Find a Particular Solution to the Non-Homogeneous Equation \\n\\nSince the non-homogeneous term is 0, the particular solution is:\\n\\n```y_p(x) = 0```\\n\\n Step 3: Combine the Homogeneous and Particular Solutions \\n\\nThe general solution to the non-homogeneous equation is:\\n\\n```y(x) = y_h(x) + y_p(x)```\\n\\n```y(x) = c_1 e^x + c_2 x e^x + c_3 x^2 e^x```text2: <|user|> Consider the differential equation:\\n\\n```y'' + y' - 2y = e^-x```\\n\\nSolve this equation using the method of undetermined coefficients.<|assistant|> Step 1: Find the Solution to the Homogeneous Equation \\n\\nThe homogeneous equation is:\\n\\n```y'' + y' - 2y = 0```\\n\\nIts characteristic equation is:\\n\\n```r^2 + r - 2 = 0```\\n\\nSolving for the roots, we get:\\n\\n```r = 1 ± √3i```\\n\\nTherefore, the solution to the homogeneous equation is:\\n\\n```y_h(x) = e^x (c_1 cos √3 x + c_2 sin √3 x)```\\n\\n Step 2: Find a Particular Solution to the Non-Homogeneous Equation \\n\\nSince the non-homogeneous term is e^-x, we guess a particular solution of the form:\\n\\n```y_p(x) = Ae^-x```\\n\\nDifferentiating twice, we get:\\n\\n```y_p'(x) = -Ae^-x```\\n\\n```y_p''(x) = Ae^-x```\\n\\nSubstituting these into the non-homogeneous equation, we get:\\n\\n```Ae^-x - Ae^-x - 2Ae^-x = e^-x```\\n\\nSolving for A, we get:\\n\\n```A = 1/2```\\n\\nTherefore, the particular solution is:\\n\\n```y_p(x) = (1/2)e^-x```\\n\\n Step 3: Combine the Homogeneous and Particular Solutions \\n\\nThe general solution to the non-homogeneous equation is:\\n\\n```y(x) = y_h(x) + y_p(x)```\\n\\n```y(x) = e^x (c_1 cos √3 x + c_2 sin √3 x) + (1/2)e^-x```<|user|> Find the general solution to the following differential equation:\\n\\n```y''' - 3y'' + 2y' - y = 0```<|assistant|> Step 1: Find the Solution to the Homogeneous Equation \\n\\nThe homogeneous equation is:\\n\\n```y''' - 3y'' + 2y' - y = 0```\\n\\nIts characteristic equation is:\\n\\n```r^3 - 3r^2 + 2r - 1 = 0```\\n\\nFactoring, we get:\\n\\n```(r - 1)^2 (r - 1) = 0```\\n\\nTherefore, the roots are:\\n\\n```r = 1, 1, 1```\\n\\nTherefore, the solution to the homogeneous equation is:\\n\\n```y_h(x) = c_1 e^x + c_2 x e^x + c_3 x^2 e^x```\\n\\n Step 2: Find a Particular Solution to the Non-Homogeneous Equation \\n\\nSince the non-homogeneous term is 0, the particular solution is:\\n\\n```y_p(x) = 0```\\n\\n Step 3: Combine the Homogeneous and Particular Solutions \\n\\nThe general solution to the non-homogeneous equation is:\\n\\n```y(x) = y_h(x) + y_p(x)```\\n\\n```y(x) = c_1 e^x + c_2 x e^x + c_3 x^2 e^x```\"]\n",
    "\n",
    "sample = extended_tokenizer(texts, add_special_tokens=False)\n",
    "ids_list = torch.tensor(sample[\"input_ids\"][0])\n",
    "print(ids_list)\n",
    "model(ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_new_tokens: 1000\n",
      "len(tokenizer): 129256\n",
      "```Ċ\n",
      "Extending model embeddings with strategy: merge\n",
      "The OrderedVocab you are attempting to save contains holes for indices [128000, 128001, 128002, 128003, 128004, 128005, 128006, 128007, 128008, 128009, 128010, 128011, 128012, 128013, 128014, 128015, 128016, 128017, 128018, 128019, 128020, 128021, 128022, 128023, 128024, 128025, 128026, 128027, 128028, 128029, 128030, 128031, 128032, 128033, 128034, 128035, 128036, 128037, 128038, 128039, 128040, 128041, 128042, 128043, 128044, 128045, 128046, 128047, 128048, 128049, 128050, 128051, 128052, 128053, 128054, 128055, 128056, 128057, 128058, 128059, 128060, 128061, 128062, 128063, 128064, 128065, 128066, 128067, 128068, 128069, 128070, 128071, 128072, 128073, 128074, 128075, 128076, 128077, 128078, 128079, 128080, 128081, 128082, 128083, 128084, 128085, 128086, 128087, 128088, 128089, 128090, 128091, 128092, 128093, 128094, 128095, 128096, 128097, 128098, 128099, 128100, 128101, 128102, 128103, 128104, 128105, 128106, 128107, 128108, 128109, 128110, 128111, 128112, 128113, 128114, 128115, 128116, 128117, 128118, 128119, 128120, 128121, 128122, 128123, 128124, 128125, 128126, 128127, 128128, 128129, 128130, 128131, 128132, 128133, 128134, 128135, 128136, 128137, 128138, 128139, 128140, 128141, 128142, 128143, 128144, 128145, 128146, 128147, 128148, 128149, 128150, 128151, 128152, 128153, 128154, 128155, 128156, 128157, 128158, 128159, 128160, 128161, 128162, 128163, 128164, 128165, 128166, 128167, 128168, 128169, 128170, 128171, 128172, 128173, 128174, 128175, 128176, 128177, 128178, 128179, 128180, 128181, 128182, 128183, 128184, 128185, 128186, 128187, 128188, 128189, 128190, 128191, 128192, 128193, 128194, 128195, 128196, 128197, 128198, 128199, 128200, 128201, 128202, 128203, 128204, 128205, 128206, 128207, 128208, 128209, 128210, 128211, 128212, 128213, 128214, 128215, 128216, 128217, 128218, 128219, 128220, 128221, 128222, 128223, 128224, 128225, 128226, 128227, 128228, 128229, 128230, 128231, 128232, 128233, 128234, 128235, 128236, 128237, 128238, 128239, 128240, 128241, 128242, 128243, 128244, 128245, 128246, 128247, 128248, 128249, 128250, 128251, 128252, 128253, 128254, 128255], your vocabulary could be corrupted !\n",
      "The OrderedVocab you are attempting to save contains holes for indices [128000, 128001, 128002, 128003, 128004, 128005, 128006, 128007, 128008, 128009, 128010, 128011, 128012, 128013, 128014, 128015, 128016, 128017, 128018, 128019, 128020, 128021, 128022, 128023, 128024, 128025, 128026, 128027, 128028, 128029, 128030, 128031, 128032, 128033, 128034, 128035, 128036, 128037, 128038, 128039, 128040, 128041, 128042, 128043, 128044, 128045, 128046, 128047, 128048, 128049, 128050, 128051, 128052, 128053, 128054, 128055, 128056, 128057, 128058, 128059, 128060, 128061, 128062, 128063, 128064, 128065, 128066, 128067, 128068, 128069, 128070, 128071, 128072, 128073, 128074, 128075, 128076, 128077, 128078, 128079, 128080, 128081, 128082, 128083, 128084, 128085, 128086, 128087, 128088, 128089, 128090, 128091, 128092, 128093, 128094, 128095, 128096, 128097, 128098, 128099, 128100, 128101, 128102, 128103, 128104, 128105, 128106, 128107, 128108, 128109, 128110, 128111, 128112, 128113, 128114, 128115, 128116, 128117, 128118, 128119, 128120, 128121, 128122, 128123, 128124, 128125, 128126, 128127, 128128, 128129, 128130, 128131, 128132, 128133, 128134, 128135, 128136, 128137, 128138, 128139, 128140, 128141, 128142, 128143, 128144, 128145, 128146, 128147, 128148, 128149, 128150, 128151, 128152, 128153, 128154, 128155, 128156, 128157, 128158, 128159, 128160, 128161, 128162, 128163, 128164, 128165, 128166, 128167, 128168, 128169, 128170, 128171, 128172, 128173, 128174, 128175, 128176, 128177, 128178, 128179, 128180, 128181, 128182, 128183, 128184, 128185, 128186, 128187, 128188, 128189, 128190, 128191, 128192, 128193, 128194, 128195, 128196, 128197, 128198, 128199, 128200, 128201, 128202, 128203, 128204, 128205, 128206, 128207, 128208, 128209, 128210, 128211, 128212, 128213, 128214, 128215, 128216, 128217, 128218, 128219, 128220, 128221, 128222, 128223, 128224, 128225, 128226, 128227, 128228, 128229, 128230, 128231, 128232, 128233, 128234, 128235, 128236, 128237, 128238, 128239, 128240, 128241, 128242, 128243, 128244, 128245, 128246, 128247, 128248, 128249, 128250, 128251, 128252, 128253, 128254, 128255], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "import efficient_tokenization.extend_embeddings as extend_embeddings\n",
    "\n",
    "import importlib\n",
    "importlib.reload(extend_embeddings)\n",
    "import json\n",
    "\n",
    "from finetune import my_custom_forward\n",
    "\n",
    "# tokenizer_json = json.loads(tokenizer._tokenizer.to_str())\n",
    "\n",
    "# merge_list = tokenizer_json[\"model\"][\"merges\"]\n",
    "# print(merge_list[0])\n",
    "# print(len(merge_list))\n",
    "embedding_init_strategy = \"merge\"\n",
    "new_vocab_size = len(tokenizer)\n",
    "original_vocab_size = model.config.vocab_size\n",
    "num_new_tokens = new_vocab_size - original_vocab_size\n",
    "print(f\"num_new_tokens: {num_new_tokens}\")\n",
    "print(f\"len(tokenizer): {new_vocab_size}\")\n",
    "\n",
    "print(tokenizer._tokenizer.id_to_token(128260))\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(128000)\n",
    "\n",
    "# Extend model embeddings\n",
    "print(f\"Extending model embeddings with strategy: {embedding_init_strategy}\")\n",
    "model = extend_embeddings.extend_model_embeddings(\n",
    "    model, \n",
    "    num_new_tokens, \n",
    "    init_strategy=embedding_init_strategy,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# model.forward = my_custom_forward.__get__(model, type(model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   2675,    527,    264,  11190,  15592,  18328,   1664,   5553,\n",
      "            291,    304,  40916,    279,   1984,   1203,    311,   1124,    304,\n",
      "            459,  13890,    719,    810,  11297,   1648,     13,    578,   4113,\n",
      "           1984,    690,    387,  16717,    555,    364,   1342,    311,  13454,\n",
      "           4989,    323,    842,    449,    364,    408,   1495,   3238,    220,\n",
      "            578,  13454,   3857,    690,    387,  16717,    555,    364,  31724,\n",
      "          92188,   1342,    311,  13454,     25,  83739,    882,     91,     29,\n",
      "            763,    264,  12960,    449,   4219,    507,     11,    264,  44321,\n",
      "          14469,    374,  77933,    311,    279,  23899,   6812,    520,   1486,\n",
      "            423,     13,   1442,   9827,    284,    220,     19,  10166,    323,\n",
      "          11162,    284,    220,     21,  10166,     11,   1505,    279,  10801,\n",
      "            315,    279,  12960,  16134,     91,  78191,     91,     29,  15166,\n",
      "            220,     16,     25,  21194,   5468,  96462,  46295,    578,  13475,\n",
      "            311,  22217,  22858,    720,  14196,   4077,   1846,     61,     17,\n",
      "            489,  11162,     61,     17,    284,  10807,     61,     17,    198,\n",
      "             19,     61,     17,    489,    220,     21,     61,     17,    284,\n",
      "          10807,     61,     17,    198,   4103,    284,  10807,     61,     17,\n",
      "            198,  14196,  19884,  15166,    220,     17,     25,   8876,  10807,\n",
      "            374,    279,  23899,     11,    433,    374,   6273,    311,    220,\n",
      "             17,     81,     11,   1405,    436,    374,    279,  10801,    315,\n",
      "            279,  12960,     13,    720,  14196,   4077,   1741,    284,    220,\n",
      "             17,     81,    198,     17,     81,    284, 122371,   4103,    198,\n",
      "             17,     81,    284,    220,     17, 110682,   1032,    198,  14196,\n",
      "          19884,  15166,    220,     18,     25,  64384,    369,    436,    720,\n",
      "          14196,   4077,     81,    284, 122371,   1032,    198,  14196,  19884,\n",
      "          55915,     11,    279,  10801,    315,    279,  12960,    374, 122371,\n",
      "           1032,  10166,  16134,     91,    882,     91,     29,   1442,    264,\n",
      "           1584,  10449,    315,   3160,    220,    605,  10166,    374,  69760,\n",
      "            311,    264,  12960,    315,  10801,    436,     11,   1505,    279,\n",
      "           6138,    505,    279,   1486,    315,  22636,   2301,    311,    279,\n",
      "           4219,    315,    279,  12960,  16134,     91,  78191,     91,     29,\n",
      "           6914,    279,   6138,    505,    279,   1486,    315,  22636,   2301,\n",
      "            311,    279,   4219,    315,    279,  12960,    387,    865,     13,\n",
      "           3296,    279,   5468,  96462,  46295,    578,  13475,     11,    584,\n",
      "            617,   1473,  14196,   4077,   2666,    489,    865,  30876,     17,\n",
      "            284,    220,    605,     61,     17,    489,    436,     61,     17,\n",
      "            198,     81,     61,     17,    489,    220,     17,  12940,    489,\n",
      "            865,     61,     17,    284,    220,   1041,    489,    436,     61,\n",
      "             17,    198,     87,     61,     17,    489,    220,     17,  12940,\n",
      "            284,    220,   1041,    198,     87,   2120,    489,    220,     17,\n",
      "             81,      8,    284,    220,   1041,    198,  14196,  19884,  12834,\n",
      "            865,    374,   6928,     11,    584,    617,    865,    489,    220,\n",
      "             17,     81,    284,    220,    508,     13,  15636,   3638,  14196,\n",
      "           4077,     87,    284,    220,    508,    482,    220,     17,     81,\n",
      "            198,  74694,    842,   1495,     13,    720,  31724,     25,    220,\n",
      "             27,     91,    882,     91,     29,    763,    264,  12960,    449,\n",
      "           4219,    507, 128572,  44321,  14469,    374,  77933, 128285,  23899,\n",
      "           6812,    520,   1486,    423, 128546,   9827, 128384,  10166,    323,\n",
      "          11162, 128701,  10166, 128978, 129040,  12960,  16134,     91,  78191,\n",
      "             91,     29,  15166, 128347,  21194,   5468,  96462,  46295, 128367,\n",
      "            311,  22217,  22858,    720, 128260,   1846, 128270,  11162, 129195,\n",
      "          10807, 128257,    198,     19, 128270, 128381, 129195,  10807, 128257,\n",
      "            198,   4103,    284,  10807, 128257, 128272, 128408,   8876,  10807,\n",
      "         128275,  23899, 128550, 129038, 128259,     81, 128447,    436, 128275,\n",
      "         129040,  12960, 129002, 128260,   1741, 128282,     81,    198,     17,\n",
      "             81,    284, 122371,   4103,    198,     17,     81, 128282, 110682,\n",
      "           1032, 128272, 128461,  64384,    369,    436,    720, 128260,     81,\n",
      "            284, 122371,   1032, 128402, 129040,  12960,    374, 122371,   1032,\n",
      "          10166,  16134,     91,    882,     91,     29,   1442,    264,   1584,\n",
      "          10449,    315,   3160, 128398,  10166,    374,  69760, 128604,  12960,\n",
      "            315,  10801,    436, 128978,   6138, 128524,   1486,    315,  22636,\n",
      "           2301, 128285,   4219, 128261,  12960,  16134,     91,  78191,     91,\n",
      "             29,   6914,    279,   6138, 128524,   1486,    315,  22636,   2301,\n",
      "         128285,   4219, 128261,  12960,    387,    865, 128834,   5468,  96462,\n",
      "          46295, 128367, 128481,   2666, 128573, 128295, 128742, 128270,    436,\n",
      "         128257,    198,     81, 128669,  12940,    489, 128366, 129175, 129197,\n",
      "         128257,    198, 128357, 128259,  12940, 129175, 128681,   2120, 128294,\n",
      "             81, 128433,   1041, 128272,  12834,    865,    374,   6928, 128318,\n",
      "            865, 128294,     81, 128256,    508, 128315, 128996,     87, 128256,\n",
      "            508, 128283,     81, 128339]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]]), 'labels': tensor([[128000,   2675,    527,    264,  11190,  15592,  18328,   1664,   5553,\n",
      "            291,    304,  40916,    279,   1984,   1203,    311,   1124,    304,\n",
      "            459,  13890,    719,    810,  11297,   1648,     13,    578,   4113,\n",
      "           1984,    690,    387,  16717,    555,    364,   1342,    311,  13454,\n",
      "           4989,    323,    842,    449,    364,    408,   1495,   3238,    220,\n",
      "            578,  13454,   3857,    690,    387,  16717,    555,    364,  31724,\n",
      "          92188,   1342,    311,  13454,     25,  83739,    882,     91,     29,\n",
      "            763,    264,  12960,    449,   4219,    507,     11,    264,  44321,\n",
      "          14469,    374,  77933,    311,    279,  23899,   6812,    520,   1486,\n",
      "            423,     13,   1442,   9827,    284,    220,     19,  10166,    323,\n",
      "          11162,    284,    220,     21,  10166,     11,   1505,    279,  10801,\n",
      "            315,    279,  12960,  16134,     91,  78191,     91,     29,  15166,\n",
      "            220,     16,     25,  21194,   5468,  96462,  46295,    578,  13475,\n",
      "            311,  22217,  22858,    720,  14196,   4077,   1846,     61,     17,\n",
      "            489,  11162,     61,     17,    284,  10807,     61,     17,    198,\n",
      "             19,     61,     17,    489,    220,     21,     61,     17,    284,\n",
      "          10807,     61,     17,    198,   4103,    284,  10807,     61,     17,\n",
      "            198,  14196,  19884,  15166,    220,     17,     25,   8876,  10807,\n",
      "            374,    279,  23899,     11,    433,    374,   6273,    311,    220,\n",
      "             17,     81,     11,   1405,    436,    374,    279,  10801,    315,\n",
      "            279,  12960,     13,    720,  14196,   4077,   1741,    284,    220,\n",
      "             17,     81,    198,     17,     81,    284, 122371,   4103,    198,\n",
      "             17,     81,    284,    220,     17, 110682,   1032,    198,  14196,\n",
      "          19884,  15166,    220,     18,     25,  64384,    369,    436,    720,\n",
      "          14196,   4077,     81,    284, 122371,   1032,    198,  14196,  19884,\n",
      "          55915,     11,    279,  10801,    315,    279,  12960,    374, 122371,\n",
      "           1032,  10166,  16134,     91,    882,     91,     29,   1442,    264,\n",
      "           1584,  10449,    315,   3160,    220,    605,  10166,    374,  69760,\n",
      "            311,    264,  12960,    315,  10801,    436,     11,   1505,    279,\n",
      "           6138,    505,    279,   1486,    315,  22636,   2301,    311,    279,\n",
      "           4219,    315,    279,  12960,  16134,     91,  78191,     91,     29,\n",
      "           6914,    279,   6138,    505,    279,   1486,    315,  22636,   2301,\n",
      "            311,    279,   4219,    315,    279,  12960,    387,    865,     13,\n",
      "           3296,    279,   5468,  96462,  46295,    578,  13475,     11,    584,\n",
      "            617,   1473,  14196,   4077,   2666,    489,    865,  30876,     17,\n",
      "            284,    220,    605,     61,     17,    489,    436,     61,     17,\n",
      "            198,     81,     61,     17,    489,    220,     17,  12940,    489,\n",
      "            865,     61,     17,    284,    220,   1041,    489,    436,     61,\n",
      "             17,    198,     87,     61,     17,    489,    220,     17,  12940,\n",
      "            284,    220,   1041,    198,     87,   2120,    489,    220,     17,\n",
      "             81,      8,    284,    220,   1041,    198,  14196,  19884,  12834,\n",
      "            865,    374,   6928,     11,    584,    617,    865,    489,    220,\n",
      "             17,     81,    284,    220,    508,     13,  15636,   3638,  14196,\n",
      "           4077,     87,    284,    220,    508,    482,    220,     17,     81,\n",
      "            198,  74694,    842,   1495,     13,    720,  31724,     25,    220,\n",
      "             27,     91,    882,     91,     29,    763,    264,  12960,    449,\n",
      "           4219,    507, 128572,  44321,  14469,    374,  77933, 128285,  23899,\n",
      "           6812,    520,   1486,    423, 128546,   9827, 128384,  10166,    323,\n",
      "          11162, 128701,  10166, 128978, 129040,  12960,  16134,     91,  78191,\n",
      "             91,     29,  15166, 128347,  21194,   5468,  96462,  46295, 128367,\n",
      "            311,  22217,  22858,    720, 128260,   1846, 128270,  11162, 129195,\n",
      "          10807, 128257,    198,     19, 128270, 128381, 129195,  10807, 128257,\n",
      "            198,   4103,    284,  10807, 128257, 128272, 128408,   8876,  10807,\n",
      "         128275,  23899, 128550, 129038, 128259,     81, 128447,    436, 128275,\n",
      "         129040,  12960, 129002, 128260,   1741, 128282,     81,    198,     17,\n",
      "             81,    284, 122371,   4103,    198,     17,     81, 128282, 110682,\n",
      "           1032, 128272, 128461,  64384,    369,    436,    720, 128260,     81,\n",
      "            284, 122371,   1032, 128402, 129040,  12960,    374, 122371,   1032,\n",
      "          10166,  16134,     91,    882,     91,     29,   1442,    264,   1584,\n",
      "          10449,    315,   3160, 128398,  10166,    374,  69760, 128604,  12960,\n",
      "            315,  10801,    436, 128978,   6138, 128524,   1486,    315,  22636,\n",
      "           2301, 128285,   4219, 128261,  12960,  16134,     91,  78191,     91,\n",
      "             29,   6914,    279,   6138, 128524,   1486,    315,  22636,   2301,\n",
      "         128285,   4219, 128261,  12960,    387,    865, 128834,   5468,  96462,\n",
      "          46295, 128367, 128481,   2666, 128573, 128295, 128742, 128270,    436,\n",
      "         128257,    198,     81, 128669,  12940,    489, 128366, 129175, 129197,\n",
      "         128257,    198, 128357, 128259,  12940, 129175, 128681,   2120, 128294,\n",
      "             81, 128433,   1041, 128272,  12834,    865,    374,   6928, 128318,\n",
      "            865, 128294,     81, 128256,    508, 128315, 128996,     87, 128256,\n",
      "            508, 128283,     81, 128339]]), 'loss_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])}\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from finetune import MyPaddingCollatorWithLossMask\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized\"\n",
    "\n",
    "ds = datasets.load_from_disk(dataset_path)\n",
    "dl = DataLoader(ds.select(range(10)), batch_size=1, shuffle=True, collate_fn=MyPaddingCollatorWithLossMask(tokenizer=tokenizer))\n",
    "\n",
    "batch = next(iter(dl))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(129256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LigerSwiGLUMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "        )\n",
      "        (input_layernorm): LigerRMSNorm((2048,), eps=1e-05, offset=0.0, in_place=True)\n",
      "        (post_attention_layernorm): LigerRMSNorm((2048,), eps=1e-05, offset=0.0, in_place=True)\n",
      "      )\n",
      "    )\n",
      "    (norm): LigerRMSNorm((2048,), eps=1e-05, offset=0.0, in_place=True)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=129256, bias=False)\n",
      ")\n",
      "input_ids torch.Size([1, 625]) cuda:0\n",
      "attention_mask torch.Size([1, 625]) cuda:0\n",
      "labels torch.Size([1, 625]) cuda:0\n",
      "loss_mask torch.Size([1, 625]) cuda:0\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "model.train()\n",
    "\n",
    "print(model.device)\n",
    "print(model)\n",
    "for b, t in batch.items():\n",
    "    print(b, t.shape, t.device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0396, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "tensor(2.3922, device='cuda:0')\n",
      "tensor(1.7377, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# first do the loss on everything\n",
    "input_ids = batch[\"input_ids\"]\n",
    "labels = batch[\"labels\"]\n",
    "loss_mask = batch[\"loss_mask\"]\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "outputs = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=labels,\n",
    "    use_cache=False,\n",
    "    # num_items_in_batch=num_items_in_batch,\n",
    "    # new_token_start_index=original_vocab_size\n",
    ")\n",
    "# gets logits\n",
    "loss = outputs.loss\n",
    "print(loss)\n",
    "# loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Loss only on some of the tokens\n",
    "    masked_labels = labels.clone()\n",
    "    masked_labels[loss_mask==0] = -100\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=masked_labels,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    loss = outputs.loss\n",
    "    print(loss)\n",
    "    # loss.backward()\n",
    "\n",
    "    # then do the loss on only the tokens from the new tokenizer (as masked)\n",
    "\n",
    "    new_labels = labels.clone()\n",
    "    new_labels[new_labels > original_vocab_size] = -100\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=new_labels,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    loss = outputs.loss\n",
    "    print(loss)\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129256"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=tensor(0.8905, device='cuda:0',\n",
       "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>), logits=None, past_key_values=None, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_embeddings_list = extend_embeddings.get_new_embeddings(model, num_new_tokens)\n",
    "# new_embeddings_list[0]\n",
    "\n",
    "embeddings_output = model.get_output_embeddings()\n",
    "print(f\"embeddings_output: {embeddings_output}\")\n",
    "print(f\"embeddings_output.weight: {embeddings_output.weight}\")\n",
    "print(f\"embeddings_output.weight.data: {embeddings_output.weight.data}\")\n",
    "print(f\"embeddings_output.weight.grad: {embeddings_output.weight.grad}\")\n",
    "\n",
    "params = extend_embeddings.get_new_embedding_params(model, num_new_tokens)\n",
    "this_param = params[0]\n",
    "this_param.retain_grad()\n",
    "print(f\"params: {params}, length: {len(params)}\")\n",
    "print(f\"this_param: {this_param}, shape: {this_param.shape}\")\n",
    "print(f\"params.data: {this_param.data}, shape: {this_param.data.shape}\")\n",
    "print(f\"params.grad: {this_param.grad}, shape: {this_param.grad.shape if this_param.grad is not None else 'None'}\")\n",
    "\n",
    "grads = extend_embeddings.get_new_embeddings_grads(model, num_new_tokens)[0]\n",
    "print(f\"grads: {grads}, shape: {grads.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficient_tokenization.tokenize_simple as tokenize_simple\n",
    "from efficient_tokenization.tokenize_simple import get_tokenizer\n",
    "\n",
    "import transformers\n",
    "\n",
    "import importlib\n",
    "importlib.reload(tokenize_simple)\n",
    "importlib.reload(transformers)\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer_path = \"/cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start/new_mergeable_ranks_2000.model\"\n",
    "base_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = get_tokenizer(tokenizer_path, old_tokenizer=base_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n",
    "    characters the bpe code barfs on.\n",
    "    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n",
    "    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n",
    "    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n",
    "    tables between utf-8 bytes and unicode strings.\n",
    "    \"\"\"\n",
    "    bs = (\n",
    "        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "byte_encoder = bytes_to_unicode()\n",
    "\n",
    "def token_bytes_to_string(b):\n",
    "    return \"\".join([byte_encoder[ord(char)] for char in b.decode(\"latin-1\")])\n",
    "\n",
    "def unicode_to_bytes():\n",
    "    \"\"\"\n",
    "    Returns a mapping from unicode strings back to their original utf-8 bytes.\n",
    "    This reverses the `bytes_to_unicode` mapping.\n",
    "    \"\"\"\n",
    "    # byte_encoder = bytes_to_unicode()  # Original byte-to-unicode mapping\n",
    "    return {v: k for k, v in byte_encoder.items()}\n",
    "\n",
    "byte_decoder = unicode_to_bytes()\n",
    "\n",
    "def string_to_token_bytes(s):\n",
    "    \"\"\"\n",
    "    Converts a string back into token bytes using the reverse mapping.\n",
    "\n",
    "    Args:\n",
    "        s (str): The input string to convert.\n",
    "\n",
    "    Returns:\n",
    "        bytes: The byte representation of the string.\n",
    "    \"\"\"\n",
    "    return bytes([byte_decoder[char] for char in s])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "# mergeable_ranks = read_tokenizer_from_model(base_tokenizer_path)\n",
    "sorted_vocab = {k: v for k, v in sorted(llama_tokenizer.vocab.items(), key=lambda item: item[1])}\n",
    "for tok, i in sorted_vocab.items():\n",
    "    my_bytes = string_to_token_bytes(tok)\n",
    "    my_string = token_bytes_to_string(my_bytes)\n",
    "    if i > 127988:\n",
    "        print(f\"{i:06d}: {tok}, {my_bytes}, {base64.b64encode(my_bytes)}, {base64.b64decode(tok)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_tokenizer import read_tokenizer_from_model\n",
    "# old_path = \"/cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer/tokenizer.model\"\n",
    "old_path = \"/cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start/new_mergeable_ranks_2000.model\"\n",
    "old_vocab = read_tokenizer_from_model(old_path)\n",
    "\n",
    "joined_tokenizer = old_vocab.copy()\n",
    "joined_tokenizer.update(llama_tokenizer.get_added_vocab())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok, i in old_vocab.items():\n",
    "    my_bytes = string_to_token_bytes(tok)\n",
    "    if i > 127988:\n",
    "        print(f\"{i:06d}: {tok}, {my_bytes}, {base64.b64encode(my_bytes)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "def compare_dicts(dict1: Dict, dict2: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    Compare two dictionaries and print differences if they exist.\n",
    "    \n",
    "    Args:\n",
    "        dict1: First dictionary\n",
    "        dict2: Second dictionary\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if dictionaries are identical, False otherwise\n",
    "    \"\"\"\n",
    "    if dict1.keys() != dict2.keys():\n",
    "        print(\"Different keys:\")\n",
    "        print(\"Keys only in first dict:\", set(dict1.keys()) - set(dict2.keys()))\n",
    "        print(\"Keys only in second dict:\", set(dict2.keys()) - set(dict1.keys()))\n",
    "        return False\n",
    "    \n",
    "    differences = {\n",
    "        k: (dict1[k], dict2[k])\n",
    "        for k in dict1\n",
    "        if dict1[k] != dict2[k]\n",
    "    }\n",
    "    \n",
    "    if differences:\n",
    "        print(\"Different values:\")\n",
    "        for k, (v1, v2) in differences.items():\n",
    "            print(f\"Key: {k}\")\n",
    "            print(f\"  Dict1: {v1}\")\n",
    "            print(f\"  Dict2: {v2}\")\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "are_same = compare_dicts(sorted_vocab, joined_tokenizer)\n",
    "print(\"Dictionaries are identical:\", are_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lm_eval \n",
    "importlib.reload(lm_eval)\n",
    "\n",
    "model_args_dict = {\n",
    "    \"pretrained\": model,  # This will be your model object directly\n",
    "    \"tokenizer\": tokenizer,  # This will be your tokenizer object directly\n",
    "    \"old_tokenizer\": base_tokenizer,  # This will be your tokenizer object directly\n",
    "    # \"parallelize\": True,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 3,\n",
    "    \"trust_remote_code\": True\n",
    "}\n",
    "\n",
    "LM_model = lm_eval.models.huggingface.HFLM(**model_args_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eff-tok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
