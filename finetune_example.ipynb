{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshomes/astein0/.pyenv/versions/3.10.4/envs/eff-tok/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, TextStreamer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import datasets\n",
    "# from unsloth.chat_templates import get_chat_template\n",
    "# from unsloth import FastLanguageModel\n",
    "# from datasets import Dataset\n",
    "# from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_tokenization.tokenize_simple import get_genqa_data, get_tokenized_data, flatten_genqa_conversations, my_tokenize\n",
    "\n",
    "import logging\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the minimum log level\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Include time, level, and message\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"  # Specify the date and time format\n",
    ")\n",
    "\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "try:\n",
    "    threads = min(psutil.cpu_count(logical=False), len(psutil.Process().cpu_affinity()))\n",
    "except:\n",
    "    threads = os.cpu_count()\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "log.info(\"Loading model and tokenizer...\")\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "raw_data_name = \"genqa\"\n",
    "ext = \"math\"\n",
    "ds_path = f\"/fs/cml-projects/llm-pretraining/datasets/raw/{raw_data_name}/{ext}\"\n",
    "\n",
    "pre_tok_name = \"empty\"\n",
    "tokenizer_path_old = f\"/cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer-genqa-{ext}-{pre_tok_name}-start\"\n",
    "tokenizer_file_old = \"new_mergeable_ranks_2000.model\"\n",
    "vocab_file_path = f\"{tokenizer_path_old}/{tokenizer_file_old}\"\n",
    "\n",
    "# DATASET\n",
    "# dataset_path = \"/fs/cml-projects/llm-pretraining/datasets/processed/ultrachat/train\"\n",
    "dataset_path = f\"/fs/cml-projects/llm-pretraining/datasets/raw/{raw_data_name}/{ext}\"\n",
    "\n",
    "log.info(\"Downloading and processing raw dataset\")\n",
    "# tokenizer, data, tokenized_dataset = get_tokenized_data(vocab_file_path, ds_path, pre_tok_name=pre_tok_name)\n",
    "\n",
    "# get original_tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# data = get_genqa_data(dataset_path)\n",
    "# tokenized_dataset = my_tokenize(data.select_columns(\"text\"), tokenizer)\n",
    "# tokenized_dataset = tokenized_dataset.map(lambda batch: {\"num_tokens\": [len(ids) for ids in batch[\"input_ids\"]]}, batched=True, batch_size=batch_size, num_proc=threads)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:26<00:00, 13.40s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficient_tokenization.tokenize_simple as tokenize_simple\n",
    "from efficient_tokenization.tokenize_simple import get_tokenized_data, flatten_genqa_conversations, my_tokenize\n",
    "\n",
    "import importlib\n",
    "importlib.reload(tokenize_simple)\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # data = tokenize_simple.get_genqa_data(dataset_path, tokenizer=tokenizer, track_role=True)\n",
    "# data = tokenize_simple.get_genqa_data(dataset_path, track_role=True)\n",
    "# tokenized_dataset = my_tokenize(data.select_columns(\"text\"), tokenizer)\n",
    "# tokenized_dataset = tokenized_dataset.map(lambda batch: {\"num_tokens\": [len(ids) for ids in batch[\"input_ids\"]]}, batched=True, batch_size=batch_size, num_proc=threads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "batch_size = 4\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/datasets/test\"\n",
    "tokenized_dataset = datasets.load_from_disk(dataset_path)\n",
    "\n",
    "# Split the dataset into train (90%) and validation (10%)\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "# tokenized_dataset[0]\n",
    "\n",
    "tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].remove_columns([\"text\", \"num_tokens\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer, max_length=2048, padding=True)\n",
    "ds = tokenized_dataset[\"train\"].select(range(16))\n",
    "print(ds[0])\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = [ds[i] for i in range(16)]\n",
    "\n",
    "# Convert lists to PyTorch tensors before collation\n",
    "for example in sample_batch:\n",
    "    example[\"input_ids\"] = torch.tensor(example[\"input_ids\"], dtype=torch.long)\n",
    "    example[\"attention_mask\"] = torch.tensor(example[\"attention_mask\"], dtype=torch.long)\n",
    "    example[\"labels\"] = torch.tensor(example[\"labels\"], dtype=torch.long)\n",
    "\n",
    "collated_batch = data_collator(sample_batch)\n",
    "print(collated_batch)\n",
    "\n",
    "# sample_batch = [tokenized_dataset[\"train\"][i] for i in range(4)]  # Pick a few examples\n",
    "# collated_batch = data_collator(sample_batch)\n",
    "# print(collated_batch)\n",
    "# print(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Context_length'] = tokenized_dataset.select_columns('num_tokens').apply(len)\n",
    "# dataset_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/datasets/test\"\n",
    "dataset_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized\"\n",
    "# args.dataset\n",
    "\n",
    "ds = datasets.load_from_disk(dataset_path)\n",
    "\n",
    "# Split the dataset into train (90%) and validation (10%)\n",
    "ds = ds.train_test_split(test_size=0.1)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(ds[\"train\"]['num_tokens'], bins=50, kde=True)\n",
    "plt.title('Distribution of Context Lengths')\n",
    "plt.xlabel('Length of Context')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHECKING NEW TOKENS\n",
    "import datasets\n",
    "batch_size = 1000\n",
    "threads = 16\n",
    "dataset_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized\"\n",
    "# args.dataset\n",
    "\n",
    "ds = datasets.load_from_disk(dataset_path)\n",
    "\n",
    "def count_large_tokens(batch):\n",
    "    # Count tokens > 1000 in each example's input_ids\n",
    "    counts = [sum(1 for token_id in ids if token_id > 128000) for ids in batch['input_ids']]\n",
    "    totals = [len(ids) for ids in batch['input_ids']]\n",
    "    percents = [count/total for count, total in zip(counts, totals)]\n",
    "    return {'large_token_count': counts, 'total_tokens': totals, 'percent_large_tokens': percents}\n",
    "\n",
    "if \"large_token_count\" not in ds.column_names:\n",
    "    # Apply the counting function to the dataset with batching\n",
    "    dataset_with_counts = ds.map(\n",
    "        count_large_tokens, \n",
    "        batched=True, \n",
    "        batch_size=batch_size, \n",
    "        num_proc=threads\n",
    "    )\n",
    "\n",
    "# You can then analyze the distribution\n",
    "plt.figure(figsize=(10, 3))\n",
    "# sns.histplot(dataset_with_counts['large_token_count'], bins=50, kde=True)\n",
    "sns.histplot(dataset_with_counts['percent_large_tokens'], bins=50, kde=True)\n",
    "plt.title('Distribution of Tokens with ID > 128000')\n",
    "plt.xlabel('Count of Tokens > 128000')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(\"Average tokens > 128000 per example:\", np.mean(dataset_with_counts['large_token_count']))\n",
    "print(\"Max tokens > 128000 in any example:\", np.max(dataset_with_counts['large_token_count']))\n",
    "print(\"Total tokens > 128000:\", sum(dataset_with_counts['large_token_count']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "gradient_accumulation_steps = 8\n",
    "processes = 8\n",
    "print(len(ds[\"train\"]) / batch_size / gradient_accumulation_steps / processes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_data = tokenized_dataset[tokenized_dataset['num_tokens'] <= 500]\n",
    "filtered_data = tokenized_dataset.filter(\n",
    "    lambda batch: [num_tokens < 2000 for num_tokens in batch[\"num_tokens\"]],\n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    num_proc=threads\n",
    ")\n",
    "\n",
    "# ln_Context = filtered_data['num_tokens'].apply(len)\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.histplot(filtered_data['num_tokens'], bins=50, kde=True)\n",
    "plt.title('Distribution of Context Lengths')\n",
    "plt.xlabel('Length of Context')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state = 32,\n",
    "    loftq_config = None,\n",
    ")\n",
    "print(model.print_trainable_parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTEND VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshomes/astein0/.pyenv/versions/3.10.4/envs/eff-tok/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from liger_kernel.transformers import AutoLigerKernelForCausalLM\n",
    "from efficient_tokenization.tokenize_simple import get_tokenizer, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# tokenizer_path = \"/cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start/new_mergeable_ranks_2000.model\"\n",
    "tokenizer_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "model = AutoLigerKernelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    use_cache=False,  # Disable KV cache during training\n",
    "    # device_map=\"auto\"  # Let accelerate handle device mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000\"\n",
    "print(\"Loading tokenizer...\")\n",
    "extended_tokenizer = AutoTokenizer.from_pretrained(new_tokenizer_path)\n",
    "texts = ['Translate the following text into the same text but with fewer tokens: text1: <|user|> Find the area of a trapezoid with bases of 8 cm and 12 cm and an altitude of 6 cm. Both bases are extended by 3 cm to form a new trapezoid. Find the area of the new trapezoid. Express your answer in simplified form.<|assistant|> The area of the original trapezoid is:\\n\\n```\\nArea = (8 cm + 12 cm) * 6 cm / 2 = 60 cm²\\n```\\n\\nThe ratio of the bases of the new trapezoid to the old trapezoid is:\\n\\n```\\nRatio = (8 cm + 3 cm + 12 cm + 3 cm) / (8 cm + 12 cm) = 1\\n```\\n\\nSince the bases are in the same ratio, the areas of the trapezoids will also be in the same ratio, so the area of the new trapezoid is:\\n\\n```\\nNew Area = 60 cm² * 1 = 60 cm²\\n```<|user|> If the original trapezoid is partitioned into two congruent right triangles by the altitude, what is the area of each triangle?<|assistant|> The area of each triangle is:\\n\\n```\\nTriangle Area = (8 cm + 12 cm) * 6 cm / 2 / 2 = 30 cm²\\n```text2: <|user|> Find the area of a trapezoid with bases of 8 cm and 12 cm and an altitude of 6 cm. Both bases are extended by 3 cm to form a new trapezoid. Find the area of the new trapezoid. Express your answer in simplified form.<|assistant|> The area of the original trapezoid is:\\n\\n```\\nArea = (8 cm + 12 cm) * 6 cm / 2 = 60 cm²\\n```\\n\\nThe ratio of the bases of the new trapezoid to the old trapezoid is:\\n\\n```\\nRatio = (8 cm + 3 cm + 12 cm + 3 cm) / (8 cm + 12 cm) = 1\\n```\\n\\nSince the bases are in the same ratio, the areas of the trapezoids will also be in the same ratio, so the area of the new trapezoid is:\\n\\n```\\nNew Area = 60 cm² * 1 = 60 cm²\\n```<|user|> If the original trapezoid is partitioned into two congruent right triangles by the altitude, what is the area of each triangle?<|assistant|> The area of each triangle is:\\n\\n```\\nTriangle Area = (8 cm + 12 cm) * 6 cm / 2 / 2 = 30 cm²\\n```', \"Translate the following text into the same text but with fewer tokens: text1: <|user|> Consider the differential equation:\\n\\n```y'' + y' - 2y = e^-x```\\n\\nSolve this equation using the method of undetermined coefficients.<|assistant|> Step 1: Find the Solution to the Homogeneous Equation \\n\\nThe homogeneous equation is:\\n\\n```y'' + y' - 2y = 0```\\n\\nIts characteristic equation is:\\n\\n```r^2 + r - 2 = 0```\\n\\nSolving for the roots, we get:\\n\\n```r = 1 ± √3i```\\n\\nTherefore, the solution to the homogeneous equation is:\\n\\n```y_h(x) = e^x (c_1 cos √3 x + c_2 sin √3 x)```\\n\\n Step 2: Find a Particular Solution to the Non-Homogeneous Equation \\n\\nSince the non-homogeneous term is e^-x, we guess a particular solution of the form:\\n\\n```y_p(x) = Ae^-x```\\n\\nDifferentiating twice, we get:\\n\\n```y_p'(x) = -Ae^-x```\\n\\n```y_p''(x) = Ae^-x```\\n\\nSubstituting these into the non-homogeneous equation, we get:\\n\\n```Ae^-x - Ae^-x - 2Ae^-x = e^-x```\\n\\nSolving for A, we get:\\n\\n```A = 1/2```\\n\\nTherefore, the particular solution is:\\n\\n```y_p(x) = (1/2)e^-x```\\n\\n Step 3: Combine the Homogeneous and Particular Solutions \\n\\nThe general solution to the non-homogeneous equation is:\\n\\n```y(x) = y_h(x) + y_p(x)```\\n\\n```y(x) = e^x (c_1 cos √3 x + c_2 sin √3 x) + (1/2)e^-x```<|user|> Find the general solution to the following differential equation:\\n\\n```y''' - 3y'' + 2y' - y = 0```<|assistant|> Step 1: Find the Solution to the Homogeneous Equation \\n\\nThe homogeneous equation is:\\n\\n```y''' - 3y'' + 2y' - y = 0```\\n\\nIts characteristic equation is:\\n\\n```r^3 - 3r^2 + 2r - 1 = 0```\\n\\nFactoring, we get:\\n\\n```(r - 1)^2 (r - 1) = 0```\\n\\nTherefore, the roots are:\\n\\n```r = 1, 1, 1```\\n\\nTherefore, the solution to the homogeneous equation is:\\n\\n```y_h(x) = c_1 e^x + c_2 x e^x + c_3 x^2 e^x```\\n\\n Step 2: Find a Particular Solution to the Non-Homogeneous Equation \\n\\nSince the non-homogeneous term is 0, the particular solution is:\\n\\n```y_p(x) = 0```\\n\\n Step 3: Combine the Homogeneous and Particular Solutions \\n\\nThe general solution to the non-homogeneous equation is:\\n\\n```y(x) = y_h(x) + y_p(x)```\\n\\n```y(x) = c_1 e^x + c_2 x e^x + c_3 x^2 e^x```text2: <|user|> Consider the differential equation:\\n\\n```y'' + y' - 2y = e^-x```\\n\\nSolve this equation using the method of undetermined coefficients.<|assistant|> Step 1: Find the Solution to the Homogeneous Equation \\n\\nThe homogeneous equation is:\\n\\n```y'' + y' - 2y = 0```\\n\\nIts characteristic equation is:\\n\\n```r^2 + r - 2 = 0```\\n\\nSolving for the roots, we get:\\n\\n```r = 1 ± √3i```\\n\\nTherefore, the solution to the homogeneous equation is:\\n\\n```y_h(x) = e^x (c_1 cos √3 x + c_2 sin √3 x)```\\n\\n Step 2: Find a Particular Solution to the Non-Homogeneous Equation \\n\\nSince the non-homogeneous term is e^-x, we guess a particular solution of the form:\\n\\n```y_p(x) = Ae^-x```\\n\\nDifferentiating twice, we get:\\n\\n```y_p'(x) = -Ae^-x```\\n\\n```y_p''(x) = Ae^-x```\\n\\nSubstituting these into the non-homogeneous equation, we get:\\n\\n```Ae^-x - Ae^-x - 2Ae^-x = e^-x```\\n\\nSolving for A, we get:\\n\\n```A = 1/2```\\n\\nTherefore, the particular solution is:\\n\\n```y_p(x) = (1/2)e^-x```\\n\\n Step 3: Combine the Homogeneous and Particular Solutions \\n\\nThe general solution to the non-homogeneous equation is:\\n\\n```y(x) = y_h(x) + y_p(x)```\\n\\n```y(x) = e^x (c_1 cos √3 x + c_2 sin √3 x) + (1/2)e^-x```<|user|> Find the general solution to the following differential equation:\\n\\n```y''' - 3y'' + 2y' - y = 0```<|assistant|> Step 1: Find the Solution to the Homogeneous Equation \\n\\nThe homogeneous equation is:\\n\\n```y''' - 3y'' + 2y' - y = 0```\\n\\nIts characteristic equation is:\\n\\n```r^3 - 3r^2 + 2r - 1 = 0```\\n\\nFactoring, we get:\\n\\n```(r - 1)^2 (r - 1) = 0```\\n\\nTherefore, the roots are:\\n\\n```r = 1, 1, 1```\\n\\nTherefore, the solution to the homogeneous equation is:\\n\\n```y_h(x) = c_1 e^x + c_2 x e^x + c_3 x^2 e^x```\\n\\n Step 2: Find a Particular Solution to the Non-Homogeneous Equation \\n\\nSince the non-homogeneous term is 0, the particular solution is:\\n\\n```y_p(x) = 0```\\n\\n Step 3: Combine the Homogeneous and Particular Solutions \\n\\nThe general solution to the non-homogeneous equation is:\\n\\n```y(x) = y_h(x) + y_p(x)```\\n\\n```y(x) = c_1 e^x + c_2 x e^x + c_3 x^2 e^x```\"]\n",
    "\n",
    "sample = extended_tokenizer(texts, add_special_tokens=False)\n",
    "ids_list = torch.tensor(sample[\"input_ids\"][0])\n",
    "print(ids_list)\n",
    "model(ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_new_tokens: 1000\n",
      "len(tokenizer): 129256\n",
      "```Ċ\n",
      "Extending model embeddings with strategy: merge\n",
      "The OrderedVocab you are attempting to save contains holes for indices [128000, 128001, 128002, 128003, 128004, 128005, 128006, 128007, 128008, 128009, 128010, 128011, 128012, 128013, 128014, 128015, 128016, 128017, 128018, 128019, 128020, 128021, 128022, 128023, 128024, 128025, 128026, 128027, 128028, 128029, 128030, 128031, 128032, 128033, 128034, 128035, 128036, 128037, 128038, 128039, 128040, 128041, 128042, 128043, 128044, 128045, 128046, 128047, 128048, 128049, 128050, 128051, 128052, 128053, 128054, 128055, 128056, 128057, 128058, 128059, 128060, 128061, 128062, 128063, 128064, 128065, 128066, 128067, 128068, 128069, 128070, 128071, 128072, 128073, 128074, 128075, 128076, 128077, 128078, 128079, 128080, 128081, 128082, 128083, 128084, 128085, 128086, 128087, 128088, 128089, 128090, 128091, 128092, 128093, 128094, 128095, 128096, 128097, 128098, 128099, 128100, 128101, 128102, 128103, 128104, 128105, 128106, 128107, 128108, 128109, 128110, 128111, 128112, 128113, 128114, 128115, 128116, 128117, 128118, 128119, 128120, 128121, 128122, 128123, 128124, 128125, 128126, 128127, 128128, 128129, 128130, 128131, 128132, 128133, 128134, 128135, 128136, 128137, 128138, 128139, 128140, 128141, 128142, 128143, 128144, 128145, 128146, 128147, 128148, 128149, 128150, 128151, 128152, 128153, 128154, 128155, 128156, 128157, 128158, 128159, 128160, 128161, 128162, 128163, 128164, 128165, 128166, 128167, 128168, 128169, 128170, 128171, 128172, 128173, 128174, 128175, 128176, 128177, 128178, 128179, 128180, 128181, 128182, 128183, 128184, 128185, 128186, 128187, 128188, 128189, 128190, 128191, 128192, 128193, 128194, 128195, 128196, 128197, 128198, 128199, 128200, 128201, 128202, 128203, 128204, 128205, 128206, 128207, 128208, 128209, 128210, 128211, 128212, 128213, 128214, 128215, 128216, 128217, 128218, 128219, 128220, 128221, 128222, 128223, 128224, 128225, 128226, 128227, 128228, 128229, 128230, 128231, 128232, 128233, 128234, 128235, 128236, 128237, 128238, 128239, 128240, 128241, 128242, 128243, 128244, 128245, 128246, 128247, 128248, 128249, 128250, 128251, 128252, 128253, 128254, 128255], your vocabulary could be corrupted !\n",
      "The OrderedVocab you are attempting to save contains holes for indices [128000, 128001, 128002, 128003, 128004, 128005, 128006, 128007, 128008, 128009, 128010, 128011, 128012, 128013, 128014, 128015, 128016, 128017, 128018, 128019, 128020, 128021, 128022, 128023, 128024, 128025, 128026, 128027, 128028, 128029, 128030, 128031, 128032, 128033, 128034, 128035, 128036, 128037, 128038, 128039, 128040, 128041, 128042, 128043, 128044, 128045, 128046, 128047, 128048, 128049, 128050, 128051, 128052, 128053, 128054, 128055, 128056, 128057, 128058, 128059, 128060, 128061, 128062, 128063, 128064, 128065, 128066, 128067, 128068, 128069, 128070, 128071, 128072, 128073, 128074, 128075, 128076, 128077, 128078, 128079, 128080, 128081, 128082, 128083, 128084, 128085, 128086, 128087, 128088, 128089, 128090, 128091, 128092, 128093, 128094, 128095, 128096, 128097, 128098, 128099, 128100, 128101, 128102, 128103, 128104, 128105, 128106, 128107, 128108, 128109, 128110, 128111, 128112, 128113, 128114, 128115, 128116, 128117, 128118, 128119, 128120, 128121, 128122, 128123, 128124, 128125, 128126, 128127, 128128, 128129, 128130, 128131, 128132, 128133, 128134, 128135, 128136, 128137, 128138, 128139, 128140, 128141, 128142, 128143, 128144, 128145, 128146, 128147, 128148, 128149, 128150, 128151, 128152, 128153, 128154, 128155, 128156, 128157, 128158, 128159, 128160, 128161, 128162, 128163, 128164, 128165, 128166, 128167, 128168, 128169, 128170, 128171, 128172, 128173, 128174, 128175, 128176, 128177, 128178, 128179, 128180, 128181, 128182, 128183, 128184, 128185, 128186, 128187, 128188, 128189, 128190, 128191, 128192, 128193, 128194, 128195, 128196, 128197, 128198, 128199, 128200, 128201, 128202, 128203, 128204, 128205, 128206, 128207, 128208, 128209, 128210, 128211, 128212, 128213, 128214, 128215, 128216, 128217, 128218, 128219, 128220, 128221, 128222, 128223, 128224, 128225, 128226, 128227, 128228, 128229, 128230, 128231, 128232, 128233, 128234, 128235, 128236, 128237, 128238, 128239, 128240, 128241, 128242, 128243, 128244, 128245, 128246, 128247, 128248, 128249, 128250, 128251, 128252, 128253, 128254, 128255], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "import efficient_tokenization.extend_embeddings as extend_embeddings\n",
    "\n",
    "import importlib\n",
    "importlib.reload(extend_embeddings)\n",
    "import json\n",
    "\n",
    "from finetune import my_custom_forward\n",
    "\n",
    "# tokenizer_json = json.loads(tokenizer._tokenizer.to_str())\n",
    "\n",
    "# merge_list = tokenizer_json[\"model\"][\"merges\"]\n",
    "# print(merge_list[0])\n",
    "# print(len(merge_list))\n",
    "embedding_init_strategy = \"merge\"\n",
    "new_vocab_size = len(tokenizer)\n",
    "original_vocab_size = model.config.vocab_size\n",
    "num_new_tokens = new_vocab_size - original_vocab_size\n",
    "print(f\"num_new_tokens: {num_new_tokens}\")\n",
    "print(f\"len(tokenizer): {new_vocab_size}\")\n",
    "\n",
    "print(tokenizer._tokenizer.id_to_token(128260))\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(128000)\n",
    "\n",
    "# Extend model embeddings\n",
    "print(f\"Extending model embeddings with strategy: {embedding_init_strategy}\")\n",
    "model = extend_embeddings.extend_model_embeddings(\n",
    "    model, \n",
    "    num_new_tokens, \n",
    "    init_strategy=embedding_init_strategy,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# model.forward = my_custom_forward.__get__(model, type(model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   2675,    527,    264,  11190,  15592,  18328,   1664,   5553,\n",
      "            291,    304,  40916,    279,   1984,   1203,    311,   1124,    304,\n",
      "            459,  13890,    719,    810,  11297,   1648,     13,    578,   4113,\n",
      "           1984,    690,    387,  16717,    555,    364,   1342,    311,  13454,\n",
      "           4989,    323,    842,    449,    364,    408,   1495,   3238,    220,\n",
      "            578,  13454,   3857,    690,    387,  16717,    555,    364,  31724,\n",
      "          92188,   1342,    311,  13454,     25,  83739,    882,     91,     29,\n",
      "            763,    264,  12960,    449,   4219,    507,     11,    264,  44321,\n",
      "          14469,    374,  77933,    311,    279,  23899,   6812,    520,   1486,\n",
      "            423,     13,   1442,   9827,    284,    220,     19,  10166,    323,\n",
      "          11162,    284,    220,     21,  10166,     11,   1505,    279,  10801,\n",
      "            315,    279,  12960,  16134,     91,  78191,     91,     29,  15166,\n",
      "            220,     16,     25,  21194,   5468,  96462,  46295,    578,  13475,\n",
      "            311,  22217,  22858,    720,  14196,   4077,   1846,     61,     17,\n",
      "            489,  11162,     61,     17,    284,  10807,     61,     17,    198,\n",
      "             19,     61,     17,    489,    220,     21,     61,     17,    284,\n",
      "          10807,     61,     17,    198,   4103,    284,  10807,     61,     17,\n",
      "            198,  14196,  19884,  15166,    220,     17,     25,   8876,  10807,\n",
      "            374,    279,  23899,     11,    433,    374,   6273,    311,    220,\n",
      "             17,     81,     11,   1405,    436,    374,    279,  10801,    315,\n",
      "            279,  12960,     13,    720,  14196,   4077,   1741,    284,    220,\n",
      "             17,     81,    198,     17,     81,    284, 122371,   4103,    198,\n",
      "             17,     81,    284,    220,     17, 110682,   1032,    198,  14196,\n",
      "          19884,  15166,    220,     18,     25,  64384,    369,    436,    720,\n",
      "          14196,   4077,     81,    284, 122371,   1032,    198,  14196,  19884,\n",
      "          55915,     11,    279,  10801,    315,    279,  12960,    374, 122371,\n",
      "           1032,  10166,  16134,     91,    882,     91,     29,   1442,    264,\n",
      "           1584,  10449,    315,   3160,    220,    605,  10166,    374,  69760,\n",
      "            311,    264,  12960,    315,  10801,    436,     11,   1505,    279,\n",
      "           6138,    505,    279,   1486,    315,  22636,   2301,    311,    279,\n",
      "           4219,    315,    279,  12960,  16134,     91,  78191,     91,     29,\n",
      "           6914,    279,   6138,    505,    279,   1486,    315,  22636,   2301,\n",
      "            311,    279,   4219,    315,    279,  12960,    387,    865,     13,\n",
      "           3296,    279,   5468,  96462,  46295,    578,  13475,     11,    584,\n",
      "            617,   1473,  14196,   4077,   2666,    489,    865,  30876,     17,\n",
      "            284,    220,    605,     61,     17,    489,    436,     61,     17,\n",
      "            198,     81,     61,     17,    489,    220,     17,  12940,    489,\n",
      "            865,     61,     17,    284,    220,   1041,    489,    436,     61,\n",
      "             17,    198,     87,     61,     17,    489,    220,     17,  12940,\n",
      "            284,    220,   1041,    198,     87,   2120,    489,    220,     17,\n",
      "             81,      8,    284,    220,   1041,    198,  14196,  19884,  12834,\n",
      "            865,    374,   6928,     11,    584,    617,    865,    489,    220,\n",
      "             17,     81,    284,    220,    508,     13,  15636,   3638,  14196,\n",
      "           4077,     87,    284,    220,    508,    482,    220,     17,     81,\n",
      "            198,  74694,    842,   1495,     13,    720,  31724,     25,    220,\n",
      "             27,     91,    882,     91,     29,    763,    264,  12960,    449,\n",
      "           4219,    507, 128572,  44321,  14469,    374,  77933, 128285,  23899,\n",
      "           6812,    520,   1486,    423, 128546,   9827, 128384,  10166,    323,\n",
      "          11162, 128701,  10166, 128978, 129040,  12960,  16134,     91,  78191,\n",
      "             91,     29,  15166, 128347,  21194,   5468,  96462,  46295, 128367,\n",
      "            311,  22217,  22858,    720, 128260,   1846, 128270,  11162, 129195,\n",
      "          10807, 128257,    198,     19, 128270, 128381, 129195,  10807, 128257,\n",
      "            198,   4103,    284,  10807, 128257, 128272, 128408,   8876,  10807,\n",
      "         128275,  23899, 128550, 129038, 128259,     81, 128447,    436, 128275,\n",
      "         129040,  12960, 129002, 128260,   1741, 128282,     81,    198,     17,\n",
      "             81,    284, 122371,   4103,    198,     17,     81, 128282, 110682,\n",
      "           1032, 128272, 128461,  64384,    369,    436,    720, 128260,     81,\n",
      "            284, 122371,   1032, 128402, 129040,  12960,    374, 122371,   1032,\n",
      "          10166,  16134,     91,    882,     91,     29,   1442,    264,   1584,\n",
      "          10449,    315,   3160, 128398,  10166,    374,  69760, 128604,  12960,\n",
      "            315,  10801,    436, 128978,   6138, 128524,   1486,    315,  22636,\n",
      "           2301, 128285,   4219, 128261,  12960,  16134,     91,  78191,     91,\n",
      "             29,   6914,    279,   6138, 128524,   1486,    315,  22636,   2301,\n",
      "         128285,   4219, 128261,  12960,    387,    865, 128834,   5468,  96462,\n",
      "          46295, 128367, 128481,   2666, 128573, 128295, 128742, 128270,    436,\n",
      "         128257,    198,     81, 128669,  12940,    489, 128366, 129175, 129197,\n",
      "         128257,    198, 128357, 128259,  12940, 129175, 128681,   2120, 128294,\n",
      "             81, 128433,   1041, 128272,  12834,    865,    374,   6928, 128318,\n",
      "            865, 128294,     81, 128256,    508, 128315, 128996,     87, 128256,\n",
      "            508, 128283,     81, 128339]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]]), 'labels': tensor([[128000,   2675,    527,    264,  11190,  15592,  18328,   1664,   5553,\n",
      "            291,    304,  40916,    279,   1984,   1203,    311,   1124,    304,\n",
      "            459,  13890,    719,    810,  11297,   1648,     13,    578,   4113,\n",
      "           1984,    690,    387,  16717,    555,    364,   1342,    311,  13454,\n",
      "           4989,    323,    842,    449,    364,    408,   1495,   3238,    220,\n",
      "            578,  13454,   3857,    690,    387,  16717,    555,    364,  31724,\n",
      "          92188,   1342,    311,  13454,     25,  83739,    882,     91,     29,\n",
      "            763,    264,  12960,    449,   4219,    507,     11,    264,  44321,\n",
      "          14469,    374,  77933,    311,    279,  23899,   6812,    520,   1486,\n",
      "            423,     13,   1442,   9827,    284,    220,     19,  10166,    323,\n",
      "          11162,    284,    220,     21,  10166,     11,   1505,    279,  10801,\n",
      "            315,    279,  12960,  16134,     91,  78191,     91,     29,  15166,\n",
      "            220,     16,     25,  21194,   5468,  96462,  46295,    578,  13475,\n",
      "            311,  22217,  22858,    720,  14196,   4077,   1846,     61,     17,\n",
      "            489,  11162,     61,     17,    284,  10807,     61,     17,    198,\n",
      "             19,     61,     17,    489,    220,     21,     61,     17,    284,\n",
      "          10807,     61,     17,    198,   4103,    284,  10807,     61,     17,\n",
      "            198,  14196,  19884,  15166,    220,     17,     25,   8876,  10807,\n",
      "            374,    279,  23899,     11,    433,    374,   6273,    311,    220,\n",
      "             17,     81,     11,   1405,    436,    374,    279,  10801,    315,\n",
      "            279,  12960,     13,    720,  14196,   4077,   1741,    284,    220,\n",
      "             17,     81,    198,     17,     81,    284, 122371,   4103,    198,\n",
      "             17,     81,    284,    220,     17, 110682,   1032,    198,  14196,\n",
      "          19884,  15166,    220,     18,     25,  64384,    369,    436,    720,\n",
      "          14196,   4077,     81,    284, 122371,   1032,    198,  14196,  19884,\n",
      "          55915,     11,    279,  10801,    315,    279,  12960,    374, 122371,\n",
      "           1032,  10166,  16134,     91,    882,     91,     29,   1442,    264,\n",
      "           1584,  10449,    315,   3160,    220,    605,  10166,    374,  69760,\n",
      "            311,    264,  12960,    315,  10801,    436,     11,   1505,    279,\n",
      "           6138,    505,    279,   1486,    315,  22636,   2301,    311,    279,\n",
      "           4219,    315,    279,  12960,  16134,     91,  78191,     91,     29,\n",
      "           6914,    279,   6138,    505,    279,   1486,    315,  22636,   2301,\n",
      "            311,    279,   4219,    315,    279,  12960,    387,    865,     13,\n",
      "           3296,    279,   5468,  96462,  46295,    578,  13475,     11,    584,\n",
      "            617,   1473,  14196,   4077,   2666,    489,    865,  30876,     17,\n",
      "            284,    220,    605,     61,     17,    489,    436,     61,     17,\n",
      "            198,     81,     61,     17,    489,    220,     17,  12940,    489,\n",
      "            865,     61,     17,    284,    220,   1041,    489,    436,     61,\n",
      "             17,    198,     87,     61,     17,    489,    220,     17,  12940,\n",
      "            284,    220,   1041,    198,     87,   2120,    489,    220,     17,\n",
      "             81,      8,    284,    220,   1041,    198,  14196,  19884,  12834,\n",
      "            865,    374,   6928,     11,    584,    617,    865,    489,    220,\n",
      "             17,     81,    284,    220,    508,     13,  15636,   3638,  14196,\n",
      "           4077,     87,    284,    220,    508,    482,    220,     17,     81,\n",
      "            198,  74694,    842,   1495,     13,    720,  31724,     25,    220,\n",
      "             27,     91,    882,     91,     29,    763,    264,  12960,    449,\n",
      "           4219,    507, 128572,  44321,  14469,    374,  77933, 128285,  23899,\n",
      "           6812,    520,   1486,    423, 128546,   9827, 128384,  10166,    323,\n",
      "          11162, 128701,  10166, 128978, 129040,  12960,  16134,     91,  78191,\n",
      "             91,     29,  15166, 128347,  21194,   5468,  96462,  46295, 128367,\n",
      "            311,  22217,  22858,    720, 128260,   1846, 128270,  11162, 129195,\n",
      "          10807, 128257,    198,     19, 128270, 128381, 129195,  10807, 128257,\n",
      "            198,   4103,    284,  10807, 128257, 128272, 128408,   8876,  10807,\n",
      "         128275,  23899, 128550, 129038, 128259,     81, 128447,    436, 128275,\n",
      "         129040,  12960, 129002, 128260,   1741, 128282,     81,    198,     17,\n",
      "             81,    284, 122371,   4103,    198,     17,     81, 128282, 110682,\n",
      "           1032, 128272, 128461,  64384,    369,    436,    720, 128260,     81,\n",
      "            284, 122371,   1032, 128402, 129040,  12960,    374, 122371,   1032,\n",
      "          10166,  16134,     91,    882,     91,     29,   1442,    264,   1584,\n",
      "          10449,    315,   3160, 128398,  10166,    374,  69760, 128604,  12960,\n",
      "            315,  10801,    436, 128978,   6138, 128524,   1486,    315,  22636,\n",
      "           2301, 128285,   4219, 128261,  12960,  16134,     91,  78191,     91,\n",
      "             29,   6914,    279,   6138, 128524,   1486,    315,  22636,   2301,\n",
      "         128285,   4219, 128261,  12960,    387,    865, 128834,   5468,  96462,\n",
      "          46295, 128367, 128481,   2666, 128573, 128295, 128742, 128270,    436,\n",
      "         128257,    198,     81, 128669,  12940,    489, 128366, 129175, 129197,\n",
      "         128257,    198, 128357, 128259,  12940, 129175, 128681,   2120, 128294,\n",
      "             81, 128433,   1041, 128272,  12834,    865,    374,   6928, 128318,\n",
      "            865, 128294,     81, 128256,    508, 128315, 128996,     87, 128256,\n",
      "            508, 128283,     81, 128339]]), 'loss_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])}\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from finetune import MyPaddingCollatorWithLossMask\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized\"\n",
    "\n",
    "ds = datasets.load_from_disk(dataset_path)\n",
    "dl = DataLoader(ds.select(range(10)), batch_size=1, shuffle=True, collate_fn=MyPaddingCollatorWithLossMask(tokenizer=tokenizer))\n",
    "\n",
    "batch = next(iter(dl))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(129256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LigerSwiGLUMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "        )\n",
      "        (input_layernorm): LigerRMSNorm((2048,), eps=1e-05, offset=0.0, in_place=True)\n",
      "        (post_attention_layernorm): LigerRMSNorm((2048,), eps=1e-05, offset=0.0, in_place=True)\n",
      "      )\n",
      "    )\n",
      "    (norm): LigerRMSNorm((2048,), eps=1e-05, offset=0.0, in_place=True)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=129256, bias=False)\n",
      ")\n",
      "input_ids torch.Size([1, 625]) cuda:0\n",
      "attention_mask torch.Size([1, 625]) cuda:0\n",
      "labels torch.Size([1, 625]) cuda:0\n",
      "loss_mask torch.Size([1, 625]) cuda:0\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "model.train()\n",
    "\n",
    "print(model.device)\n",
    "print(model)\n",
    "for b, t in batch.items():\n",
    "    print(b, t.shape, t.device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0396, device='cuda:0',\n",
      "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>)\n",
      "tensor(2.3922, device='cuda:0')\n",
      "tensor(1.7377, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# first do the loss on everything\n",
    "input_ids = batch[\"input_ids\"]\n",
    "labels = batch[\"labels\"]\n",
    "loss_mask = batch[\"loss_mask\"]\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "outputs = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=labels,\n",
    "    use_cache=False,\n",
    "    # num_items_in_batch=num_items_in_batch,\n",
    "    # new_token_start_index=original_vocab_size\n",
    ")\n",
    "# gets logits\n",
    "loss = outputs.loss\n",
    "print(loss)\n",
    "# loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Loss only on some of the tokens\n",
    "    masked_labels = labels.clone()\n",
    "    masked_labels[loss_mask==0] = -100\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=masked_labels,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    loss = outputs.loss\n",
    "    print(loss)\n",
    "    # loss.backward()\n",
    "\n",
    "    # then do the loss on only the tokens from the new tokenizer (as masked)\n",
    "\n",
    "    new_labels = labels.clone()\n",
    "    new_labels[new_labels > original_vocab_size] = -100\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=new_labels,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    loss = outputs.loss\n",
    "    print(loss)\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129256"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=tensor(0.8905, device='cuda:0',\n",
       "       grad_fn=<LigerFusedLinearCrossEntropyFunctionBackward>), logits=None, past_key_values=None, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_embeddings_list = extend_embeddings.get_new_embeddings(model, num_new_tokens)\n",
    "# new_embeddings_list[0]\n",
    "\n",
    "embeddings_output = model.get_output_embeddings()\n",
    "print(f\"embeddings_output: {embeddings_output}\")\n",
    "print(f\"embeddings_output.weight: {embeddings_output.weight}\")\n",
    "print(f\"embeddings_output.weight.data: {embeddings_output.weight.data}\")\n",
    "print(f\"embeddings_output.weight.grad: {embeddings_output.weight.grad}\")\n",
    "\n",
    "params = extend_embeddings.get_new_embedding_params(model, num_new_tokens)\n",
    "this_param = params[0]\n",
    "this_param.retain_grad()\n",
    "print(f\"params: {params}, length: {len(params)}\")\n",
    "print(f\"this_param: {this_param}, shape: {this_param.shape}\")\n",
    "print(f\"params.data: {this_param.data}, shape: {this_param.data.shape}\")\n",
    "print(f\"params.grad: {this_param.grad}, shape: {this_param.grad.shape if this_param.grad is not None else 'None'}\")\n",
    "\n",
    "grads = extend_embeddings.get_new_embeddings_grads(model, num_new_tokens)[0]\n",
    "print(f\"grads: {grads}, shape: {grads.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficient_tokenization.tokenize_simple as tokenize_simple\n",
    "from efficient_tokenization.tokenize_simple import get_tokenizer\n",
    "\n",
    "import transformers\n",
    "\n",
    "import importlib\n",
    "importlib.reload(tokenize_simple)\n",
    "importlib.reload(transformers)\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer_path = \"/cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start/new_mergeable_ranks_2000.model\"\n",
    "base_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = get_tokenizer(tokenizer_path, old_tokenizer=base_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n",
    "    characters the bpe code barfs on.\n",
    "    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n",
    "    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n",
    "    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n",
    "    tables between utf-8 bytes and unicode strings.\n",
    "    \"\"\"\n",
    "    bs = (\n",
    "        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "byte_encoder = bytes_to_unicode()\n",
    "\n",
    "def token_bytes_to_string(b):\n",
    "    return \"\".join([byte_encoder[ord(char)] for char in b.decode(\"latin-1\")])\n",
    "\n",
    "def unicode_to_bytes():\n",
    "    \"\"\"\n",
    "    Returns a mapping from unicode strings back to their original utf-8 bytes.\n",
    "    This reverses the `bytes_to_unicode` mapping.\n",
    "    \"\"\"\n",
    "    # byte_encoder = bytes_to_unicode()  # Original byte-to-unicode mapping\n",
    "    return {v: k for k, v in byte_encoder.items()}\n",
    "\n",
    "byte_decoder = unicode_to_bytes()\n",
    "\n",
    "def string_to_token_bytes(s):\n",
    "    \"\"\"\n",
    "    Converts a string back into token bytes using the reverse mapping.\n",
    "\n",
    "    Args:\n",
    "        s (str): The input string to convert.\n",
    "\n",
    "    Returns:\n",
    "        bytes: The byte representation of the string.\n",
    "    \"\"\"\n",
    "    return bytes([byte_decoder[char] for char in s])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "# mergeable_ranks = read_tokenizer_from_model(base_tokenizer_path)\n",
    "sorted_vocab = {k: v for k, v in sorted(llama_tokenizer.vocab.items(), key=lambda item: item[1])}\n",
    "for tok, i in sorted_vocab.items():\n",
    "    my_bytes = string_to_token_bytes(tok)\n",
    "    my_string = token_bytes_to_string(my_bytes)\n",
    "    if i > 127988:\n",
    "        print(f\"{i:06d}: {tok}, {my_bytes}, {base64.b64encode(my_bytes)}, {base64.b64decode(tok)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_tokenizer import read_tokenizer_from_model\n",
    "# old_path = \"/cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer/tokenizer.model\"\n",
    "old_path = \"/cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start/new_mergeable_ranks_2000.model\"\n",
    "old_vocab = read_tokenizer_from_model(old_path)\n",
    "\n",
    "joined_tokenizer = old_vocab.copy()\n",
    "joined_tokenizer.update(llama_tokenizer.get_added_vocab())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok, i in old_vocab.items():\n",
    "    my_bytes = string_to_token_bytes(tok)\n",
    "    if i > 127988:\n",
    "        print(f\"{i:06d}: {tok}, {my_bytes}, {base64.b64encode(my_bytes)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "def compare_dicts(dict1: Dict, dict2: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    Compare two dictionaries and print differences if they exist.\n",
    "    \n",
    "    Args:\n",
    "        dict1: First dictionary\n",
    "        dict2: Second dictionary\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if dictionaries are identical, False otherwise\n",
    "    \"\"\"\n",
    "    if dict1.keys() != dict2.keys():\n",
    "        print(\"Different keys:\")\n",
    "        print(\"Keys only in first dict:\", set(dict1.keys()) - set(dict2.keys()))\n",
    "        print(\"Keys only in second dict:\", set(dict2.keys()) - set(dict1.keys()))\n",
    "        return False\n",
    "    \n",
    "    differences = {\n",
    "        k: (dict1[k], dict2[k])\n",
    "        for k in dict1\n",
    "        if dict1[k] != dict2[k]\n",
    "    }\n",
    "    \n",
    "    if differences:\n",
    "        print(\"Different values:\")\n",
    "        for k, (v1, v2) in differences.items():\n",
    "            print(f\"Key: {k}\")\n",
    "            print(f\"  Dict1: {v1}\")\n",
    "            print(f\"  Dict2: {v2}\")\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "are_same = compare_dicts(sorted_vocab, joined_tokenizer)\n",
    "print(\"Dictionaries are identical:\", are_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lm_eval \n",
    "importlib.reload(lm_eval)\n",
    "\n",
    "model_args_dict = {\n",
    "    \"pretrained\": model,  # This will be your model object directly\n",
    "    \"tokenizer\": tokenizer,  # This will be your tokenizer object directly\n",
    "    \"old_tokenizer\": base_tokenizer,  # This will be your tokenizer object directly\n",
    "    # \"parallelize\": True,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 3,\n",
    "    \"trust_remote_code\": True\n",
    "}\n",
    "\n",
    "LM_model = lm_eval.models.huggingface.HFLM(**model_args_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'update_step': 3000,\n",
       " 'epoch': 2,\n",
       " 'epoch_step': 3000,\n",
       " 'total_batched_samples': 96032,\n",
       " 'cumulative_batch_counter': 768256,\n",
       " 'cumulative_token_counter': 472031348,\n",
       " 'cumulative_new_token_counter': 61548130}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.load(\"/cmlscratch/astein0/efficient_tokenization_for_inference/output/cb56868f-Llama-3.2-1B-translation/checkpoints/checkpoint_2/checkpoint_meta.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshomes/astein0/.pyenv/versions/3.10.4/envs/eff-tok/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask', 'labels', 'loss_mask', 'num_tokens'],\n",
       "    num_rows: 515509\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "ds = load_from_disk(\"/cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized\")\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64438\n",
      "2013\n"
     ]
    }
   ],
   "source": [
    "rows = 515509\n",
    "devices = 8\n",
    "grad_steps = 32\n",
    "per_device = rows // devices\n",
    "print(per_device)\n",
    "num_updates = per_device // grad_steps\n",
    "print(num_updates)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshomes/astein0/.pyenv/versions/3.10.4/envs/eff-tok/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from efficient_tokenization.extend_embeddings import extend_model_embeddings\n",
    "from liger_kernel.transformers import AutoLigerKernelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "import_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/output/baseline_embeddings/e08a94c1-Llama-3.2-3B-mixed-1000/final_model\"\n",
    "\n",
    "embedding_init_strategy = \"import\"\n",
    "\n",
    "model_path = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "num_new_tokens = 1000\n",
    "tokenizer_path = \"/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoLigerKernelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "model = extend_model_embeddings(\n",
    "    model,\n",
    "    num_new_tokens,\n",
    "    init_strategy=embedding_init_strategy,\n",
    "    tokenizer=tokenizer,\n",
    "    import_path=import_path\n",
    ")\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790, 128460,   1419,    198,  15724,   2696,     25,    220,\n",
       "           2705,   3297, 128460,    914,    271, 128009, 128006,    882, 128007,\n",
       "            271, 128438,    220,   3101,     10,   3492,  12399, 128258,  59318,\n",
       "           2199,    330,   2485,   1129,    268,  34466,   2726,  26583,  19945,\n",
       "            352,  12669,     62,  23440,     11,  51875,   3659,   1159,   4664,\n",
       "          14559,   3343,   3234,    539,   1005,    904,  77702,    323,  11415,\n",
       "         129070, 128387,  14491, 129211,  15671,    304,  51594,   3645,     11,\n",
       "            369,   3187,    353,  36298,    291,   3857,    961, 128335,  12594,\n",
       "            353,  36298,    291,   3857,    961, 128359,  12594,    353,  36298,\n",
       "            291,   3857,    961, 128387,  20517, 128009, 128006,  78191, 128007,\n",
       "            271]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 06 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWrite a 300+ word summary of the wikipedia page \\\"https://en.wikipedia.org/wiki/Raymond_III,_Count_of_Tripoli\\\". Do not use any commas and highlight at least 3 sections that has titles in markdown format, for example *highlighted section part 1*, *highlighted section part 2*, *highlighted section part 3*.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "tokenized_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(**tokenized_prompt, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790, 128460,   1419,    198,  15724,   2696,     25,    220,\n",
      "           2705,   3297, 128460,    914,    271, 128009, 128006,    882, 128007,\n",
      "            271, 128438,    220,   3101,     10,   3492,  12399, 128258,  59318,\n",
      "           2199,    330,   2485,   1129,    268,  34466,   2726,  26583,  19945,\n",
      "            352,  12669,     62,  23440,     11,  51875,   3659,   1159,   4664,\n",
      "          14559,   3343,   3234,    539,   1005,    904,  77702,    323,  11415,\n",
      "         129070, 128387,  14491, 129211,  15671,    304,  51594,   3645,     11,\n",
      "            369,   3187,    353,  36298,    291,   3857,    961, 128335,  12594,\n",
      "            353,  36298,    291,   3857,    961, 128359,  12594,    353,  36298,\n",
      "            291,   3857,    961, 128387,  20517, 128009, 128006,  78191, 128007,\n",
      "            271,  30287,  12669,  14767,   4605,    315,  27852,  14559, 128371,\n",
      "          19912,    315,   6898,    822,    331,    323,  49080,    315,  27852,\n",
      "          14559,    505,    220,   7322,     17,   3156,    813,   4648,    304,\n",
      "            220,   8899,     22, 128724,   7126,    574,  43670,   8105,   4605,\n",
      "            315,  27852,  14559,    323,    813,   6691,    574,   8613,  18120,\n",
      "         128300,  14433,   1088,   8078, 128724,   1283,    574,    279,  39637,\n",
      "            315,   3892,  26419,    323,    574,   2728,    279,   2316,    315,\n",
      "           4605,    315,  27852,  14559,   1306,    813,   7126,    596,   4648,\n",
      "            304,    220,   7322,     17,    271,      9,  50083,    950,   7220,\n",
      "         100104,  22242,  30287,  12669,  14767, 128371,   5199,   7216,    304,\n",
      "            279,   3925,    315,    279,  51203,   3536, 128724,   1283,    574,\n",
      "            264,   1401]], device='cuda:0')\n",
      "[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 128460, 1419, 198, 15724, 2696, 25, 220, 2705, 3297, 128460, 914, 271, 128009, 128006, 882, 128007, 271, 128438, 220, 3101, 10, 3492, 12399, 128258, 59318, 2199, 330, 2485, 1129, 268, 34466, 2726, 26583, 19945, 352, 12669, 62, 23440, 11, 51875, 3659, 1159, 4664, 14559, 3343, 3234, 539, 1005, 904, 77702, 323, 11415, 129070, 128387, 14491, 129211, 15671, 304, 51594, 3645, 11, 369, 3187, 353, 36298, 291, 3857, 961, 128335, 12594, 353, 36298, 291, 3857, 961, 128359, 12594, 353, 36298, 291, 3857, 961, 128387, 20517, 128009, 128006, 78191, 128007, 271, 30287, 12669, 14767, 4605, 315, 27852, 14559, 128371, 19912, 315, 6898, 822, 331, 323, 49080, 315, 27852, 14559, 505, 220, 7322, 17, 3156, 813, 4648, 304, 220, 8899, 22, 128724, 7126, 574, 43670, 8105, 4605, 315, 27852, 14559, 323, 813, 6691, 574, 8613, 18120, 128300, 14433, 1088, 8078, 128724, 1283, 574, 279, 39637, 315, 3892, 26419, 323, 574, 2728, 279, 2316, 315, 4605, 315, 27852, 14559, 1306, 813, 7126, 596, 4648, 304, 220, 7322, 17, 271, 9, 50083, 950, 7220, 100104, 22242, 30287, 12669, 14767, 128371, 5199, 7216, 304, 279, 3925, 315, 279, 51203, 3536, 128724, 1283, 574, 264, 1401]\n",
      "0: 128000, <|begin_of_text|>\n",
      "1: 128006, <|start_header_id|>\n",
      "2: 9125, system\n",
      "3: 128007, <|end_header_id|>\n",
      "4: 271, ĊĊ\n",
      "5: 38766, Cut\n",
      "6: 1303, ting\n",
      "7: 33025, ĠKnowledge\n",
      "8: 2696, ĠDate\n",
      "9: 25, :\n",
      "10: 6790, ĠDecember\n",
      "11: 128460, Ġ20\n",
      "12: 1419, 23\n",
      "13: 198, Ċ\n",
      "14: 15724, Today\n",
      "15: 2696, ĠDate\n",
      "16: 25, :\n",
      "17: 220, Ġ\n",
      "18: 2705, 06\n",
      "19: 3297, ĠMay\n",
      "20: 128460, Ġ20\n",
      "21: 914, 25\n",
      "22: 271, ĊĊ\n",
      "23: 128009, <|eot_id|>\n",
      "24: 128006, <|start_header_id|>\n",
      "25: 882, user\n",
      "26: 128007, <|end_header_id|>\n",
      "27: 271, ĊĊ\n",
      "28: 128438, WriteĠa\n",
      "29: 220, Ġ\n",
      "30: 3101, 300\n",
      "31: 10, +\n",
      "32: 3492, Ġword\n",
      "33: 12399, Ġsummary\n",
      "34: 128258, ĠofĠthe\n",
      "35: 59318, Ġwikipedia\n",
      "36: 2199, Ġpage\n",
      "37: 330, Ġ\"\n",
      "38: 2485, https\n",
      "39: 1129, ://\n",
      "40: 268, en\n",
      "41: 34466, .wikipedia\n",
      "42: 2726, .org\n",
      "43: 26583, /wiki\n",
      "44: 19945, /R\n",
      "45: 352, ay\n",
      "46: 12669, mond\n",
      "47: 62, _\n",
      "48: 23440, III\n",
      "49: 11, ,\n",
      "50: 51875, _Count\n",
      "51: 3659, _of\n",
      "52: 1159, _T\n",
      "53: 4664, rip\n",
      "54: 14559, oli\n",
      "55: 3343, \".\n",
      "56: 3234, ĠDo\n",
      "57: 539, Ġnot\n",
      "58: 1005, Ġuse\n",
      "59: 904, Ġany\n",
      "60: 77702, Ġcommas\n",
      "61: 323, Ġand\n",
      "62: 11415, Ġhighlight\n",
      "63: 129070, ĠatĠleast\n",
      "64: 128387, Ġ3\n",
      "65: 14491, Ġsections\n",
      "66: 129211, ĠthatĠhas\n",
      "67: 15671, Ġtitles\n",
      "68: 304, Ġin\n",
      "69: 51594, Ġmarkdown\n",
      "70: 3645, Ġformat\n",
      "71: 11, ,\n",
      "72: 369, Ġfor\n",
      "73: 3187, Ġexample\n",
      "74: 353, Ġ*\n",
      "75: 36298, highlight\n",
      "76: 291, ed\n",
      "77: 3857, Ġsection\n",
      "78: 961, Ġpart\n",
      "79: 128335, Ġ1\n",
      "80: 12594, *,\n",
      "81: 353, Ġ*\n",
      "82: 36298, highlight\n",
      "83: 291, ed\n",
      "84: 3857, Ġsection\n",
      "85: 961, Ġpart\n",
      "86: 128359, Ġ2\n",
      "87: 12594, *,\n",
      "88: 353, Ġ*\n",
      "89: 36298, highlight\n",
      "90: 291, ed\n",
      "91: 3857, Ġsection\n",
      "92: 961, Ġpart\n",
      "93: 128387, Ġ3\n",
      "94: 20517, *.\n",
      "95: 128009, <|eot_id|>\n",
      "96: 128006, <|start_header_id|>\n",
      "97: 78191, assistant\n",
      "98: 128007, <|end_header_id|>\n",
      "99: 271, ĊĊ\n",
      "100: 30287, Ray\n",
      "101: 12669, mond\n",
      "102: 14767, ĠIII\n",
      "103: 4605, ĠCount\n",
      "104: 315, Ġof\n",
      "105: 27852, ĠTrip\n",
      "106: 14559, oli\n",
      "107: 128371, ĠwasĠa\n",
      "108: 19912, ĠPrince\n",
      "109: 315, Ġof\n",
      "110: 6898, ĠAnt\n",
      "111: 822, io\n",
      "112: 331, ch\n",
      "113: 323, Ġand\n",
      "114: 49080, Ġruler\n",
      "115: 315, Ġof\n",
      "116: 27852, ĠTrip\n",
      "117: 14559, oli\n",
      "118: 505, Ġfrom\n",
      "119: 220, Ġ\n",
      "120: 7322, 115\n",
      "121: 17, 2\n",
      "122: 3156, Ġuntil\n",
      "123: 813, Ġhis\n",
      "124: 4648, Ġdeath\n",
      "125: 304, Ġin\n",
      "126: 220, Ġ\n",
      "127: 8899, 118\n",
      "128: 22, 7\n",
      "129: 128724, .ĠHis\n",
      "130: 7126, Ġfather\n",
      "131: 574, Ġwas\n",
      "132: 43670, ĠRaymond\n",
      "133: 8105, ĠII\n",
      "134: 4605, ĠCount\n",
      "135: 315, Ġof\n",
      "136: 27852, ĠTrip\n",
      "137: 14559, oli\n",
      "138: 323, Ġand\n",
      "139: 813, Ġhis\n",
      "140: 6691, Ġmother\n",
      "141: 574, Ġwas\n",
      "142: 8613, ĠMor\n",
      "143: 18120, phia\n",
      "144: 128300, ĠofĠa\n",
      "145: 14433, ĠHa\n",
      "146: 1088, ute\n",
      "147: 8078, ville\n",
      "148: 128724, .ĠHis\n",
      "149: 1283, ĠHe\n",
      "150: 574, Ġwas\n",
      "151: 279, Ġthe\n",
      "152: 39637, Ġyoungest\n",
      "153: 315, Ġof\n",
      "154: 3892, Ġseveral\n",
      "155: 26419, Ġsons\n",
      "156: 323, Ġand\n",
      "157: 574, Ġwas\n",
      "158: 2728, Ġgiven\n",
      "159: 279, Ġthe\n",
      "160: 2316, Ġtitle\n",
      "161: 315, Ġof\n",
      "162: 4605, ĠCount\n",
      "163: 315, Ġof\n",
      "164: 27852, ĠTrip\n",
      "165: 14559, oli\n",
      "166: 1306, Ġafter\n",
      "167: 813, Ġhis\n",
      "168: 7126, Ġfather\n",
      "169: 596, 's\n",
      "170: 4648, Ġdeath\n",
      "171: 304, Ġin\n",
      "172: 220, Ġ\n",
      "173: 7322, 115\n",
      "174: 17, 2\n",
      "175: 271, ĊĊ\n",
      "176: 9, *\n",
      "177: 50083, Histor\n",
      "178: 950, ical\n",
      "179: 7220, ĠSign\n",
      "180: 100104, ificance\n",
      "181: 22242, *ĊĊ\n",
      "182: 30287, Ray\n",
      "183: 12669, mond\n",
      "184: 14767, ĠIII\n",
      "185: 128371, ĠwasĠa\n",
      "186: 5199, Ġsignificant\n",
      "187: 7216, Ġfigure\n",
      "188: 304, Ġin\n",
      "189: 279, Ġthe\n",
      "190: 3925, Ġhistory\n",
      "191: 315, Ġof\n",
      "192: 279, Ġthe\n",
      "193: 51203, ĠCrus\n",
      "194: 3536, ades\n",
      "195: 128724, .ĠHis\n",
      "196: 1283, ĠHe\n",
      "197: 574, Ġwas\n",
      "198: 264, Ġa\n",
      "199: 1401, Ġkey\n"
     ]
    }
   ],
   "source": [
    "print(generated_ids)\n",
    "input_id_examples = generated_ids.tolist()[0]\n",
    "print(input_id_examples)\n",
    "\n",
    "for i in range(len(input_id_examples)):\n",
    "    print(f\"{i}: {input_id_examples[i]}, {tokenizer.convert_ids_to_tokens(input_id_examples[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', '\\n\\n', 'Cut', 'ting', ' Knowledge', ' Date', ':', ' December', ' 20', '23', '\\n', 'Today', ' Date', ':', ' ', '06', ' May', ' 20', '25', '\\n\\n', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', '\\n\\n', 'Write a', ' ', '300', '+', ' word', ' summary', ' of the', ' wikipedia', ' page', ' \"', 'https', '://', 'en', '.wikipedia', '.org', '/wiki', '/R', 'ay', 'mond', '_', 'III', ',', '_Count', '_of', '_T', 'rip', 'oli', '\".', ' Do', ' not', ' use', ' any', ' commas', ' and', ' highlight', ' at least', ' 3', ' sections', ' that has', ' titles', ' in', ' markdown', ' format', ',', ' for', ' example', ' *', 'highlight', 'ed', ' section', ' part', ' 1', '*,', ' *', 'highlight', 'ed', ' section', ' part', ' 2', '*,', ' *', 'highlight', 'ed', ' section', ' part', ' 3', '*.', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '\\n\\n', 'Ray', 'mond', ' III', ' Count', ' of', ' Trip', 'oli', ' was a', ' Prince', ' of', ' Ant', 'io', 'ch', ' and', ' ruler', ' of', ' Trip', 'oli', ' from', ' ', '115', '2', ' until', ' his', ' death', ' in', ' ', '118', '7', '. His', ' father', ' was', ' Raymond', ' II', ' Count', ' of', ' Trip', 'oli', ' and', ' his', ' mother', ' was', ' Mor', 'phia', ' of a', ' Ha', 'ute', 'ville', '. His', ' He', ' was', ' the', ' youngest', ' of', ' several', ' sons', ' and', ' was', ' given', ' the', ' title', ' of', ' Count', ' of', ' Trip', 'oli', ' after', ' his', ' father', \"'s\", ' death', ' in', ' ', '115', '2', '\\n\\n', '*', 'Histor', 'ical', ' Sign', 'ificance', '*\\n\\n', 'Ray', 'mond', ' III', ' was a', ' significant', ' figure', ' in', ' the', ' history', ' of', ' the', ' Crus', 'ades', '. His', ' He', ' was', ' a', ' key']\n",
      "input_length: 200\n",
      "<|begin_of_text|>\n",
      "<|start_header_id|>\n",
      "system\n",
      "<|end_header_id|>\n",
      "ĊĊ\n",
      "Cut\n",
      "ting\n",
      "ĠKnowledge\n",
      "ĠDate\n",
      ":\n",
      "ĠDecember\n",
      "Ġ20\n",
      "23\n",
      "Ċ\n",
      "Today\n",
      "ĠDate\n",
      ":\n",
      "Ġ\n",
      "06\n",
      "ĠMay\n",
      "Ġ20\n",
      "25\n",
      "ĊĊ\n",
      "<|eot_id|>\n",
      "<|start_header_id|>\n",
      "user\n",
      "<|end_header_id|>\n",
      "ĊĊ\n",
      "WriteĠa\n",
      "Ġ\n",
      "300\n",
      "+\n",
      "Ġword\n",
      "Ġsummary\n",
      "ĠofĠthe\n",
      "Ġwikipedia\n",
      "Ġpage\n",
      "Ġ\"\n",
      "https\n",
      "://\n",
      "en\n",
      ".wikipedia\n",
      ".org\n",
      "/wiki\n",
      "/R\n",
      "ay\n",
      "mond\n",
      "_\n",
      "III\n",
      ",\n",
      "_Count\n",
      "_of\n",
      "_T\n",
      "rip\n",
      "oli\n",
      "\".\n",
      "ĠDo\n",
      "Ġnot\n",
      "Ġuse\n",
      "Ġany\n",
      "Ġcommas\n",
      "Ġand\n",
      "Ġhighlight\n",
      "ĠatĠleast\n",
      "Ġ3\n",
      "Ġsections\n",
      "ĠthatĠhas\n",
      "Ġtitles\n",
      "Ġin\n",
      "Ġmarkdown\n",
      "Ġformat\n",
      ",\n",
      "Ġfor\n",
      "Ġexample\n",
      "Ġ*\n",
      "highlight\n",
      "ed\n",
      "Ġsection\n",
      "Ġpart\n",
      "Ġ1\n",
      "*,\n",
      "Ġ*\n",
      "highlight\n",
      "ed\n",
      "Ġsection\n",
      "Ġpart\n",
      "Ġ2\n",
      "*,\n",
      "Ġ*\n",
      "highlight\n",
      "ed\n",
      "Ġsection\n",
      "Ġpart\n",
      "Ġ3\n",
      "*.\n",
      "<|eot_id|>\n",
      "<|start_header_id|>\n",
      "assistant\n",
      "<|end_header_id|>\n",
      "ĊĊ\n",
      "Ray\n",
      "mond\n",
      "ĠIII\n",
      "ĠCount\n",
      "Ġof\n",
      "ĠTrip\n",
      "oli\n",
      "ĠwasĠa\n",
      "ĠPrince\n",
      "Ġof\n",
      "ĠAnt\n",
      "io\n",
      "ch\n",
      "Ġand\n",
      "Ġruler\n",
      "Ġof\n",
      "ĠTrip\n",
      "oli\n",
      "Ġfrom\n",
      "Ġ\n",
      "115\n",
      "2\n",
      "Ġuntil\n",
      "Ġhis\n",
      "Ġdeath\n",
      "Ġin\n",
      "Ġ\n",
      "118\n",
      "7\n",
      ".ĠHis\n",
      "Ġfather\n",
      "Ġwas\n",
      "ĠRaymond\n",
      "ĠII\n",
      "ĠCount\n",
      "Ġof\n",
      "ĠTrip\n",
      "oli\n",
      "Ġand\n",
      "Ġhis\n",
      "Ġmother\n",
      "Ġwas\n",
      "ĠMor\n",
      "phia\n",
      "ĠofĠa\n",
      "ĠHa\n",
      "ute\n",
      "ville\n",
      ".ĠHis\n",
      "ĠHe\n",
      "Ġwas\n",
      "Ġthe\n",
      "Ġyoungest\n",
      "Ġof\n",
      "Ġseveral\n",
      "Ġsons\n",
      "Ġand\n",
      "Ġwas\n",
      "Ġgiven\n",
      "Ġthe\n",
      "Ġtitle\n",
      "Ġof\n",
      "ĠCount\n",
      "Ġof\n",
      "ĠTrip\n",
      "oli\n",
      "Ġafter\n",
      "Ġhis\n",
      "Ġfather\n",
      "'s\n",
      "Ġdeath\n",
      "Ġin\n",
      "Ġ\n",
      "115\n",
      "2\n",
      "ĊĊ\n",
      "*\n",
      "Histor\n",
      "ical\n",
      "ĠSign\n",
      "ificance\n",
      "*ĊĊ\n",
      "Ray\n",
      "mond\n",
      "ĠIII\n",
      "ĠwasĠa\n",
      "Ġsignificant\n",
      "Ġfigure\n",
      "Ġin\n",
      "Ġthe\n",
      "Ġhistory\n",
      "Ġof\n",
      "Ġthe\n",
      "ĠCrus\n",
      "ades\n",
      ".ĠHis\n",
      "ĠHe\n",
      "Ġwas\n",
      "Ġa\n",
      "Ġkey\n",
      "<|begin_of_text|> <|start_header_id|> system <|end_header_id|> ĊĊ  Cut   ting ĠKnowledge ĠDate :  ĠDecember Ġ20    23   Ċ   Today ĠDate :  Ġ   06   ĠMay Ġ20    25  ĊĊ  <|eot_id|> <|start_header_id|>\n",
      "\u001b[1;37m128000\u001b[0m            \u001b[1;31m128006\u001b[0m              \u001b[1;37m9125\u001b[0m   \u001b[1;31m128007\u001b[0m            \u001b[1;37m271\u001b[0m \u001b[1;37m38766\u001b[0m \u001b[1;37m1303\u001b[0m \u001b[1;37m33025\u001b[0m      \u001b[1;37m2696\u001b[0m  \u001b[1;37m25\u001b[0m \u001b[1;37m6790\u001b[0m      \u001b[1;31m128460\u001b[0m \u001b[1;37m1419\u001b[0m \u001b[1;37m198\u001b[0m \u001b[1;37m15724\u001b[0m \u001b[1;37m2696\u001b[0m  \u001b[1;37m25\u001b[0m \u001b[1;37m220\u001b[0m \u001b[1;37m2705\u001b[0m \u001b[1;37m3297\u001b[0m \u001b[1;31m128460\u001b[0m \u001b[1;37m914\u001b[0m \u001b[1;37m271\u001b[0m \u001b[1;31m128009\u001b[0m     \u001b[1;31m128006\u001b[0m             \n",
      "\n",
      "user <|end_header_id|> ĊĊ  WriteĠa Ġ   300  +  Ġword Ġsummary ĠofĠthe Ġwikipedia Ġpage Ġ\"  https ://  en  .wikipedia .org /wiki /R    ay  mond  _  III   ,  _Count _of  _T   rip  oli   \".   ĠDo  Ġnot\n",
      "\u001b[1;37m882\u001b[0m  \u001b[1;31m128007\u001b[0m            \u001b[1;37m271\u001b[0m \u001b[1;31m128438\u001b[0m  \u001b[1;37m220\u001b[0m \u001b[1;37m3101\u001b[0m \u001b[1;37m10\u001b[0m \u001b[1;37m3492\u001b[0m  \u001b[1;37m12399\u001b[0m    \u001b[1;31m128258\u001b[0m  \u001b[1;37m59318\u001b[0m      \u001b[1;37m2199\u001b[0m  \u001b[1;37m330\u001b[0m \u001b[1;37m2485\u001b[0m  \u001b[1;37m1129\u001b[0m \u001b[1;37m268\u001b[0m \u001b[1;37m34466\u001b[0m      \u001b[1;37m2726\u001b[0m \u001b[1;37m26583\u001b[0m \u001b[1;37m19945\u001b[0m \u001b[1;37m352\u001b[0m \u001b[1;37m12669\u001b[0m \u001b[1;37m62\u001b[0m \u001b[1;37m23440\u001b[0m \u001b[1;37m11\u001b[0m \u001b[1;37m51875\u001b[0m  \u001b[1;37m3659\u001b[0m \u001b[1;37m1159\u001b[0m \u001b[1;37m4664\u001b[0m \u001b[1;37m14559\u001b[0m \u001b[1;37m3343\u001b[0m \u001b[1;37m3234\u001b[0m \u001b[1;37m539\u001b[0m \n",
      "\n",
      "Ġuse Ġany Ġcommas Ġand Ġhighlight ĠatĠleast Ġ3     Ġsections ĠthatĠhas Ġtitles Ġin Ġmarkdown Ġformat ,  Ġfor Ġexample Ġ*  highlight ed  Ġsection Ġpart Ġ1     *,    Ġ*  highlight ed  Ġsection Ġpart\n",
      "\u001b[1;37m1005\u001b[0m \u001b[1;37m904\u001b[0m  \u001b[1;37m77702\u001b[0m   \u001b[1;37m323\u001b[0m  \u001b[1;37m11415\u001b[0m      \u001b[1;31m129070\u001b[0m    \u001b[1;31m128387\u001b[0m \u001b[1;37m14491\u001b[0m     \u001b[1;31m129211\u001b[0m    \u001b[1;37m15671\u001b[0m   \u001b[1;37m304\u001b[0m \u001b[1;37m51594\u001b[0m     \u001b[1;37m3645\u001b[0m    \u001b[1;37m11\u001b[0m \u001b[1;37m369\u001b[0m  \u001b[1;37m3187\u001b[0m     \u001b[1;37m353\u001b[0m \u001b[1;37m36298\u001b[0m     \u001b[1;37m291\u001b[0m \u001b[1;37m3857\u001b[0m     \u001b[1;37m961\u001b[0m   \u001b[1;31m128335\u001b[0m \u001b[1;37m12594\u001b[0m \u001b[1;37m353\u001b[0m \u001b[1;37m36298\u001b[0m     \u001b[1;37m291\u001b[0m \u001b[1;37m3857\u001b[0m     \u001b[1;37m961\u001b[0m  \n",
      "\n",
      "Ġ2     *,    Ġ*  highlight ed  Ġsection Ġpart Ġ3     *.    <|eot_id|> <|start_header_id|> assistant <|end_header_id|> ĊĊ  Ray   mond  ĠIII  ĠCount Ġof ĠTrip oli   ĠwasĠa ĠPrince Ġof ĠAnt io  ch  Ġand\n",
      "\u001b[1;31m128359\u001b[0m \u001b[1;37m12594\u001b[0m \u001b[1;37m353\u001b[0m \u001b[1;37m36298\u001b[0m     \u001b[1;37m291\u001b[0m \u001b[1;37m3857\u001b[0m     \u001b[1;37m961\u001b[0m   \u001b[1;31m128387\u001b[0m \u001b[1;37m20517\u001b[0m \u001b[1;31m128009\u001b[0m     \u001b[1;31m128006\u001b[0m              \u001b[1;37m78191\u001b[0m     \u001b[1;31m128007\u001b[0m            \u001b[1;37m271\u001b[0m \u001b[1;37m30287\u001b[0m \u001b[1;37m12669\u001b[0m \u001b[1;37m14767\u001b[0m \u001b[1;37m4605\u001b[0m   \u001b[1;37m315\u001b[0m \u001b[1;37m27852\u001b[0m \u001b[1;37m14559\u001b[0m \u001b[1;31m128371\u001b[0m \u001b[1;37m19912\u001b[0m   \u001b[1;37m315\u001b[0m \u001b[1;37m6898\u001b[0m \u001b[1;37m822\u001b[0m \u001b[1;37m331\u001b[0m \u001b[1;37m323\u001b[0m \n",
      "\n",
      "Ġruler Ġof ĠTrip oli   Ġfrom Ġ   115  2  Ġuntil Ġhis Ġdeath Ġin Ġ   118  7  .ĠHis  Ġfather Ġwas ĠRaymond ĠII  ĠCount Ġof ĠTrip oli   Ġand Ġhis Ġmother Ġwas ĠMor phia  ĠofĠa  ĠHa   ute  ville .ĠHis \n",
      "\u001b[1;37m49080\u001b[0m  \u001b[1;37m315\u001b[0m \u001b[1;37m27852\u001b[0m \u001b[1;37m14559\u001b[0m \u001b[1;37m505\u001b[0m   \u001b[1;37m220\u001b[0m \u001b[1;37m7322\u001b[0m \u001b[1;37m17\u001b[0m \u001b[1;37m3156\u001b[0m   \u001b[1;37m813\u001b[0m  \u001b[1;37m4648\u001b[0m   \u001b[1;37m304\u001b[0m \u001b[1;37m220\u001b[0m \u001b[1;37m8899\u001b[0m \u001b[1;37m22\u001b[0m \u001b[1;31m128724\u001b[0m \u001b[1;37m7126\u001b[0m    \u001b[1;37m574\u001b[0m  \u001b[1;37m43670\u001b[0m    \u001b[1;37m8105\u001b[0m \u001b[1;37m4605\u001b[0m   \u001b[1;37m315\u001b[0m \u001b[1;37m27852\u001b[0m \u001b[1;37m14559\u001b[0m \u001b[1;37m323\u001b[0m  \u001b[1;37m813\u001b[0m  \u001b[1;37m6691\u001b[0m    \u001b[1;37m574\u001b[0m  \u001b[1;37m8613\u001b[0m \u001b[1;37m18120\u001b[0m \u001b[1;31m128300\u001b[0m \u001b[1;37m14433\u001b[0m \u001b[1;37m1088\u001b[0m \u001b[1;37m8078\u001b[0m  \u001b[1;31m128724\u001b[0m\n",
      "\n",
      "ĠHe  Ġwas Ġthe Ġyoungest Ġof Ġseveral Ġsons Ġand Ġwas Ġgiven Ġthe Ġtitle Ġof ĠCount Ġof ĠTrip oli   Ġafter Ġhis Ġfather 's  Ġdeath Ġin Ġ   115  2  ĊĊ  * Histor ical ĠSign ificance *ĊĊ   Ray   mond \n",
      "\u001b[1;37m1283\u001b[0m \u001b[1;37m574\u001b[0m  \u001b[1;37m279\u001b[0m  \u001b[1;37m39637\u001b[0m     \u001b[1;37m315\u001b[0m \u001b[1;37m3892\u001b[0m     \u001b[1;37m26419\u001b[0m \u001b[1;37m323\u001b[0m  \u001b[1;37m574\u001b[0m  \u001b[1;37m2728\u001b[0m   \u001b[1;37m279\u001b[0m  \u001b[1;37m2316\u001b[0m   \u001b[1;37m315\u001b[0m \u001b[1;37m4605\u001b[0m   \u001b[1;37m315\u001b[0m \u001b[1;37m27852\u001b[0m \u001b[1;37m14559\u001b[0m \u001b[1;37m1306\u001b[0m   \u001b[1;37m813\u001b[0m  \u001b[1;37m7126\u001b[0m    \u001b[1;37m596\u001b[0m \u001b[1;37m4648\u001b[0m   \u001b[1;37m304\u001b[0m \u001b[1;37m220\u001b[0m \u001b[1;37m7322\u001b[0m \u001b[1;37m17\u001b[0m \u001b[1;37m271\u001b[0m \u001b[1;37m9\u001b[0m \u001b[1;37m50083\u001b[0m  \u001b[1;37m950\u001b[0m  \u001b[1;37m7220\u001b[0m  \u001b[1;37m100104\u001b[0m   \u001b[1;37m22242\u001b[0m \u001b[1;37m30287\u001b[0m \u001b[1;37m12669\u001b[0m\n",
      "\n",
      "ĠIII  ĠwasĠa Ġsignificant Ġfigure Ġin Ġthe Ġhistory Ġof Ġthe ĠCrus ades .ĠHis  ĠHe  Ġwas Ġa  Ġkey\n",
      "\u001b[1;37m14767\u001b[0m \u001b[1;31m128371\u001b[0m \u001b[1;37m5199\u001b[0m         \u001b[1;37m7216\u001b[0m    \u001b[1;37m304\u001b[0m \u001b[1;37m279\u001b[0m  \u001b[1;37m3925\u001b[0m     \u001b[1;37m315\u001b[0m \u001b[1;37m279\u001b[0m  \u001b[1;37m51203\u001b[0m \u001b[1;37m3536\u001b[0m \u001b[1;31m128724\u001b[0m \u001b[1;37m1283\u001b[0m \u001b[1;37m574\u001b[0m  \u001b[1;37m264\u001b[0m \u001b[1;37m1401\u001b[0m\n",
      "\n",
      "\n",
      "Optimal_tokenization (194 tokens):\n",
      "\n",
      "<|begin_of_text|>\n",
      "<|start_header_id|>\n",
      "system\n",
      "<|end_header_id|>\n",
      "ĊĊ\n",
      "Cut\n",
      "ting\n",
      "ĠKnowledge\n",
      "ĠDate\n",
      ":\n",
      "ĠDecember\n",
      "Ġ20\n",
      "23\n",
      "Ċ\n",
      "Today\n",
      "ĠDate\n",
      ":\n",
      "Ġ\n",
      "06\n",
      "ĠMay\n",
      "Ġ20\n",
      "25\n",
      "ĊĊ\n",
      "<|eot_id|>\n",
      "<|start_header_id|>\n",
      "user\n",
      "<|end_header_id|>\n",
      "ĊĊ\n",
      "WriteĠa\n",
      "Ġ\n",
      "300\n",
      "+\n",
      "Ġword\n",
      "Ġsummary\n",
      "ĠofĠthe\n",
      "Ġwikipedia\n",
      "Ġpage\n",
      "Ġ\"\n",
      "https\n",
      "://\n",
      "en\n",
      ".wikipedia\n",
      ".org\n",
      "/wiki\n",
      "/R\n",
      "ay\n",
      "mond\n",
      "_\n",
      "III\n",
      ",\n",
      "_Count\n",
      "_of\n",
      "_T\n",
      "rip\n",
      "oli\n",
      "\".\n",
      "ĠDo\n",
      "Ġnot\n",
      "Ġuse\n",
      "Ġany\n",
      "Ġcommas\n",
      "Ġand\n",
      "Ġhighlight\n",
      "ĠatĠleast\n",
      "Ġ3\n",
      "Ġsections\n",
      "ĠthatĠhas\n",
      "Ġtitles\n",
      "Ġin\n",
      "Ġmarkdown\n",
      "Ġformat\n",
      ",\n",
      "Ġfor\n",
      "Ġexample\n",
      "Ġ*\n",
      "highlight\n",
      "ed\n",
      "Ġsection\n",
      "Ġpart\n",
      "Ġ1\n",
      "*,\n",
      "Ġ*\n",
      "highlight\n",
      "ed\n",
      "Ġsection\n",
      "Ġpart\n",
      "Ġ2\n",
      "*,\n",
      "Ġ*\n",
      "highlight\n",
      "ed\n",
      "Ġsection\n",
      "Ġpart\n",
      "Ġ3\n",
      "*.\n",
      "<|eot_id|>\n",
      "<|start_header_id|>\n",
      "assistant\n",
      "<|end_header_id|>\n",
      "ĊĊ\n",
      "Ray\n",
      "mond\n",
      "ĠIII\n",
      "ĠCount\n",
      "Ġof\n",
      "ĠTrip\n",
      "oli\n",
      "ĠwasĠa\n",
      "ĠPrince\n",
      "Ġof\n",
      "ĠAnt\n",
      "io\n",
      "ch\n",
      "Ġand\n",
      "Ġruler\n",
      "Ġof\n",
      "ĠTrip\n",
      "oli\n",
      "Ġfrom\n",
      "Ġ\n",
      "11\n",
      "52\n",
      "Ġuntil\n",
      "Ġhis\n",
      "Ġdeath\n",
      "ĠinĠ\n",
      "11\n",
      "87\n",
      ".ĠHis\n",
      "Ġfather\n",
      "Ġwas\n",
      "ĠRaymond\n",
      "ĠII\n",
      "ĠCount\n",
      "Ġof\n",
      "ĠTrip\n",
      "oli\n",
      "Ġand\n",
      "Ġhis\n",
      "Ġmother\n",
      "Ġwas\n",
      "ĠMor\n",
      "phia\n",
      "ĠofĠa\n",
      "ĠHa\n",
      "ute\n",
      "ville\n",
      ".ĠHis\n",
      "ĠHe\n",
      "ĠwasĠthe\n",
      "Ġyoungest\n",
      "Ġof\n",
      "Ġseveral\n",
      "Ġsons\n",
      "Ġand\n",
      "Ġwas\n",
      "Ġgiven\n",
      "Ġthe\n",
      "Ġtitle\n",
      "Ġof\n",
      "ĠCount\n",
      "Ġof\n",
      "ĠTrip\n",
      "oli\n",
      "Ġafter\n",
      "Ġhis\n",
      "Ġfather\n",
      "'s\n",
      "Ġdeath\n",
      "ĠinĠ\n",
      "11\n",
      "52\n",
      "ĊĊ\n",
      "*\n",
      "Histor\n",
      "ical\n",
      "ĠSign\n",
      "ificance\n",
      "*ĊĊ\n",
      "Ray\n",
      "mond\n",
      "ĠIII\n",
      "ĠwasĠa\n",
      "Ġsignificant\n",
      "Ġfigure\n",
      "ĠinĠthe\n",
      "Ġhistory\n",
      "ĠofĠthe\n",
      "ĠCrus\n",
      "ades\n",
      ".ĠHis\n",
      "ĠHe\n",
      "ĠwasĠa\n",
      "Ġkey\n",
      "<|begin_of_text|> <|start_header_id|> system <|end_header_id|> ĊĊ  Cut   ting ĠKnowledge ĠDate :  ĠDecember Ġ20    23   Ċ   Today ĠDate :  Ġ   06   ĠMay Ġ20    25  ĊĊ  <|eot_id|> <|start_header_id|>\n",
      "\u001b[1;37m128000\u001b[0m            \u001b[1;31m128006\u001b[0m              \u001b[1;37m9125\u001b[0m   \u001b[1;31m128007\u001b[0m            \u001b[1;37m271\u001b[0m \u001b[1;37m38766\u001b[0m \u001b[1;37m1303\u001b[0m \u001b[1;37m33025\u001b[0m      \u001b[1;37m2696\u001b[0m  \u001b[1;37m25\u001b[0m \u001b[1;37m6790\u001b[0m      \u001b[1;31m128460\u001b[0m \u001b[1;37m1419\u001b[0m \u001b[1;37m198\u001b[0m \u001b[1;37m15724\u001b[0m \u001b[1;37m2696\u001b[0m  \u001b[1;37m25\u001b[0m \u001b[1;37m220\u001b[0m \u001b[1;37m2705\u001b[0m \u001b[1;37m3297\u001b[0m \u001b[1;31m128460\u001b[0m \u001b[1;37m914\u001b[0m \u001b[1;37m271\u001b[0m \u001b[1;31m128009\u001b[0m     \u001b[1;31m128006\u001b[0m             \n",
      "\n",
      "user <|end_header_id|> ĊĊ  WriteĠa Ġ   300  +  Ġword Ġsummary ĠofĠthe Ġwikipedia Ġpage Ġ\"  https ://  en  .wikipedia .org /wiki /R    ay  mond  _  III   ,  _Count _of  _T   rip  oli   \".   ĠDo  Ġnot\n",
      "\u001b[1;37m882\u001b[0m  \u001b[1;31m128007\u001b[0m            \u001b[1;37m271\u001b[0m \u001b[1;31m128438\u001b[0m  \u001b[1;37m220\u001b[0m \u001b[1;37m3101\u001b[0m \u001b[1;37m10\u001b[0m \u001b[1;37m3492\u001b[0m  \u001b[1;37m12399\u001b[0m    \u001b[1;31m128258\u001b[0m  \u001b[1;37m59318\u001b[0m      \u001b[1;37m2199\u001b[0m  \u001b[1;37m330\u001b[0m \u001b[1;37m2485\u001b[0m  \u001b[1;37m1129\u001b[0m \u001b[1;37m268\u001b[0m \u001b[1;37m34466\u001b[0m      \u001b[1;37m2726\u001b[0m \u001b[1;37m26583\u001b[0m \u001b[1;37m19945\u001b[0m \u001b[1;37m352\u001b[0m \u001b[1;37m12669\u001b[0m \u001b[1;37m62\u001b[0m \u001b[1;37m23440\u001b[0m \u001b[1;37m11\u001b[0m \u001b[1;37m51875\u001b[0m  \u001b[1;37m3659\u001b[0m \u001b[1;37m1159\u001b[0m \u001b[1;37m4664\u001b[0m \u001b[1;37m14559\u001b[0m \u001b[1;37m3343\u001b[0m \u001b[1;37m3234\u001b[0m \u001b[1;37m539\u001b[0m \n",
      "\n",
      "Ġuse Ġany Ġcommas Ġand Ġhighlight ĠatĠleast Ġ3     Ġsections ĠthatĠhas Ġtitles Ġin Ġmarkdown Ġformat ,  Ġfor Ġexample Ġ*  highlight ed  Ġsection Ġpart Ġ1     *,    Ġ*  highlight ed  Ġsection Ġpart\n",
      "\u001b[1;37m1005\u001b[0m \u001b[1;37m904\u001b[0m  \u001b[1;37m77702\u001b[0m   \u001b[1;37m323\u001b[0m  \u001b[1;37m11415\u001b[0m      \u001b[1;31m129070\u001b[0m    \u001b[1;31m128387\u001b[0m \u001b[1;37m14491\u001b[0m     \u001b[1;31m129211\u001b[0m    \u001b[1;37m15671\u001b[0m   \u001b[1;37m304\u001b[0m \u001b[1;37m51594\u001b[0m     \u001b[1;37m3645\u001b[0m    \u001b[1;37m11\u001b[0m \u001b[1;37m369\u001b[0m  \u001b[1;37m3187\u001b[0m     \u001b[1;37m353\u001b[0m \u001b[1;37m36298\u001b[0m     \u001b[1;37m291\u001b[0m \u001b[1;37m3857\u001b[0m     \u001b[1;37m961\u001b[0m   \u001b[1;31m128335\u001b[0m \u001b[1;37m12594\u001b[0m \u001b[1;37m353\u001b[0m \u001b[1;37m36298\u001b[0m     \u001b[1;37m291\u001b[0m \u001b[1;37m3857\u001b[0m     \u001b[1;37m961\u001b[0m  \n",
      "\n",
      "Ġ2     *,    Ġ*  highlight ed  Ġsection Ġpart Ġ3     *.    <|eot_id|> <|start_header_id|> assistant <|end_header_id|> ĊĊ  Ray   mond  ĠIII  ĠCount Ġof ĠTrip oli   ĠwasĠa ĠPrince Ġof ĠAnt io  ch  Ġand\n",
      "\u001b[1;31m128359\u001b[0m \u001b[1;37m12594\u001b[0m \u001b[1;37m353\u001b[0m \u001b[1;37m36298\u001b[0m     \u001b[1;37m291\u001b[0m \u001b[1;37m3857\u001b[0m     \u001b[1;37m961\u001b[0m   \u001b[1;31m128387\u001b[0m \u001b[1;37m20517\u001b[0m \u001b[1;31m128009\u001b[0m     \u001b[1;31m128006\u001b[0m              \u001b[1;37m78191\u001b[0m     \u001b[1;31m128007\u001b[0m            \u001b[1;37m271\u001b[0m \u001b[1;37m30287\u001b[0m \u001b[1;37m12669\u001b[0m \u001b[1;37m14767\u001b[0m \u001b[1;37m4605\u001b[0m   \u001b[1;37m315\u001b[0m \u001b[1;37m27852\u001b[0m \u001b[1;37m14559\u001b[0m \u001b[1;31m128371\u001b[0m \u001b[1;37m19912\u001b[0m   \u001b[1;37m315\u001b[0m \u001b[1;37m6898\u001b[0m \u001b[1;37m822\u001b[0m \u001b[1;37m331\u001b[0m \u001b[1;37m323\u001b[0m \n",
      "\n",
      "Ġruler Ġof ĠTrip oli   Ġfrom Ġ   11  52   Ġuntil Ġhis Ġdeath ĠinĠ   11  87   .ĠHis  Ġfather Ġwas ĠRaymond ĠII  ĠCount Ġof ĠTrip oli   Ġand Ġhis Ġmother Ġwas ĠMor phia  ĠofĠa  ĠHa   ute  ville .ĠHis \n",
      "\u001b[1;37m49080\u001b[0m  \u001b[1;37m315\u001b[0m \u001b[1;37m27852\u001b[0m \u001b[1;37m14559\u001b[0m \u001b[1;37m505\u001b[0m   \u001b[1;37m220\u001b[0m \u001b[1;37m806\u001b[0m \u001b[1;37m4103\u001b[0m \u001b[1;37m3156\u001b[0m   \u001b[1;37m813\u001b[0m  \u001b[1;37m4648\u001b[0m   \u001b[1;31m128709\u001b[0m \u001b[1;37m806\u001b[0m \u001b[1;37m4044\u001b[0m \u001b[1;31m128724\u001b[0m \u001b[1;37m7126\u001b[0m    \u001b[1;37m574\u001b[0m  \u001b[1;37m43670\u001b[0m    \u001b[1;37m8105\u001b[0m \u001b[1;37m4605\u001b[0m   \u001b[1;37m315\u001b[0m \u001b[1;37m27852\u001b[0m \u001b[1;37m14559\u001b[0m \u001b[1;37m323\u001b[0m  \u001b[1;37m813\u001b[0m  \u001b[1;37m6691\u001b[0m    \u001b[1;37m574\u001b[0m  \u001b[1;37m8613\u001b[0m \u001b[1;37m18120\u001b[0m \u001b[1;31m128300\u001b[0m \u001b[1;37m14433\u001b[0m \u001b[1;37m1088\u001b[0m \u001b[1;37m8078\u001b[0m  \u001b[1;31m128724\u001b[0m\n",
      "\n",
      "ĠHe  ĠwasĠthe Ġyoungest Ġof Ġseveral Ġsons Ġand Ġwas Ġgiven Ġthe Ġtitle Ġof ĠCount Ġof ĠTrip oli   Ġafter Ġhis Ġfather 's  Ġdeath ĠinĠ   11  52   ĊĊ  * Histor ical ĠSign ificance *ĊĊ   Ray   mond \n",
      "\u001b[1;37m1283\u001b[0m \u001b[1;31m129006\u001b[0m   \u001b[1;37m39637\u001b[0m     \u001b[1;37m315\u001b[0m \u001b[1;37m3892\u001b[0m     \u001b[1;37m26419\u001b[0m \u001b[1;37m323\u001b[0m  \u001b[1;37m574\u001b[0m  \u001b[1;37m2728\u001b[0m   \u001b[1;37m279\u001b[0m  \u001b[1;37m2316\u001b[0m   \u001b[1;37m315\u001b[0m \u001b[1;37m4605\u001b[0m   \u001b[1;37m315\u001b[0m \u001b[1;37m27852\u001b[0m \u001b[1;37m14559\u001b[0m \u001b[1;37m1306\u001b[0m   \u001b[1;37m813\u001b[0m  \u001b[1;37m7126\u001b[0m    \u001b[1;37m596\u001b[0m \u001b[1;37m4648\u001b[0m   \u001b[1;31m128709\u001b[0m \u001b[1;37m806\u001b[0m \u001b[1;37m4103\u001b[0m \u001b[1;37m271\u001b[0m \u001b[1;37m9\u001b[0m \u001b[1;37m50083\u001b[0m  \u001b[1;37m950\u001b[0m  \u001b[1;37m7220\u001b[0m  \u001b[1;37m100104\u001b[0m   \u001b[1;37m22242\u001b[0m \u001b[1;37m30287\u001b[0m \u001b[1;37m12669\u001b[0m\n",
      "\n",
      "ĠIII  ĠwasĠa Ġsignificant Ġfigure ĠinĠthe Ġhistory ĠofĠthe ĠCrus ades .ĠHis  ĠHe  ĠwasĠa Ġkey\n",
      "\u001b[1;37m14767\u001b[0m \u001b[1;31m128371\u001b[0m \u001b[1;37m5199\u001b[0m         \u001b[1;37m7216\u001b[0m    \u001b[1;31m128265\u001b[0m  \u001b[1;37m3925\u001b[0m     \u001b[1;31m128258\u001b[0m  \u001b[1;37m51203\u001b[0m \u001b[1;37m3536\u001b[0m \u001b[1;31m128724\u001b[0m \u001b[1;37m1283\u001b[0m \u001b[1;31m128371\u001b[0m \u001b[1;37m1401\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copied examples\n",
    "import importlib\n",
    "import chat_templating\n",
    "importlib.reload(chat_templating)\n",
    "\n",
    "\n",
    "normal_result = {\n",
    "    \"input_ids\": [input_id_examples],\n",
    "    \"tokenizer\": tokenizer\n",
    "}\n",
    "normal_result[\"loss_mask\"] = [None] * len(normal_result[\"input_ids\"])\n",
    "\n",
    "for i in range(len(normal_result[\"input_ids\"])):    \n",
    "        # print(prompt_list[i])\n",
    "        print(tokenizer.batch_decode(normal_result[\"input_ids\"][i], skip_special_tokens=False))\n",
    "        print(f\"input_length: {len(normal_result['input_ids'][i])}\")\n",
    "        viz = chat_templating.visualize_loss_mask(\n",
    "            input_ids=normal_result[\"input_ids\"][i],\n",
    "            tokenizer=tokenizer,\n",
    "            loss_mask=normal_result[\"loss_mask\"][i],\n",
    "        )\n",
    "        print(viz)\n",
    "        print(\"\\n\")\n",
    "        optimal_tokenization = chat_templating.optimally_tokenize(normal_result[\"input_ids\"][i], tokenizer)\n",
    "        print(f\"Optimal_tokenization ({len(optimal_tokenization)} tokens):\\n\")\n",
    "        viz2 = chat_templating.visualize_loss_mask(\n",
    "            input_ids=optimal_tokenization,\n",
    "            tokenizer=tokenizer,\n",
    "            loss_mask=None,\n",
    "        )\n",
    "        print(viz2)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eff-tok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
