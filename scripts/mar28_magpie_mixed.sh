# python train_tokenizer.py \
#     --raw-data-name magpie_pro_300k_filtered \
#     --dataset-source-path /cmlscratch/astein0/efficient_tokenization_for_inference/datasets \
#     --tokenized-data-name magpie_tokenized-llama3 \
#     --pre-tok-name empty \
#     --cont-or-start start \
#     --batch-size 1000 \
#     --added-tokens 1000 \
#     --tokenizer-path-old meta-llama/Llama-3.2-1B-Instruct \
#     --tokenizer-source huggingface \
#     --save-tokenized-data

# python shrink_tokenizer.py \
#     --old-tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 \
#     --num-new-tokens-list 1,5,20,50,200,300,400,600,700,800,900

# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_0 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path meta-llama/Llama-3.2-1B-Instruct
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_1 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_5 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-5
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_10 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-10
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_20 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-20
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_50 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-50
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_100 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-100
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_200 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-200
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_300 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-300
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_400 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-400
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_500 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-500
#  python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_600 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-600
#  python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_700 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-700
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_800 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-800
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_900 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-900
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_1000 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000


# #64 bs, .00001 lr, merge, full finetune, 7000 steps
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_0 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path meta-llama/Llama-3.2-1B-Instruct --wandb-tags SFT,all_params_free,baseline,new_runs,magpie --eval-losses-to-track all --num-new-tokens 0 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_1,magpie-translation-tokenized_1 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 1 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_5,magpie-translation-tokenized_5 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-5 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 5 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 10 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_20,magpie-translation-tokenized_20 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-20 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 20 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_50,magpie-translation-tokenized_50 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-50 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 50 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 100 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_200,magpie-translation-tokenized_200 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-200 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 200 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_300,magpie-translation-tokenized_300 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-300 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 300 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_400,magpie-translation-tokenized_400 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-400 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 400 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_500,magpie-translation-tokenized_500 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 500 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_600,magpie-translation-tokenized_600 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-600 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 600 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_700,magpie-translation-tokenized_700 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-700 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 700 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_800,magpie-translation-tokenized_800 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-800 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 800 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_900,magpie-translation-tokenized_900 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-900 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 900 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 1000 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B

# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_0 --task-name SFT --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path meta-llama/Llama-3.2-1B-Instruct --wandb-tags SFT,all_params_free,baseline,new_runs,magpie,unfreeze1000 --eval-losses-to-track all --num-new-tokens 0 --experiment-name magpie_sweep  --benchmark-tasks ifeval --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_1,magpie-translation-tokenized_1 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 1 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_5,magpie-translation-tokenized_5 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-5 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 5 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 10 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_20,magpie-translation-tokenized_20 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-20 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 20 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_50,magpie-translation-tokenized_50 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-50 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 50 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 100 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_200,magpie-translation-tokenized_200 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-200 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 200 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_300,magpie-translation-tokenized_300 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-300 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 300 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_400,magpie-translation-tokenized_400 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-400 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 400 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_500,magpie-translation-tokenized_500 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 500 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_600,magpie-translation-tokenized_600 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-600 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 600 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_700,magpie-translation-tokenized_700 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-700 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 700 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_800,magpie-translation-tokenized_800 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-800 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 800 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_900,magpie-translation-tokenized_900 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-900 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 900 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 1000 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B

# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_0 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path meta-llama/Llama-3.2-1B-Instruct --wandb-tags SFT,all_params_free,baseline,new_runs,magpie --eval-losses-to-track all --num-new-tokens 0 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_1,magpie-translation-tokenized_1 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 1 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_5,magpie-translation-tokenized_5 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-5 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 5 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 10 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_20,magpie-translation-tokenized_20 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-20 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 20 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_50,magpie-translation-tokenized_50 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-50 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 50 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 100 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_200,magpie-translation-tokenized_200 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-200 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 200 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_300,magpie-translation-tokenized_300 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-300 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 300 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_400,magpie-translation-tokenized_400 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-400 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 400 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_500,magpie-translation-tokenized_500 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 500 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_600,magpie-translation-tokenized_600 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-600 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 600 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_700,magpie-translation-tokenized_700 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-700 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 700 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_800,magpie-translation-tokenized_800 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-800 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 800 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_900,magpie-translation-tokenized_900 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-900 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 900 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 --wandb-tags mixed,all_params_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 1000 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B

# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_0 --task-name SFT --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path meta-llama/Llama-3.2-1B-Instruct --wandb-tags SFT,all_params_free,baseline,new_runs,magpie,unfreeze1000 --eval-losses-to-track all --num-new-tokens 0 --experiment-name magpie_sweep  --benchmark-tasks ifeval --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_1,magpie-translation-tokenized_1 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 1 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_5,magpie-translation-tokenized_5 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-5 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 5 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 10 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_20,magpie-translation-tokenized_20 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-20 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 20 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_50,magpie-translation-tokenized_50 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-50 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 50 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 100 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_200,magpie-translation-tokenized_200 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-200 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 200 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_300,magpie-translation-tokenized_300 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-300 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 300 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_400,magpie-translation-tokenized_400 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-400 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 400 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_500,magpie-translation-tokenized_500 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 500 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_600,magpie-translation-tokenized_600 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-600 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 600 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_700,magpie-translation-tokenized_700 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-700 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 700 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_800,magpie-translation-tokenized_800 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-800 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 800 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_900,magpie-translation-tokenized_900 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-900 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 900 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 1000 --experiment-name magpie_sweep  --benchmark-tasks ifeval  --unfreeze-params-steps 1000 --model meta-llama/Llama-3.2-1B

# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_0 --task-name SFT --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path meta-llama/Llama-3.2-1B-Instruct --wandb-tags SFT,all_params_free,baseline,new_runs,magpie,unfreeze1000 --eval-losses-to-track all --num-new-tokens 0 --experiment-name magpie_sweep  --benchmark-tasks ifeva --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_1,magpie-translation-tokenized_1 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 1 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_5,magpie-translation-tokenized_5 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-5 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 5 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 10 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_20,magpie-translation-tokenized_20 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-20 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 20 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_50,magpie-translation-tokenized_50 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-50 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 50 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 100 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_200,magpie-translation-tokenized_200 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-200 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 200 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_300,magpie-translation-tokenized_300 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-300 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 300 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_400,magpie-translation-tokenized_400 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-400 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 400 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_500,magpie-translation-tokenized_500 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 500 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_600,magpie-translation-tokenized_600 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-600 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 600 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_700,magpie-translation-tokenized_700 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-700 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 700 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_800,magpie-translation-tokenized_800 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-800 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 800 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_900,magpie-translation-tokenized_900 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-900 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 900 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 2000 --batch-size 1 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 250 --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000 --task-name mixed --finetune-params embeddings --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 --wandb-tags mixed,embeddings_free,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all --num-new-tokens 1000 --experiment-name magpie_sweep  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-1B