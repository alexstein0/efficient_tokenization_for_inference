finetune.py --gradient-accumulate-every 32 --max-train-steps 10 --batch-size 1 --eval-steps 1 --eval-batch-size 1 --learning-rate 8e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags batch128,translation,extend_mean,all_params_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated
# finetune.py --gradient-accumulate-every 32 --max-train-steps 5000 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 8e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags batch128,translation,extend_mean,all_params_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated

# checkpoint testing
accelerate launch --num_processes 4 finetune.py --dry-run --total-batch-size 32 --max-train-steps 5 --batch-size 1 --eval-steps 1 --eval-batch-size 1 --learning-rate 8e-5 --checkpointing-steps 1 --wandb efficient_tokenization --wandb-tags test --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated
