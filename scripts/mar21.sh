# # Baseline
# accelerate launch --num_processes 8 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 2 --eval-steps 100 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/llama_tokenized --task-name SFT --finetune-params full --wandb-tags SFT,all_params_free,baseline,new_runs
# accelerate launch --num_processes 8 finetune.py --max-train-steps 5000 --total-batch-size 128 --batch-size 2 --eval-steps 100 --learning-rate 8e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/llama_tokenized --task-name SFT --finetune-params full --wandb-tags SFT,all_params_free,baseline,new_runs
# accelerate launch --num_processes 8 finetune.py --max-train-steps 5000 --total-batch-size 256 --batch-size 2 --eval-steps 100 --learning-rate 1.6e-4 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/llama_tokenized --task-name SFT --finetune-params full --wandb-tags SFT,all_params_free,baseline,new_runs

# Extend mean, params are free, translated_loss
accelerate launch --num_processes 8 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 2 --eval-steps 100 --eval-batch-size 1 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_mean,all_params_free,new_runs  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized_10 --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-10 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated  --num-new-tokens 10

# Extend merged, params are free, translated_loss
accelerate launch --num_processes 8 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 2 --eval-steps 100 --eval-batch-size 1 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_merged,all_params_free,new_runs  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized_10 --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-10 --embedding-init-strategy merge --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated  --num-new-tokens 10

# Extend mean, embeddings are free, translated_loss
accelerate launch --num_processes 8 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 2 --eval-steps 100 --eval-batch-size 1 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_mean,embeddings_free,new_runs  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized_10 --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-10 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings  --num-new-tokens 10

# Extend merged, embeddings are free, translated_loss
accelerate launch --num_processes 8 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 2 --eval-steps 100 --eval-batch-size 1 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_merged,embeddings_free,new_runs  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized_10 --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-10 --embedding-init-strategy merge --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings  --num-new-tokens 10

# UNFREEZE at 1000 steps
# Extend mean, embeddings are free, translated_loss
accelerate launch --num_processes 8 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 2 --eval-steps 100 --eval-batch-size 1 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_mean,embeddings_free,unfreeze1000,new_runs  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized_10 --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-10 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --unfreeze-params-steps 1000  --num-new-tokens 10

# Extend merged, embeddings are free, translated_loss
accelerate launch --num_processes 8 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 2 --eval-steps 100 --eval-batch-size 1 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_merged,embeddings_free,unfreeze1000,new_runs  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized_10 --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-10 --embedding-init-strategy merge --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --unfreeze-params-steps 1000  --num-new-tokens 10
