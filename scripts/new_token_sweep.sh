
# python shrink_tokenizer.py \
#     --old-tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 \
#     --num-new-tokens-list 1,5,20,50,100,200,300,400,500,600,700,800,900


# launch scripts/new_token_sweep.sh --time 48 --name tokenize_math --gpus 0 --mem 128 --cpus 16 --unset_tmpdir
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1 new_tokenized_1 
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-5 new_tokenized_5 
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-10 new_tokenized_10
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-20 new_tokenized_20 
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-50 new_tokenized_50 
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-100 new_tokenized_100 
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-200 new_tokenized_200
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-300 new_tokenized_300
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-400 new_tokenized_400
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-500 new_tokenized_500
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-600 new_tokenized_600
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-700 new_tokenized_700
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-800 new_tokenized_800
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --task translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-900 new_tokenized_900

# launch scripts/new_token_sweep.sh --time 24 --name translation_complete --gpus 8 --mem 248 --cpus 20 --gpu_type rtxa4000 --classical_logfile_names
#64 bs, .00001 lr, merge, full finetune, 7000 steps
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_1 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 1 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_5 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-5 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 5 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_10 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-10 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 10 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_20 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-20 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 20 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_50 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-50 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 50 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_100 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-100 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 100 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_200 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-200 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 200 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_300 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-300 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 300 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_400 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-400 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 400 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_500 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-500 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 500 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_600 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-600 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 600 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_700 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-700 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 700 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_800 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-800 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 800 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_900 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-900 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 900 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized_1000 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 1000 --experiment-name sweep
