# python train_tokenizer.py \
#     --raw-data-name magpie_pro_300k_filtered \
#     --dataset-source-path /cmlscratch/astein0/efficient_tokenization_for_inference/datasets \
#     --tokenized-data-name magpie_tokenized-llama3 \
#     --pre-tok-name empty \
#     --cont-or-start start \
#     --batch-size 1000 \
#     --added-tokens 1000 \
#     --tokenizer-path-old meta-llama/Llama-3.2-1B-Instruct \
#     --tokenizer-source huggingface \
#     --save-tokenized-data

# python shrink_tokenizer.py \
#     --old-tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 \
#     --num-new-tokens-list 1,5,10,20,50,100,200,300,400,600,700,800,900,1000 \
#     --add-special-tokens

# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_0 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path meta-llama/Llama-3.2-1B-Instruct
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_1 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_5 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-5
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_10 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_20 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-20
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_50 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-50
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_100 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_200 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-200
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_300 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-300
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_400 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-400
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_500 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_600 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-600
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_700 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-700
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_800 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-800
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_900 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-900
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_1000 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000

# test
# accelerate launch --num_processes 1 finetune.py --dry-run --run-lm-eval --limit 2 --total-batch-size 16 --max-train-steps 25    --batch-size 1 --eval-batch-size 1 --eval-steps 4 --learning-rate 2e-5 --warmup-steps 3 --checkpointing-steps 1    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,first_last,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name TESTING  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings   --unfreeze-params-steps 10  --finetune-params-after-unfreeze lora  --reset-optimizer  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine

# #1000
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,first_last,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name templating  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings   --unfreeze-params-steps 500  --finetune-params-after-unfreeze first_last  --reset-optimizer  --lr-schedule cosine
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name templating  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings   --unfreeze-params-steps 500  --finetune-params-after-unfreeze lora  --reset-optimizer  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine

#100
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,first_last,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 100 --experiment-name templating  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings   --unfreeze-params-steps 500  --finetune-params-after-unfreeze first_last  --reset-optimizer   --lr-schedule cosine
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 100 --experiment-name templating  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings   --unfreeze-params-steps 500  --finetune-params-after-unfreeze lora  --reset-optimizer  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine

#10
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,first_last,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 10 --experiment-name templating  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings   --unfreeze-params-steps 500  --finetune-params-after-unfreeze first_last  --reset-optimizer   --lr-schedule cosine
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 10 --experiment-name templating  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings   --unfreeze-params-steps 500  --finetune-params-after-unfreeze lora  --reset-optimizer  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine

# #0
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0,magpie-translation-tokenized_0    --task-name SFT    --wandb-tags SFT,baseline,first_last,new_runs,magpie  --eval-losses-to-track new_tokens,all --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name templating  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings   --unfreeze-params-steps 500  --finetune-params-after-unfreeze first_last  --reset-optimizer   --lr-schedule cosine
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0,magpie-translation-tokenized_0    --task-name SFT    --wandb-tags SFT,baseline,lora,new_runs,magpie  --eval-losses-to-track new_tokens,all --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name templating  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings   --unfreeze-params-steps 500  --finetune-params-after-unfreeze lora  --reset-optimizer  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine
