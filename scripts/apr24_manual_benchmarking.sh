accelerate launch --num_processes 1 -m lm_eval     --model_args pretrained=output/base_model/78a5d0c4-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000_templated     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model          --limit 10 	--log_samples               --apply_chat_template
# accelerate launch --num_processes 1 -m lm_eval     --model_args pretrained=output/base_model/255fb45b-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model          --limit 10 	--log_samples               
# accelerate launch --num_processes 1 -m lm_eval     --model_args pretrained=meta-llama/Llama-3.2-3B --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model          --limit 10 	--log_samples                --apply_chat_template
# accelerate launch --num_processes 1 -m lm_eval     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model          --limit 10 	--log_samples                --apply_chat_template
