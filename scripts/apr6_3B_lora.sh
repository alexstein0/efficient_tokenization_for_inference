# Test:
accelerate launch --num_processes 8 finetune.py --dry-run --total-batch-size 64 --max-train-steps 10    --batch-size 1 --eval-batch-size 1 --eval-steps 2 --learning-rate 1e-5 --checkpointing-steps 1  --save-checkpoints all  --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings_free,unfreeze1000,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct  --finetune-params embeddings  --unfreeze-params-steps 500  --finetune-params-after-unfreeze lora --reset-optimizer --extra-info linear_4_8_05  --lora-target-modules linear --lora-r 4 --lora-alpha 8  --lora-dropout 0.05

# # embeddings only
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings_free,unfreeze1000,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings

# # just lora
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct  --finetune-params lora --extra-info linear_8_16_05 --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct  --finetune-params lora --extra-info linear_4_8_05  --lora-target-modules linear --lora-r 4 --lora-alpha 8  --lora-dropout 0.05

# # just embeddings and first_last
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,first_last,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params first_last

# # full (6000)
accelerate launch --num_processes 2 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params full

# # embeddings then add first_last (3B_add_lora_fl)
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings_free,unfreeze1000,first_last,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings  --unfreeze-params-steps 500  --finetune-params-after-unfreeze first_last  --reset-optimizer

# # embeddings then lora (lora3B_redo) (3B_add_lora_fl) 
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings_free,unfreeze1000,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct  --finetune-params embeddings  --unfreeze-params-steps 500  --finetune-params-after-unfreeze lora --reset-optimizer --extra-info linear_8_16_05 --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings_free,unfreeze1000,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct  --finetune-params embeddings  --unfreeze-params-steps 500  --finetune-params-after-unfreeze lora --reset-optimizer --extra-info linear_4_8_05  --lora-target-modules linear --lora-r 4 --lora-alpha 8  --lora-dropout 0.05

# # embeddings then full (6000 second)
accelerate launch --num_processes 2 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings_free,unfreeze1000,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings  --unfreeze-params-steps 500  --finetune-params-after-unfreeze full  --reset-optimizer



# Same as above but SFT only
#test
# accelerate launch --num_processes 2 finetune.py --dryrun --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags SFT,embeddings_free,unfreeze1000,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings

# embeddings only
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags SFT,embeddings_free,unfreeze1000,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings

# # just lora
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags SFT,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct  --finetune-params lora --extra-info linear_8_16_05 --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags SFT,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct  --finetune-params lora --extra-info linear_4_8_05  --lora-target-modules linear --lora-r 4 --lora-alpha 8  --lora-dropout 0.05

# # just embeddings and first_last
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags SFT,first_last,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params first_last

# # full (6000)
# accelerate launch --num_processes 2 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags SFT,embeddings_free,unfreeze1000,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params full

# # embeddings then add first_last (3B_add_lora_fl)
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags SFT,embeddings_free,unfreeze1000,first_last,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings  --unfreeze-params-steps 500  --finetune-params-after-unfreeze first_last  --reset-optimizer

# # embeddings then lora (lora3B_redo) (3B_add_lora_fl)
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags SFT,embeddings_free,unfreeze1000,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct  --finetune-params embeddings  --unfreeze-params-steps 500  --finetune-params-after-unfreeze lora --reset-optimizer --extra-info linear_8_16_05 --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags SFT,embeddings_free,unfreeze1000,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct  --finetune-params embeddings  --unfreeze-params-steps 500  --finetune-params-after-unfreeze lora --reset-optimizer --extra-info linear_4_8_05  --lora-target-modules linear --lora-r 4 --lora-alpha 8  --lora-dropout 0.05

# # embeddings then full (6000 second)
# accelerate launch --num_processes 2 finetune.py --total-batch-size 64 --max-train-steps 7000    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 250    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags SFT,embeddings_free,unfreeze1000,lora,extend_merge,new_runs,magpie --eval-losses-to-track new_tokens,all    --num-new-tokens 1000 --experiment-name magpie-3B-loratests  --benchmark-tasks ifeval --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings  --unfreeze-params-steps 500  --finetune-params-after-unfreeze full  --reset-optimizer
