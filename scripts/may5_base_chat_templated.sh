# # 1000
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 5e-4 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 1000 --experiment-name baseline_embeddings  --benchmark-tasks ifeval  --log-samples  --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine --save-results 3

# 100
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 5e-4 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 100 --experiment-name baseline_embeddings  --benchmark-tasks ifeval  --log-samples  --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine --save-results 3

# 10
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 5e-4 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 10 --experiment-name baseline_embeddings  --benchmark-tasks ifeval  --log-samples  --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine --save-results 3

# 0
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 5e-4 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0,magpie-translation-tokenized_0    --task-name mixed    --wandb-tags mixed,baseline,lora,new_runs,magpie   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name baseline_embeddings  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine --save-results 3

#Baseline model experiments:
# batch size of 32, lr 5e-4
# will lora afterwards
# questions:
# bigger learning rate for embeddings part?
# using merge as embedding init strategy?
# learning rate schedule?

# finetuning step:
# 1000
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500 --log-step 2500    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 5e-4 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 1000 --experiment-name baseline_embeddings  --benchmark-tasks ifeval  --log-samples  --original-model meta-llama/Llama-3.2-3B --model output/baseline_embeddings/e08a94c1-Llama-3.2-3B-mixed-1000/final_model --finetune-params lora  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine --save-results 3

# 100
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500 --log-step 2500    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 5e-4 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 100 --experiment-name baseline_embeddings  --benchmark-tasks ifeval  --log-samples  --original-model meta-llama/Llama-3.2-3B --model output/baseline_embeddings/b9ece787-Llama-3.2-3B-mixed-100/final_model --finetune-params lora  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine --save-results 3

# 10
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500 --log-step 2500    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 5e-4 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 10 --experiment-name baseline_embeddings  --benchmark-tasks ifeval  --log-samples  --original-model meta-llama/Llama-3.2-3B --model output/baseline_embeddings/75ec96b0-Llama-3.2-3B-mixed-10/final_model --finetune-params lora  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine --save-results 3

# 0
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500 --log-step 2500    --batch-size 1 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 5e-4 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0,magpie-translation-tokenized_0    --task-name mixed    --wandb-tags mixed,baseline,lora,new_runs,magpie   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name baseline_embeddings  --benchmark-tasks ifeval  --log-samples --original-model meta-llama/Llama-3.2-3B --model output/baseline_embeddings/838db6d0-Llama-3.2-3B-mixed-0/final_model --finetune-params lora  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine --save-results 3


# output/baseline_embeddings/e08a94c1-Llama-3.2-3B-mixed-1000/final_model/
# output/baseline_embeddings/b9ece787-Llama-3.2-3B-mixed-100/final_model/
# output/baseline_embeddings/75ec96b0-Llama-3.2-3B-mixed-10/final_model/
# output/baseline_embeddings/838db6d0-Llama-3.2-3B-mixed-0/final_model/