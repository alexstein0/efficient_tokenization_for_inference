# tokenize the dataset
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-10 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-10


# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-1 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-5 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-5
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-10 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-10
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-20 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-20
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-50 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-50
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-100 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-100
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-200 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-200
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-300 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-300
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-400 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-400
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-500 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-500
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-600 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-600
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-700 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-700
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-800 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-800
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-900 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-900
# python data_preprocessing.py --dataset-path /fs/cml-projects/llm-pretraining/datasets/raw/genqa/math --save-dataset-name tokenized-1000 --task default,translation --model meta-llama/Llama-3.2-1B --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000

# TESTING finetune
# accelerate launch --num_processes 8 finetune.py --dry-run --total-batch-size 64 --max-train-steps 7000 --batch-size 2 \
#     --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  \
#     --dataset genqa-default-tokenized-10,genqa-translation-tokenized-10 --task-name mixed \
#     --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 \
#     --wandb-tags SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all \
#     --num-new-tokens 1000 --experiment-name TESTING_MIXED


# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-1,genqa-translation-tokenized-1 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 1 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-5,genqa-translation-tokenized-5 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-5 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 5 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-10,genqa-translation-tokenized-10 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-10 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 10 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-20,genqa-translation-tokenized-20 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-20 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 20 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-50,genqa-translation-tokenized-50 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-50 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 50 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-100,genqa-translation-tokenized-100 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-100 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 100 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-200,genqa-translation-tokenized-200 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-200 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 200 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-300,genqa-translation-tokenized-300 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-300 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 300 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-400,genqa-translation-tokenized-400 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-400 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 400 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-500,genqa-translation-tokenized-500 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-500 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 500 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-600,genqa-translation-tokenized-600 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-600 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 600 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-700,genqa-translation-tokenized-700 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-700 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 700 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-800,genqa-translation-tokenized-800 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-800 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 800 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-900,genqa-translation-tokenized-900 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-900 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 900 --experiment-name sweep
# accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 1 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset genqa-default-tokenized-1000,genqa-translation-tokenized-1000 --task-name mixed --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --wandb-tags mixed,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 1000 --experiment-name sweep


