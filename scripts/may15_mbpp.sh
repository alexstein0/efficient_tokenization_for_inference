# python data_preprocessing.py --raw-data-name mbpp --dataset-path datasets/mbpp --save-dataset-name tokenized_1000 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 --chat-template-name llama32
# python data_preprocessing.py --raw-data-name mbpp --dataset-path datasets/mbpp --save-dataset-name tokenized_500 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500 --chat-template-name llama32
# python data_preprocessing.py --raw-data-name mbpp --dataset-path datasets/mbpp --save-dataset-name tokenized_100 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100 --chat-template-name llama32
# python data_preprocessing.py --raw-data-name mbpp --dataset-path datasets/mbpp --save-dataset-name tokenized_10 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10 --chat-template-name llama32
# python data_preprocessing.py --raw-data-name mbpp --dataset-path datasets/mbpp --save-dataset-name tokenized_0 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path meta-llama/Llama-3.2-1B-Instruct --chat-template-name llama32

# 1000
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 1 --eval-batch-size 1 --eval-steps 50  --eval-iters 16 --benchmark-steps 100 --learning-rate 2e-5 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset mbpp-default-llama32-tokenized_1000,mbpp-translation-llama32-tokenized_1000    --task-name mixed --embedding-init-strategy import  --import-path output/full_patching/80c6810f-Llama-3.2-3B-Instruct-mixed-1000/final_model    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,full,extend_merge,new_runs,mbpp     --num-new-tokens 1000 --experiment-name mbpp  --benchmark-tasks mbpp  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params lora --lora-r 8 --lora-alpha 16 --lora-dropout 0.05 --lr-schedule cosine  --save-results 3

# 500
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 1 --eval-batch-size 1 --eval-steps 50  --eval-iters 16 --benchmark-steps 100 --learning-rate 2e-5 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset mbpp-default-llama32-tokenized_500,mbpp-translation-llama32-tokenized_500    --task-name mixed --embedding-init-strategy import  --import-path output/full_patching/4b9504d3-Llama-3.2-3B-Instruct-mixed-500/final_model    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500    --wandb-tags mixed,full,extend_merge,new_runs,mbpp     --num-new-tokens 500 --experiment-name mbpp  --benchmark-tasks mbpp  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params lora --lora-r 8 --lora-alpha 16 --lora-dropout 0.05 --lr-schedule cosine  --save-results 3

# 100
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 1 --eval-batch-size 1 --eval-steps 50  --eval-iters 16 --benchmark-steps 100 --learning-rate 2e-5 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset mbpp-default-llama32-tokenized_100,mbpp-translation-llama32-tokenized_100    --task-name mixed --embedding-init-strategy import  --import-path output/full_patching/5c0eb32b-Llama-3.2-3B-Instruct-mixed-100/final_model    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,full,extend_merge,new_runs,mbpp     --num-new-tokens 100 --experiment-name mbpp  --benchmark-tasks mbpp  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params lora --lora-r 8 --lora-alpha 16 --lora-dropout 0.05 --lr-schedule cosine  --save-results 3

# 10
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 1 --eval-batch-size 1 --eval-steps 50  --eval-iters 16 --benchmark-steps 100 --learning-rate 2e-5 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset mbpp-default-llama32-tokenized_10,mbpp-translation-llama32-tokenized_10    --task-name mixed --embedding-init-strategy import  --import-path output/full_patching/f276bbe3-Llama-3.2-3B-Instruct-mixed-10/final_model    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,full,extend_merge,new_runs,mbpp     --num-new-tokens 10 --experiment-name mbpp  --benchmark-tasks mbpp  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params lora --lora-r 8 --lora-alpha 16 --lora-dropout 0.05 --lr-schedule cosine  --save-results 3

# 0
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 1 --eval-batch-size 1 --eval-steps 50  --eval-iters 16 --benchmark-steps 100 --learning-rate 2e-5 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset mbpp-default-llama32-tokenized_0,mbpp-translation-llama32-tokenized_0    --task-name mixed    --wandb-tags mixed,baseline,full,new_runs,mbpp   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name mbpp  --benchmark-tasks mbpp  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params lora --lora-r 8 --lora-alpha 16 --lora-dropout 0.05 --lr-schedule cosine  --save-results 3
