# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=meta-llama/Llama-3.2-3B,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template               
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/af1121e7-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/59ebfc6a-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/075520a7-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/f0ab0e60-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/da382992-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/fca1987a-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/26e451d8-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/678dc4b5-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/78a5d0c4-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/ea3803d0-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/0ff4f4a1-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/bfe73ac1-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/e4309376-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/1c31b609-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/84123479-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/d2563dfa-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/d63bc629-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/c874feb7-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/30819dc4-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/b0cebb7a-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/305f57dd-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/7e83c5e1-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/45e6015c-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/ceffae22-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/5f9d6539-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/95aa9131-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/abc4ba5c-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/271f01ca-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/a8c95a79-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/463a4736-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/08cd3fa4-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/ff1dd3d9-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/26e0bc14-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/e06f4e3a-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/45574179-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/255fb45b-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/3a370b42-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/bbe3a0de-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/41a4af2b-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/9d3a3d1f-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/dec9f9e1-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/f58bc4b1-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/c475a6c1-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template

# accelerate launch --num_processes 2  --main_process_port 29601 lm_eval_new_tokens     --model_args pretrained=output/templating/381d099f-Llama-3.2-3B-Instruct-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29602 lm_eval_new_tokens     --model_args pretrained=output/templating/747113b2-Llama-3.2-3B-Instruct-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29603 lm_eval_new_tokens     --model_args pretrained=output/templating/ce7afe78-Llama-3.2-3B-Instruct-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29604 lm_eval_new_tokens     --model_args pretrained=output/templating/b0948c7c-Llama-3.2-3B-Instruct-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29605 lm_eval_new_tokens     --model_args pretrained=output/templating/f2270e48-Llama-3.2-3B-Instruct-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29606 lm_eval_new_tokens     --model_args pretrained=output/templating/eb5e0411-Llama-3.2-3B-Instruct-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29607 lm_eval_new_tokens     --model_args pretrained=output/templating/5226688f-Llama-3.2-3B-Instruct-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29608 lm_eval_new_tokens     --model_args pretrained=output/templating/043305d0-Llama-3.2-3B-Instruct-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples


# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=output/longer_embeddings/01514c83-final_model-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings     --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=output/longer_embeddings/1657a6a7-final_model-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings     --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=output/longer_embeddings/557bfe7e-final_model-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings     --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=output/longer_embeddings/4d00499a-Llama-3.2-3B-Instruct-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings     --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samplesaccelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples      --limit 100                    --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct

# # scripts/may5_base_chat_templated.sh
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/e08a94c1-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/b9ece787-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/75ec96b0-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/838db6d0-Llama-3.2-3B-mixed-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct

# scripts/may5_base_chat_templated.sh
# accelerate launch --num_processes 8 --main_process_port 29601 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 --main_process_port 29602 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/e08a94c1-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 --main_process_port 29603 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/b9ece787-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 --main_process_port 29604 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/75ec96b0-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 --main_process_port 29605 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/838db6d0-Llama-3.2-3B-mixed-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/e08a94c1-Llama-3.2-3B-mixed-1000/final_model     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct

# scripts/may5_base_chat_templated.sh
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/f38a7698-final_model-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/25cdf8bf-final_model-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/5268e299-final_model-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/37eb18ee-final_model-mixed-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct

# scripts/may7_patching.sh
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/fd51a57e-Llama-3.2-3B-Instruct_plus_e08a94c1-Llama-3.2-3B-mixed-1000-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/607f97cd-Llama-3.2-3B-Instruct_plus_b9ece787-Llama-3.2-3B-mixed-100-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/c329e22b-Llama-3.2-3B-Instruct_plus_75ec96b0-Llama-3.2-3B-mixed-10-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/f67f0583-Llama-3.2-3B-Instruct-mixed-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/patching/Llama-3.2-3B-Instruct_plus_e08a94c1-Llama-3.2-3B-mixed-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct

# scripts/may7_patching.sh
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/fd51a57e-Llama-3.2-3B-Instruct_plus_e08a94c1-Llama-3.2-3B-mixed-1000-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/patching_plus_finetuning     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/607f97cd-Llama-3.2-3B-Instruct_plus_b9ece787-Llama-3.2-3B-mixed-100-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/patching_plus_finetuning     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/c329e22b-Llama-3.2-3B-Instruct_plus_75ec96b0-Llama-3.2-3B-mixed-10-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/patching_plus_finetuning     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/baseline_embeddings/f67f0583-Llama-3.2-3B-Instruct-mixed-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/patching_plus_finetuning     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=output/patching/Llama-3.2-3B-Instruct_plus_e08a94c1-Llama-3.2-3B-mixed-1000     --gen_kwargs do_sample=False,temperature=None,top_p=None     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct

# # scripts/may12_full_patching.sh
# # accelerate launch --num_processes 4 --main_process_port 29500 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/ba7c8b60-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29501 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/234bdfaa-Llama-3.2-3B-mixed-500/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29502 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/3c315a04-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29503 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/d2d49ae1-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29504 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/a84197a7-Llama-3.2-3B-mixed-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29505 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/80c6810f-Llama-3.2-3B-Instruct-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29506 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/4b9504d3-Llama-3.2-3B-Instruct-mixed-500/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29507 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/5c0eb32b-Llama-3.2-3B-Instruct-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29508 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/f276bbe3-Llama-3.2-3B-Instruct-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29509 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/c5764fce-Llama-3.2-3B-Instruct-mixed-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29510 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/461b9dfe-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29511 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/e8e7ce8d-Llama-3.2-3B-mixed-500/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29512 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/2ecfe6ea-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29513 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/bd539cf5-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29514 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/d1f6703c-Llama-3.2-3B-mixed-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29515 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/8ce7f0d6-Llama-3.2-3B-Instruct-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29516 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/4d6edc1e-Llama-3.2-3B-Instruct-mixed-500/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29517 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/e5175fd2-Llama-3.2-3B-Instruct-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29518 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/e93b6877-Llama-3.2-3B-Instruct-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29519 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/8d72bbbe-Llama-3.2-3B-Instruct-mixed-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29520 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/63d35748-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 4 --main_process_port 29521 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/dfe1b80e-Llama-3.2-3B-mixed-500/final_model/embeddings_only.pt
# # accelerate launch --num_processes 4 --main_process_port 29522 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/508aaf68-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29523 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/daac169e-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29524 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/8760b763-Llama-3.2-3B-mixed-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 8 --main_process_port 29525 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/6e1e953a-Llama-3.2-3B-Instruct-mixed-1000/final_model/embeddings_only.pt
# accelerate launch --num_processes 8 --main_process_port 29526 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/755abfca-Llama-3.2-3B-Instruct-mixed-500/final_model/embeddings_only.pt
# accelerate launch --num_processes 8 --main_process_port 29527 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/2916d55c-Llama-3.2-3B-Instruct-mixed-100/final_model/embeddings_only.pt
# # accelerate launch --num_processes 4 --main_process_port 29528 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/12f3cea0-Llama-3.2-3B-Instruct-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29529 lm_eval_new_tokens     --model hf     --model_args pretrained=output/full_patching/cb47c58d-Llama-3.2-3B-Instruct-mixed-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# accelerate launch --num_processes 4 --main_process_port 29530 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/4eb8fba9-Llama-3.2-3B-mixed-1000/final_model/embeddings_only.pt
# accelerate launch --num_processes 4 --main_process_port 29531 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/e45070ca-Llama-3.2-3B-mixed-500/final_model/embeddings_only.pt
# accelerate launch --num_processes 4 --main_process_port 29532 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/0a90e719-Llama-3.2-3B-mixed-100/final_model/embeddings_only.pt
# accelerate launch --num_processes 4 --main_process_port 29533 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/461ac385-Llama-3.2-3B-mixed-10/final_model/embeddings_only.pt
# accelerate launch --num_processes 4 --main_process_port 29534 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/74ed1b01-Llama-3.2-3B-mixed-0/final_model/embeddings_only.pt
# accelerate launch --num_processes 4 --main_process_port 29535 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/9df250b6-Llama-3.2-3B-Instruct-mixed-1000/final_model/embeddings_only.pt
# accelerate launch --num_processes 4 --main_process_port 29536 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/0a4e765d-Llama-3.2-3B-Instruct-mixed-500/final_model/embeddings_only.pt
# accelerate launch --num_processes 4 --main_process_port 29537 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/958352ea-Llama-3.2-3B-Instruct-mixed-100/final_model/embeddings_only.pt
# accelerate launch --num_processes 8 --main_process_port 29538 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/9e5180ab-Llama-3.2-3B-Instruct-mixed-10/final_model/embeddings_only.pt
# accelerate launch --num_processes 4 --main_process_port 29539 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct,embeddings=output/full_patching/368f5252-Llama-3.2-3B-Instruct-mixed-0/final_model/embeddings_only.pt
# # accelerate launch --num_processes 8 --main_process_port 29540 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct
# # accelerate launch --num_processes 4 --main_process_port 29541 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/full_patching     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct


# PATCHED
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=meta-llama/Llama-3.2-3B-Instruct --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/d1f6703c-Llama-3.2-3B-mixed-0/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=meta-llama/Llama-3.2-3B-Instruct --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/74ed1b01-Llama-3.2-3B-mixed-0/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/bd539cf5-Llama-3.2-3B-mixed-10/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/461ac385-Llama-3.2-3B-mixed-10/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/0a90e719-Llama-3.2-3B-mixed-100/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/2ecfe6ea-Llama-3.2-3B-mixed-100/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/e45070ca-Llama-3.2-3B-mixed-500/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/e8e7ce8d-Llama-3.2-3B-mixed-500/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/4eb8fba9-Llama-3.2-3B-mixed-1000/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/461b9dfe-Llama-3.2-3B-mixed-1000/final_model/embeddings_only.pt,new_only=True

# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=meta-llama/Llama-3.2-3B-Instruct --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/8760b763-Llama-3.2-3B-mixed-0/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=meta-llama/Llama-3.2-3B-Instruct --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/a84197a7-Llama-3.2-3B-mixed-0/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/d2d49ae1-Llama-3.2-3B-mixed-10/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/daac169e-Llama-3.2-3B-mixed-10/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/508aaf68-Llama-3.2-3B-mixed-100/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/3c315a04-Llama-3.2-3B-mixed-100/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/dfe1b80e-Llama-3.2-3B-mixed-500/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/234bdfaa-Llama-3.2-3B-mixed-500/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/ba7c8b60-Llama-3.2-3B-mixed-1000/final_model/embeddings_only.pt,new_only=True
# accelerate launch --num_processes 8 lm_eval_new_tokens --model hf --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0 --tasks ifeval --batch_size auto --output_path ./eval_results/full_patching --apply_chat_template --log_samples --extra_config base_tokenizer=meta-llama/Llama-3.2-3B,embeddings=output/full_patching/63d35748-Llama-3.2-3B-mixed-1000/final_model/embeddings_only.pt,new_only=True

# scripts/may15_mbpp.sh
# accelerate launch --num_processes 8 --main_process_port 29500 lm_eval_new_tokens     --model hf     --model_args pretrained=output/mbpp/ec41a8c7-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks mbpp     --batch_size auto     --output_path ./eval_results/mbpp     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B
# accelerate launch --num_processes 8 --main_process_port 29501 lm_eval_new_tokens     --model hf     --model_args pretrained=output/mbpp/125c3b13-Llama-3.2-3B-mixed-500/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks mbpp     --batch_size auto     --output_path ./eval_results/mbpp     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B
# accelerate launch --num_processes 8 --main_process_port 29502 lm_eval_new_tokens     --model hf     --model_args pretrained=output/mbpp/9b5b7e56-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks mbpp     --batch_size auto     --output_path ./eval_results/mbpp     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B
accelerate launch --num_processes 8 --main_process_port 29503 lm_eval_new_tokens     --model hf     --model_args pretrained=output/mbpp/9303690a-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks mbpp     --batch_size auto     --output_path ./eval_results/mbpp     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B
accelerate launch --num_processes 8 --main_process_port 29504 lm_eval_new_tokens     --model hf     --model_args pretrained=output/mbpp/42b455c0-Llama-3.2-3B-mixed-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks mbpp     --batch_size auto     --output_path ./eval_results/mbpp     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B
accelerate launch --num_processes 8 --main_process_port 29505 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks mbpp     --batch_size auto     --output_path ./eval_results/mbpp     --apply_chat_template     --log_samples                          --extra_config base_tokenizer=meta-llama/Llama-3.2-3B
