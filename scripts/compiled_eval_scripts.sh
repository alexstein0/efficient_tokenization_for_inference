accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/dc76de75-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/2ad207e8-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/85f1aa7a-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/7cbea462-Llama-3.2-1B-SFT/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/e396cbfe-Llama-3.2-1B-SFT/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/24256a5c-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/2b46272f-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/743e0726-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/c25e4ee2-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/7ff7a0a4-Llama-3.2-1B-SFT/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/a64e0740-Llama-3.2-1B-SFT/final_model,tokenizer=meta-llama/Llama-3.2-1B     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/4bd8f485-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/59937f6c-Llama-3.2-1B-SFT/final_model,tokenizer=meta-llama/Llama-3.2-1B     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/8373b306-Llama-3.2-1B-SFT/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/7af44269-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/8fcec1c2-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/32344c7d-Llama-3.2-1B-SFT/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/67865f41-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000      --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/b4763d9c-Llama-3.2-1B-SFT/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/cb56868f-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/47573af2-Llama-3.2-1B-SFT/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/c13368d2-Llama-3.2-1B-SFT/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/963504bb-Llama-3.2-1B-SFT/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/f98d0390-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/3422d602-Llama-3.2-1B-SFT/final_model,tokenizer=meta-llama/Llama-3.2-1B     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
