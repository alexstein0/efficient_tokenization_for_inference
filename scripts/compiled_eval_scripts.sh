accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/warmup-tests                         
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/warmup-tests/fc24e189-Llama-3.2-3B-Instruct-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/warmup-tests
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/warmup-tests/4c31a43d-Llama-3.2-3B-Instruct-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/warmup-tests
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/warmup-tests/2ceef737-Llama-3.2-3B-Instruct-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/warmup-tests
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/warmup-tests/b2f05342-Llama-3.2-3B-Instruct-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/warmup-tests
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/warmup-tests/24758d67-Llama-3.2-3B-Instruct-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/warmup-tests
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/warmup-tests/ea0ef346-Llama-3.2-3B-Instruct-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/warmup-tests
