accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/Llama-3.2-1B-task_SFT-finetuning_params_full-batch64/final_model,tokenizer=meta-llama/Llama-3.2-1B     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/Llama-3.2-1B-task_translation-finetuning_params_full-batch128-extend_mean/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/Llama-3.2-1B-task_translation-finetuning_params_full-batch128-extend_merge/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/dryrun-67865f41-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/67865f41-Llama-3.2-1B-translation/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/Llama-3.2-1B-task_SFT-finetuning_params_full-batch256/final_model,tokenizer=meta-llama/Llama-3.2-1B     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/Llama-3.2-1B-task_SFT-finetuning_params_full-batch128/final_model,tokenizer=meta-llama/Llama-3.2-1B     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/Llama-3.2-1B-task_translation-finetuning_params_embeddings-batch64-extend_merge/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/Llama-3.2-1B-task_translation-finetuning_params_full-batch64-extend_mean/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/Llama-3.2-1B-task_SFT-finetuning_params_full-batch64-extend_mean/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/Llama-3.2-1B-task_translation-finetuning_params_embeddings-batch64-extend_mean/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100
accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/Llama-3.2-1B-task_translation-finetuning_params_full-batch64-extend_merge/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000     --gen_kwargs do_sample=True,temperature=0.7,top_p=3     --tasks minerva_math     --batch_size auto     --output_path ./eval_results          --limit 100