# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=meta-llama/Llama-3.2-3B,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template               
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/af1121e7-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/59ebfc6a-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/075520a7-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/f0ab0e60-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/da382992-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/fca1987a-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/26e451d8-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/678dc4b5-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/78a5d0c4-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/ea3803d0-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/0ff4f4a1-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/bfe73ac1-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/e4309376-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/1c31b609-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/84123479-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/d2563dfa-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/d63bc629-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/c874feb7-Llama-3.2-3B-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/30819dc4-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/b0cebb7a-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/305f57dd-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/7e83c5e1-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/45e6015c-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/ceffae22-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/5f9d6539-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/95aa9131-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# # haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/abc4ba5c-Llama-3.2-3B-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/271f01ca-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/a8c95a79-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/463a4736-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/08cd3fa4-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/ff1dd3d9-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/26e0bc14-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/e06f4e3a-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/45574179-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# accelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/255fb45b-Llama-3.2-3B-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct      --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/3a370b42-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/bbe3a0de-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/41a4af2b-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/9d3a3d1f-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/dec9f9e1-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/f58bc4b1-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template
# haccelerate launch --num_processes 8 -m lm_eval     --model_args pretrained=output/base_model/c475a6c1-Llama-3.2-3B-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/base_model_runs --apply_chat_template

# accelerate launch --num_processes 2  --main_process_port 29601 lm_eval_new_tokens     --model_args pretrained=output/templating/381d099f-Llama-3.2-3B-Instruct-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29602 lm_eval_new_tokens     --model_args pretrained=output/templating/747113b2-Llama-3.2-3B-Instruct-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29603 lm_eval_new_tokens     --model_args pretrained=output/templating/ce7afe78-Llama-3.2-3B-Instruct-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29604 lm_eval_new_tokens     --model_args pretrained=output/templating/b0948c7c-Llama-3.2-3B-Instruct-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29605 lm_eval_new_tokens     --model_args pretrained=output/templating/f2270e48-Llama-3.2-3B-Instruct-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29606 lm_eval_new_tokens     --model_args pretrained=output/templating/eb5e0411-Llama-3.2-3B-Instruct-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29607 lm_eval_new_tokens     --model_args pretrained=output/templating/5226688f-Llama-3.2-3B-Instruct-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 2  --main_process_port 29608 lm_eval_new_tokens     --model_args pretrained=output/templating/043305d0-Llama-3.2-3B-Instruct-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/templating  --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples


# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=output/longer_embeddings/01514c83-final_model-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings     --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=output/longer_embeddings/1657a6a7-final_model-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings     --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=output/longer_embeddings/557bfe7e-final_model-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings     --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=output/longer_embeddings/4d00499a-Llama-3.2-3B-Instruct-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings     --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples