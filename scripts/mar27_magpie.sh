# python train_tokenizer.py \
#     --raw-data-name magpie_pro_300k_filtered \
#     --dataset-source-path /cmlscratch/astein0/efficient_tokenization_for_inference/datasets \
#     --tokenized-data-name magpie_tokenized-llama3 \
#     --pre-tok-name empty \
#     --cont-or-start start \
#     --batch-size 1000 \
#     --added-tokens 1000 \
#     --tokenizer-path-old meta-llama/Llama-3.2-1B-Instruct \
#     --tokenizer-source huggingface \
#     --save-tokenized-data

# python shrink_tokenizer.py \
#     --old-tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 \
#     --num-new-tokens-list 100

# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path meta-llama/Llama-3.2-1B-Instruct --save-dataset-name tokenized_0 
# # python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1 --save-dataset-name tokenized_1 
# # python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-5 --save-dataset-name tokenized_5 
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10 --save-dataset-name tokenized_10
# # python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-20 --save-dataset-name tokenized_20 
# # python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-50 --save-dataset-name tokenized_50 
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100 --save-dataset-name tokenized_100 
# # python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-200 --save-dataset-name tokenized_200
# # python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-300 --save-dataset-name tokenized_300
# # python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-400 --save-dataset-name tokenized_400
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500 --save-dataset-name tokenized_500
# # python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-600 --save-dataset-name tokenized_600
# # python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-700 --save-dataset-name tokenized_700
# # python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-800 --save-dataset-name tokenized_800
# # python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-900 --save-dataset-name tokenized_900
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --task default --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 --save-dataset-name tokenized_1000


# launch scripts/mar27_magpie.sh --time 24 --name magpie_finetune --gpus 8 --mem 248 --cpus 20 --gpu_type rtxa4000 --classical_logfile_names
#64 bs, .00001 lr, merge, full finetune, 7000 steps
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset magpie-default-tokenized_0 --task-name SFT --finetune-params full --wandb-tags magpie,SFT,all_params_free,baseline,new_runs --model meta-llama/Llama-3.2-1B-Instruct --experiment-name magpie --benchmark-tasks ifeval
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset magpie-default-tokenized_10 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10 --wandb-tags magpie,SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 10 --experiment-name magpie --model meta-llama/Llama-3.2-1B-Instruct --benchmark-tasks ifeval
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset magpie-default-tokenized_100 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100 --wandb-tags magpie,SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 100 --experiment-name magpie --model meta-llama/Llama-3.2-1B-Instruct --benchmark-tasks ifeval
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset magpie-default-tokenized_500 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500 --wandb-tags magpie,SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 500 --experiment-name magpie --model meta-llama/Llama-3.2-1B-Instruct --benchmark-tasks ifeval
accelerate launch --num_processes 8 finetune.py --total-batch-size 64 --max-train-steps 7000 --batch-size 2 --eval-steps 100 --learning-rate 1e-5 --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000 --task-name SFT --finetune-params full --embedding-init-strategy merge --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 --wandb-tags magpie,SFT,all_params_free,extend_merge,new_runs --eval-losses-to-track new_tokens,all --num-new-tokens 1000 --experiment-name magpie --model meta-llama/Llama-3.2-1B-Instruct --benchmark-tasks ifeval
