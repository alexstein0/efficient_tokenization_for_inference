# # #1000
# accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 5000    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 250 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 1000 --experiment-name longer_embeddings  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params-prefreeze embeddings   --unfreeze-params-steps 2500  --finetune-params lora  --reset-optimizer  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine   --lr-schedule-prefreeze cosine

# #100
# accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 5000    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 250 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 100 --experiment-name longer_embeddings  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params-prefreeze embeddings   --unfreeze-params-steps 2500  --finetune-params lora  --reset-optimizer  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine   --lr-schedule-prefreeze cosine

# #10
# accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 5000    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 250 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 10 --experiment-name longer_embeddings  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params-prefreeze embeddings   --unfreeze-params-steps 2500  --finetune-params lora  --reset-optimizer  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine   --lr-schedule-prefreeze cosine

# #0
# accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 5000    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 250  --warmup-steps-prefreeze 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0,magpie-translation-tokenized_0    --task-name SFT    --wandb-tags mixed,baseline,lora,new_runs,magpie   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name longer_embeddings  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params-prefreeze embeddings   --unfreeze-params-steps 2500  --finetune-params lora  --reset-optimizer  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine   --lr-schedule-prefreeze cosine


# # continued:
# #1000
# accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --log-step 2500 --max-train-steps 2500    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 1000 --experiment-name longer_embeddings  --benchmark-tasks ifeval  --log-samples --original-model meta-llama/Llama-3.2-3B-Instruct --model  output/longer_embeddings/b4c3dd14-Llama-3.2-3B-Instruct-mixed-1000/final_model --finetune-params-prefreeze embeddings --finetune-params lora --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine

# # #100
# accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --log-step 2500 --max-train-steps 2500    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 100 --experiment-name longer_embeddings  --benchmark-tasks ifeval  --log-samples --original-model meta-llama/Llama-3.2-3B-Instruct --model  output/longer_embeddings/c729b2ae-Llama-3.2-3B-Instruct-mixed-100/final_model --finetune-params-prefreeze embeddings --finetune-params lora --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine

# # #10
# accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --log-step 2500 --max-train-steps 2500    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 10 --experiment-name longer_embeddings  --benchmark-tasks ifeval  --log-samples --original-model meta-llama/Llama-3.2-3B-Instruct --model  output/longer_embeddings/c0ab68af-Llama-3.2-3B-Instruct-mixed-10/final_model --finetune-params-prefreeze embeddings --finetune-params lora --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine

# #0
# accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --log-step 2500 --max-train-steps 2500    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0,magpie-translation-tokenized_0    --task-name mixed    --wandb-tags mixed,baseline,lora,new_runs,magpie   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name longer_embeddings  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params lora --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine



# # no training embeddings:
# # #1000
# accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 1000 --experiment-name longer_embeddings  --benchmark-tasks ifeval  --log-samples  --model meta-llama/Llama-3.2-3B-Instruct --finetune-params lora --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine

# # #100
# accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 100 --experiment-name longer_embeddings  --benchmark-tasks ifeval  --log-samples  --model meta-llama/Llama-3.2-3B-Instruct --finetune-params lora --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine

# # #10
# accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,lora,extend_merge,new_runs,magpie     --num-new-tokens 10 --experiment-name longer_embeddings  --benchmark-tasks ifeval  --log-samples  --model meta-llama/Llama-3.2-3B-Instruct --finetune-params lora --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine

# #0
# accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 2500    --batch-size 2 --eval-batch-size 1 --eval-steps 100 --benchmark-steps 250 --learning-rate 2e-5 --warmup-steps 250   --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0,magpie-translation-tokenized_0    --task-name mixed    --wandb-tags mixed,baseline,lora,new_runs,magpie   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name longer_embeddings  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params lora  --lora-target-modules linear --lora-r 8 --lora-alpha 16 --lora-dropout 0.05  --lr-schedule cosine

# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=output/longer_embeddings/01514c83-final_model-mixed-1000/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings     --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=output/longer_embeddings/1657a6a7-final_model-mixed-100/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings     --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=output/longer_embeddings/557bfe7e-final_model-mixed-10/final_model,tokenizer=/cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings     --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model_args pretrained=output/longer_embeddings/4d00499a-Llama-3.2-3B-Instruct-SFT-0/final_model,tokenizer=meta-llama/Llama-3.2-3B-Instruct     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/longer_embeddings     --apply_chat_template  --extra_config base_tokenizer=meta-llama/Llama-3.2-3B-Instruct  --log_samples
# accelerate launch --num_processes 8 lm_eval_new_tokens     --model hf     --model_args pretrained=meta-llama/Llama-3.2-3B     --gen_kwargs do_sample=False,temperature=0.0,top_p=1.0     --tasks ifeval     --batch_size auto     --output_path ./eval_results/baseline_embeddings     --apply_chat_template     --log_samples      --limit 100                    --extra_config base_tokenizer=meta-llama/Llama-3.2-3B

