# python shrink_tokenizer.py \
#     --old-tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000 \
#     --num-new-tokens-list 500 \
#     --add-special-tokens

# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_1000 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_500 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_100 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_10 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10
# # python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_1 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1
# python data_preprocessing.py --raw-data-name magpie --dataset-path datasets/magpie_pro_300k_filtered --save-dataset-name tokenized_0 --task default,translation --model meta-llama/Llama-3.2-1B-Instruct --tokenizer-path meta-llama/Llama-3.2-1B-Instruct


# #### base model new tokens only
# # #1000
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 1000 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

# 500
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_500,magpie-translation-tokenized_500    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 500 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

# 100
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 100 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

# 10
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 10 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

# 0
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0,magpie-translation-tokenized_0    --task-name mixed    --wandb-tags mixed,baseline,embeddings,new_runs,magpie   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3


# Instruct model new tokens only
# #1000
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 1000 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

# # 500
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_500,magpie-translation-tokenized_500    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 500 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

#100
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 100 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

#10
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 10 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

#0
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0,magpie-translation-tokenized_0    --task-name mixed    --wandb-tags mixed,baseline,embeddings,new_runs,magpie   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3


### base model embeddings
#1000
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 1000 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 500
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_500,magpie-translation-tokenized_500    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 500 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 100
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 100 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 10
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 10 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 0
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0,magpie-translation-tokenized_0    --task-name mixed    --wandb-tags mixed,baseline,embeddings,new_runs,magpie   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine  --save-results 3


# Instruct model embeddings
#1000
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000,magpie-translation-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 1000 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 500
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_500,magpie-translation-tokenized_500    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 500 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 100
accelerate launch --num_processes 4 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100,magpie-translation-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 100 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 10
accelerate launch --num_processes 4 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10,magpie-translation-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 10 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 0
accelerate launch --num_processes 4 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0,magpie-translation-tokenized_0    --task-name mixed    --wandb-tags mixed,baseline,embeddings,new_runs,magpie   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings --lr-schedule cosine  --save-results 3


##### same experiments except no translation data

### base model new tokens only
# #1000
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 1000 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

# 500
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_500    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 500 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

# 100
accelerate launch --num_processes 4 --main-process-port 12300 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 100 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

# 10
accelerate launch --num_processes 4 --main-process-port 12301 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 10 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

# 0
accelerate launch --num_processes 4 --main-process-port 12302 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0    --task-name mixed    --wandb-tags mixed,baseline,embeddings,new_runs,magpie   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3


# Instruct model new tokens only
# #1000
accelerate launch --num_processes 4 --main-process-port 12303 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 1000 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

# # 500
accelerate launch --num_processes 4 --main-process-port 12304 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_500    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 500 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

#100
accelerate launch --num_processes 4 --main-process-port 12305 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 100 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

#10
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 10 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3

#0
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0    --task-name mixed    --wandb-tags mixed,baseline,embeddings,new_runs,magpie   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params new_tokens_only --lr-schedule cosine  --save-results 3


## base model embeddings
#1000
accelerate launch --num_processes 4 --main-process-port 12306 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 1000 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 500
accelerate launch --num_processes 4 --main-process-port 12307 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_500    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 500 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 100
accelerate launch --num_processes 4 --main-process-port 12308 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 100 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 10
accelerate launch --num_processes 4 --main-process-port 12309 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 10 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 0
accelerate launch --num_processes 4 --main-process-port 12310 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0    --task-name mixed    --wandb-tags mixed,baseline,embeddings,new_runs,magpie   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B --finetune-params embeddings --lr-schedule cosine  --save-results 3


# # Instruct model embeddings
#1000
accelerate launch --num_processes 4 --main-process-port 12311 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_1000    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-1000    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 1000 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 500
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_500    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-500    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 500 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 100
accelerate launch --num_processes 8 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_100    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-100    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 100 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 10
accelerate launch --num_processes 4 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_10    --task-name mixed --embedding-init-strategy merge    --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-magpie_pro_300k_filtered-math-empty-start-10    --wandb-tags mixed,embeddings,extend_merge,new_runs,magpie     --num-new-tokens 10 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings --lr-schedule cosine  --save-results 3

# 0
accelerate launch --num_processes 4 finetune.py --run-lm-eval --limit 100 --total-batch-size 32 --max-train-steps 1000    --batch-size 2 --eval-batch-size 1 --eval-steps 50 --benchmark-steps 100 --learning-rate 5e-4 --warmup-steps 100 --checkpointing-steps 100    --wandb efficient_tokenization  --dataset magpie-default-tokenized_0    --task-name mixed    --wandb-tags mixed,baseline,embeddings,new_runs,magpie   --tokenizer-path meta-llama/Llama-3.2-3B-Instruct    --num-new-tokens 0 --experiment-name full_patching  --benchmark-tasks ifeval  --log-samples --model meta-llama/Llama-3.2-3B-Instruct --finetune-params embeddings --lr-schedule cosine  --save-results 3
