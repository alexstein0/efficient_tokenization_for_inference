# export TMPDIR="/cmlscratch/astein0/tmp"

# python finetune.py

# TESTS
# accelerate launch --num_processes 8 finetune.py --max-train-steps 10 --gradient-accumulate-every 8 --batch-size 1 --eval-steps 1 --eval-batch-size 1 --learning-rate 8e-5 --output-dir output/batch_128_TEST_other_losses --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags test,batch128,translation,losses  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --train-losses-to-track translated,new_tokens,all --eval-losses-to-track translated,new_tokens,all --main-loss translated --num-new-tokens 1000
# accelerate launch --num_processes 8 finetune.py --max-train-steps 5 --gradient-accumulate-every 8 --batch-size 2 --eval-steps 1 --learning-rate 8e-5 --output-dir output/batch_128_TEST --checkpointing-steps 1000 --wandb efficient_tokenization  --wandb-tags test,batch128  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/llama_tokenized  --num-new-tokens 1000
# accelerate launch --num_processes 8 finetune.py --max-train-steps 1 --gradient-accumulate-every 8 --batch-size 2 --eval-steps 1 --learning-rate 8e-5 --output-dir output/extend_test --checkpointing-steps 1000 --wandb efficient_tokenization  --wandb-tags test,batch128,extend  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --num-new-tokens 1000

#Ablate
# finetuning mode
# extend strategy
# batch size
# frozen params
# dataset/task

# params are free, translated_loss
# Baseline
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,baseline,all_params_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --num-new-tokens 1000
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 128 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 8e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,baseline,all_params_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --num-new-tokens 1000
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 256 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 1.6e-4 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,baseline,all_params_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --num-new-tokens 1000

# # Extend mean, params are free, translated_loss
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_mean,all_params_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --num-new-tokens 1000
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 128 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 8e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_mean,all_params_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --num-new-tokens 1000
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 256 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 1.6e-4 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_mean,all_params_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --num-new-tokens 1000

# # Extend merged, params are free, translated_loss
accelerate launch --num_processes 2 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_merged,all_params_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy merge --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --num-new-tokens 1000
accelerate launch --num_processes 2 finetune.py --max-train-steps 5000 --total-batch-size 128 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 8e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_merged,all_params_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy merge --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --num-new-tokens 1000
accelerate launch --num_processes 2 finetune.py --max-train-steps 5000 --total-batch-size 256 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 1.6e-4 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_merged,all_params_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy merge --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --num-new-tokens 1000

# # Extend mean, embeddings are free, translated_loss
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_mean,embeddings_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --num-new-tokens 1000
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 128 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 8e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_mean,embeddings_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --num-new-tokens 1000
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 256 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 1.6e-4 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_mean,embeddings_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --num-new-tokens 1000

# # Extend merged, embeddings are free, translated_loss
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_merged,embeddings_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy merge --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --num-new-tokens 1000
accelerate launch --num_processes 2 finetune.py --max-train-steps 5000 --total-batch-size 128 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 8e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_merged,embeddings_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy merge --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --num-new-tokens 1000
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 256 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 1.6e-4 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_merged,embeddings_free  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy merge --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --num-new-tokens 1000

# # UNFREEZE at 1000 steps
# # Extend mean, embeddings are free, translated_loss
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_mean,embeddings_free,unfreeze1000  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --unfreeze-params-steps 1000 --num-new-tokens 1000
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 128 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 8e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_mean,embeddings_free,unfreeze1000  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --unfreeze-params-steps 1000 --num-new-tokens 1000
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 256 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 1.6e-4 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_mean,embeddings_free,unfreeze1000  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy mean --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --unfreeze-params-steps 1000 --num-new-tokens 1000

# # Extend merged, embeddings are free, translated_loss
accelerate launch --num_processes 2 finetune.py --max-train-steps 5000 --total-batch-size 64 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 4e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_merged,embeddings_free,unfreeze1000  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy merge --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --unfreeze-params-steps 1000 --num-new-tokens 1000
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 128 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 8e-5 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_merged,embeddings_free,unfreeze1000  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy merge --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --unfreeze-params-steps 1000 --num-new-tokens 1000
# accelerate launch --num_processes 4 finetune.py --max-train-steps 5000 --total-batch-size 256 --batch-size 1 --eval-steps 100 --eval-batch-size 1 --learning-rate 1.6e-4 --checkpointing-steps 1000 --wandb efficient_tokenization --wandb-tags translation,extend_merged,embeddings_free,unfreeze1000  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/translation_tokenized --task translation --tokenizer-path /cmlscratch/astein0/efficient_tokenization_for_inference/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start-1000 --embedding-init-strategy merge --train-losses-to-track translated --eval-losses-to-track translated,new_tokens,all --main-loss translated --finetune-params embeddings --unfreeze-params-steps 1000 --num-new-tokens 1000
