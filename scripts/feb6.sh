# python finetune.py


# accelerate launch --num_processes 8 finetune_old.py --gradient-accumulate-every 8 --max-train-steps 5 --batch-size 2 --eval-steps 1 --learning-rate 8e-5 --output-dir output/batch_128_TEST --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/llama_tokenized 
# accelerate launch --num_processes 8 finetune_old.py --gradient-accumulate-every 8 --max-train-steps 5 --batch-size 2 --eval-steps 1 --learning-rate 8e-5 --output-dir output/extend_test --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized --tokenizer-path /cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start/new_mergeable_ranks_2000.model

# accelerate launch --num_processes 8 finetune_old.py --gradient-accumulate-every 8 --max-train-steps 5000 --batch-size 2 --eval-steps 100 --learning-rate 8e-5 --output-dir output/batch_128_checkpointing --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/llama_tokenized
# accelerate launch --num_processes 8 finetune_old.py --gradient-accumulate-every 16 --max-train-steps 5000 --batch-size 2 --eval-steps 100 --learning-rate 1.6e-4 --output-dir output/batch_256_checkpointing --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/llama_tokenized
accelerate launch --num_processes 8 finetune_old.py --gradient-accumulate-every 32 --max-train-steps 5000 --batch-size 2 --eval-steps 100 --learning-rate 3.2e-4 --output-dir output/batch_512_checkpointing --checkpointing-steps 1000 --wandb efficient_tokenization  --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/llama_tokenized

accelerate launch --num_processes 8 finetune_old.py --gradient-accumulate-every 8 --max-train-steps 5000 --batch-size 2 --eval-steps 100 --learning-rate 8e-5 --output-dir output/batch_128_extend --checkpointing-steps 1000 --wandb efficient_tokenization --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized --tokenizer-path /cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start/new_mergeable_ranks_2000.model
accelerate launch --num_processes 8 finetune_old.py --gradient-accumulate-every 16 --max-train-steps 5000 --batch-size 2 --eval-steps 100 --learning-rate 1.6e-4 --output-dir output/batch_256_extend --checkpointing-steps 1000 --wandb efficient_tokenization --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized --tokenizer-path /cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start/new_mergeable_ranks_2000.model
accelerate launch --num_processes 8 finetune_old.py --gradient-accumulate-every 32 --max-train-steps 5000 --batch-size 2 --eval-steps 100 --learning-rate 3.2e-4 --output-dir output/batch_512_extend --checkpointing-steps 1000 --wandb efficient_tokenization --dataset /cmlscratch/astein0/efficient_tokenization_for_inference/datasets/new_tokenized --tokenizer-path /cmlscratch/astein0/LLM-pretraining/LLM-pretraining-tokenization/tokenizers/Llama-3.2-tokenizer-genqa-math-empty-start/new_mergeable_ranks_2000.model
